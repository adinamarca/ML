{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "cf7922f0840c5393fef0e56634befbde274dd0b581610b62f6a1754d511fb74c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Redes neuronales\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado en el video de <b>DOT.CSV</b>: [Redes Neuronales](https://www.youtube.com/watch?v=W8AeOXa_FqU) y curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Redes neuronales\n",
    "\n",
    "Las redes neuronales fueron desarrolladas como simulación de las neuronas o de las redes de neuronas en el cerebro. \n",
    "\n",
    "## Neuronas\n",
    "\n",
    "<img src=\"2.png\"></img>\n",
    "\n",
    "Las neuronas - <i>como la que está adjunta en la imagen</i> - son una perfecta analogía para entender la representación de la hipótesis de las redes neuronales.\n",
    "\n",
    "Las primeras cosas que llaman la atención es que, las neuronas, tienen un cuerpo celular morado, con ramificaciones, llamadas \"dentritas\". Estas dentritas son similares a nuestras entradas de la función, 'inputs' o X (características).\n",
    "\n",
    "Además, estos cables de entrada reciben inputs de otras ubicaciones.\n",
    "\n",
    "Finalmente, tenemos que estos cables de entrada se conectan a un cable de salida, denominado axón, cuya función es enviar señales a otras neuronas. Este 'output' vendría siendo nuestras salidas 'h(x)'.\n",
    "\n",
    "Cobra sentido, ¿no parece?\n",
    "\n",
    "## Unidad computacional\n",
    "\n",
    "En analogía a las neuronas biológicas, y en un nivel simple, una neurona es una <b>unidad computacional</b> que tiene un número de entradas. Es, a través de estos cables de entrada, que se reciben características con las cuales se realizan algunos cálculos, y luego, se envían estos resultados mediante su axón (output) a otros nodos o a otras neuronas del cerebro.\n",
    "\n",
    "## Neurona: unidad de logística\n",
    "\n",
    "En una red neuronal artificial, lo que implementamos en una computadora es un modelo muy simple respecto a lo que realizan las neuronas biológicas.\n",
    "\n",
    "Modelaremos una neurona como una unidad logística. Así que, en el caso de la representación del modelo:\n",
    "\n",
    "• El círculo amarillo representaría una función análoga del cuerpo de una neurona (los cálculos sobre los input que darán un output).\n",
    "\n",
    "• Nuestro resultado, $ h_ \\theta (x) $, vendría siendo el output gracias al axón (en analogía biológica).\n",
    "\n",
    "<img src=\"3.png\"></img>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La función de activación\n",
    "\n",
    "Por lo general, lo que representa el cálculo $ h_{\\theta}(x) $ es:\n",
    "\n",
    "- Dada las entradas $ X_{1}, X_{2} $ y $ X_{3} $ se aplica una función (que puede ser, por ejemplo, una regresión logística) $ h_{\\theta} $, resultando en $ h_{\\theta}(x) $.\n",
    "\n",
    "En este sentido, recordemos que de notebooks pasados...\n",
    "\n",
    "Nuestra función está parametrizada por nuestro vector de parámetros:\n",
    "\n",
    "$ \\theta = \\begin{pmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} $\n",
    "\n",
    "Y definida por nuestro vector de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias)\n",
    "\n",
    "Hasta ahora, en los notebooks solo se ha hablado acerca de los parámetros $ \\theta $ como $\\theta_{i}$. Sin embargo, y por lo general, en terminología de las redes neuronales, a estos parámetros se les conoce como 'pesos' del modelo:\n",
    "\n",
    "- En inglés, 'weight'.\n",
    "\n",
    "Por lo que es usual encontrar el vector de parámetros expresados en función de $ W $ en vez de $ \\theta $.\n",
    "\n",
    "## Red neuronal\n",
    "\n",
    "En el diagrama anterior definimos una neurona, que es la base de una red neuronal.\n",
    "\n",
    "Una red neuronal consiste en diferentes neuronas unidas. Específicamente, tenemos:\n",
    "\n",
    "- Capa de entrada X (donde se introducen las variables o características).\n",
    "\n",
    "    - $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias)\n",
    "\n",
    "- Capas intermedias o ocultas (neuronas).\n",
    "\n",
    "    - $ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $, siendo el superíndice el número de capa y $ a_{0}^{2} = 1 $ (neurona de oscilación)\n",
    "\n",
    "- Capa de salida Y (resultado final de la hipótesis).\n",
    "\n",
    "    - $ h_{\\Theta} $ que es el valor o valores que calcula la hipótesis.\n",
    "\n",
    "Más adelante, veremos redes neuronales con más de una capa oculta, pero básicamente cualquier capa que no sea de entrada o salida será una capa oculta.\n",
    "\n",
    "## Notación 1\n",
    "\n",
    "Sí, lo sé. La notación es poco entendible al principio, pero créeme, te acostumbras, por lo que la odias al principio y luego la amas por su simplicidad.\n",
    "\n",
    "Sigamos.\n",
    "\n",
    "Arriba, si eres atento a los detalles, cambiamos $ \\theta $ por $ \\Theta $, y también, por supuesto, introducimos neuronas denotadas como $ a $, ¿por qué?\n",
    "\n",
    "Para explicar los cálculos específicos representados por una red neuronal, introducimos un poco más de notación:\n",
    "\n",
    "- Usaremos $ \\Theta^{(j)} $ para referirnos a los pesos de las neuronas de una determinada capa, siendo j la capa.\n",
    "\n",
    "- Además, las unidades de activación (neurona) serán definidas como $ a_{i}^{(j)} $ de la capa j y unidad i.\n",
    "\n",
    "- En sí, el nombre \"neurona\" no deja de ser un nombre extraordinariamente interesante para referirnos a una \"función\".\n",
    "\n",
    "## Notación 2\n",
    "\n",
    "Practiquemos un poco la notación. Sea:\n",
    "\n",
    "$ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $\n",
    "\n",
    "¿Qué nos dice la neurona $ a_{1}^{2} $?\n",
    "\n",
    "- Pues, implica que, es la primera unidad de la capa dos, realizando la activación de dicha unidad. Por cierto, con \"activación\" nos referimos al valor calculado por la neurona, es decir, su resultado. \n",
    "\n",
    "¿Qué nos dice $ \\Theta^{(j)} $?\n",
    "\n",
    "Como habíamos definido, $ \\Theta^{(j)} $ se encarga de parametrizar la red neuronal. En sí, es una matriz de ondas, la que controla el mapeo de la función de las capas.\n",
    "\n",
    "## El mapeo de la función de las capas\n",
    "\n",
    "¿Qué es el mapeo de la función de capas? \n",
    "\n",
    "Recordemos lo que hicimos en la regresión lineal, o en la regresión logística. ¿No es que intentamos ajustar una función a los datos? Pues sí, y esa es la base del \"mapeo\".\n",
    "\n",
    "Ésto lo define Hans Lehnert Merino, en [“Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales\"](https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y).\n",
    "\n",
    "- \"Lo que busca un método de machine learning es encontrar un mapeo entre\n",
    "una entrada X y una salida Y. Esto es precisamente lo que realiza una red neuronal: buscar una aproximación de una función mediante un modelo parametrizado\"\n",
    "\n",
    "Lehnert Merino, H. L. M. (2019, abril). Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales. Universidad Técnica Federico Santa María. Recuperado de https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Los cálculos\n",
    "\n",
    "Así que, teniendo definido los puntos anteriores, presentaremos los cálculos:\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Definiremos las matrices de mapeo para aclarar mejor la fórmula (recordamos que debemos trasponerlas para realizar los cálculos). Entonces, definimos de la segunda capa de la red neuronal, pero, con la primera capa de neuronas.\n",
    "\n",
    "La matriz de mapeo acotada (1) $ \\Theta^{(1)} = \\begin{pmatrix} \\Theta_{1}^{(1)} & \\Theta_{2}^{(1)} & \\Theta_{3}^{(1)} \\end{pmatrix} $, en donde:\n",
    "\n",
    "- $ \\Theta_{1}^{(1)} = \\begin{pmatrix} \\Theta_{10}^{(1)} \\\\ \\Theta_{11}^{(1)} \\\\ \\Theta_{12}^{(1)} \\\\ \\Theta_{13}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{2}^{(1)} = \\begin{pmatrix} \\Theta_{20}^{(1)} \\\\ \\Theta_{21}^{(1)} \\\\ \\Theta_{22}^{(1)} \\\\ \\Theta_{23}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{3}^{(1)} = \\begin{pmatrix} \\Theta_{30}^{(1)} \\\\ \\Theta_{31}^{(1)} \\\\ \\Theta_{32}^{(1)} \\\\ \\Theta_{33}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "De forma que la matriz de mapeo completa, que define la segunda capa, se construye a partir de reemplazar las matrices anteriores en la matriz acotada expuesta en (1). Esta matriz quedará de 3 x 4 dimensiones en este caso particular.\n",
    "\n",
    "Definimos de la tercera capa en la red neuronal, pero, con la segunda capa de neuronas:\n",
    "\n",
    "$ \\Theta_{1}^{(2)} = \\begin{pmatrix} \\Theta_{10}^{(2)} \\\\ \\Theta_{11}^{(2)} \\\\ \\Theta_{12}^{(2)} \\\\ \\Theta_{13}^{(2)} \\end{pmatrix} $\n",
    "\n",
    "Y la matriz de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $\n",
    "\n",
    "Siendo definidas nuestras neuronas en la capa oculta como:\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "$ a_{2}^{(2)} = g(\\Theta_{20}^{2}X_{0}+\\Theta_{21}^{2}X_{1}+\\Theta_{22}^{2}X_{2} + \\Theta_{23}^{2}X_{3}) $\n",
    "\n",
    "$ a_{3}^{(2)} = g(\\Theta_{30}^{3}X_{0}+\\Theta_{31}^{3}X_{1}+\\Theta_{32}^{3}X_{2} + \\Theta_{33}^{3}X_{3}) $\n",
    "\n",
    "Y una vez realizados su cálculos, se calcula la hipótesis en la tercera capa de la red neuronal, pero con la segunda capa de neuronas:\n",
    "\n",
    "$ h_{\\Theta}(X) = a_{1}^{(3)} = g(\\Theta_{10}^{2}X_{0}+\\Theta_{11}^{2}X_{1}+\\Theta_{12}^{2}X_{2} + \\Theta_{13}^{2}X_{3}) $\n",
    "\n",
    "Bastante jaleo, ¿no es cierto? Evidentemente, el procedimiento de transponer se realiza en base a la necesidad de operar las matrices (contenidos que se exploran en álgebra matricial).\n",
    "\n",
    "## Las conexiones\n",
    "\n",
    "En sí, todos los cálculos en una red neuronal se encuentran intrínsicamente relacionados unos con otros. \n",
    "\n",
    "Los cálculos de la segunda capa se explican a través de la primera capa:\n",
    "\n",
    "- La primera capa corresponden a las entradas (input), X.\n",
    "\n",
    "- La segunda capa realiza el cálculo sobre las variables $ X $ de la primera capa, parametrizando con $ \\Theta_{i}^{(1)} $ y resultando en $ a_{i}^{(2)} $, aplicando la combinación lineal de sus entradas y la función sigmoide.\n",
    "\n",
    "- La tercera capa realiza el cálculo sobre las variables $ a_{i}^{(2)} $ originadas en la capa anterior, parametrizando con $ \\Theta_{i}^{(2)}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}