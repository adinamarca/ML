{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "cf7922f0840c5393fef0e56634befbde274dd0b581610b62f6a1754d511fb74c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Redes neuronales\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado en el video de <b>DOT.CSV</b>: [Redes Neuronales](https://www.youtube.com/watch?v=W8AeOXa_FqU) y curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Redes neuronales\n",
    "\n",
    "Las redes neuronales fueron desarrolladas como simulación de las neuronas o de las redes de neuronas en el cerebro. \n",
    "\n",
    "## Neuronas\n",
    "\n",
    "<img src=\"2.png\"></img>\n",
    "\n",
    "Las neuronas - <i>como la que está adjunta en la imagen</i> - son una perfecta analogía para entender la representación de la hipótesis de las redes neuronales.\n",
    "\n",
    "Las primeras cosas que llaman la atención es que, las neuronas, tienen un cuerpo celular morado, con ramificaciones, llamadas \"dentritas\". Estas dentritas son similares a nuestras entradas de la función, 'inputs' o X (características).\n",
    "\n",
    "Además, estos cables de entrada reciben inputs de otras ubicaciones.\n",
    "\n",
    "Finalmente, tenemos que estos cables de entrada se conectan a un cable de salida, denominado axón, cuya función es enviar señales a otras neuronas. Este 'output' vendría siendo nuestras salidas 'h(x)'.\n",
    "\n",
    "Cobra sentido, ¿no parece?\n",
    "\n",
    "## Unidad computacional\n",
    "\n",
    "En analogía a las neuronas biológicas, y en un nivel simple, una neurona es una <b>unidad computacional</b> que tiene un número de entradas. Es, a través de estos cables de entrada, que se reciben características con las cuales se realizan algunos cálculos, y luego, se envían estos resultados mediante su axón (output) a otros nodos o a otras neuronas del cerebro.\n",
    "\n",
    "## Neurona: unidad de logística\n",
    "\n",
    "En una red neuronal artificial, lo que implementamos en una computadora es un modelo muy simple respecto a lo que realizan las neuronas biológicas.\n",
    "\n",
    "Modelaremos una neurona como una unidad logística. Así que, en el caso de la representación del modelo:\n",
    "\n",
    "• El círculo amarillo representaría una función análoga del cuerpo de una neurona (los cálculos sobre los input que darán un output).\n",
    "\n",
    "• Nuestro resultado, $ h_ \\theta (x) $, vendría siendo el output gracias al axón (en analogía biológica).\n",
    "\n",
    "<img src=\"3.png\"></img>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La función de activación\n",
    "\n",
    "Por lo general, lo que representa el cálculo $ h_{\\theta}(x) $ es:\n",
    "\n",
    "- Dada las entradas $ X_{1}, X_{2} $ y $ X_{3} $ se aplica una función (que puede ser, por ejemplo, una regresión logística) $ h_{\\theta} $, resultando en $ h_{\\theta}(x) $.\n",
    "\n",
    "En este sentido, recordemos que de notebooks pasados...\n",
    "\n",
    "Nuestra función está parametrizada por nuestro vector de parámetros:\n",
    "\n",
    "$ \\theta = \\begin{pmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} $\n",
    "\n",
    "Y definida por nuestro vector de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias)\n",
    "\n",
    "Hasta ahora, en los notebooks solo se ha hablado acerca de los parámetros $ \\theta $ como $\\theta_{i}$. Sin embargo, y por lo general, en terminología de las redes neuronales, a estos parámetros se les conoce como 'pesos' del modelo:\n",
    "\n",
    "- En inglés, 'weight'.\n",
    "\n",
    "Por lo que es usual encontrar el vector de parámetros expresados en función de $ W $ en vez de $ \\theta $.\n",
    "\n",
    "## Red neuronal\n",
    "\n",
    "En el diagrama anterior definimos una neurona, que es la base de una red neuronal.\n",
    "\n",
    "Una red neuronal consiste en diferentes neuronas unidas. Específicamente, tenemos:\n",
    "\n",
    "- Capa de entrada X (donde se introducen las variables o características).\n",
    "\n",
    "    - $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Capas intermedias o ocultas (neuronas).\n",
    "\n",
    "    - $ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $, siendo el superíndice el número de capa y $ a_{0}^{2} = 1 $ (neurona de oscilación).\n",
    "\n",
    "- Capa de salida Y (resultado final de la hipótesis).\n",
    "\n",
    "    - $ h_{\\Theta} $ que es el valor o valores que calcula la hipótesis.\n",
    "\n",
    "Más adelante, veremos redes neuronales con más de una capa oculta, pero básicamente cualquier capa que no sea de entrada o salida será una capa oculta.\n",
    "\n",
    "## Notación 1\n",
    "\n",
    "Sí, lo sé. La notación es poco entendible al principio, pero créeme, te acostumbras, por lo que la odias al principio y luego la amas por su simplicidad.\n",
    "\n",
    "Sigamos.\n",
    "\n",
    "Arriba, si eres atento a los detalles, cambiamos $ \\theta $ por $ \\Theta $, y también, por supuesto, introducimos neuronas denotadas como $ a $, ¿por qué?\n",
    "\n",
    "Para explicar los cálculos específicos representados por una red neuronal, introducimos un poco más de notación:\n",
    "\n",
    "- Usaremos $ \\Theta^{(j)} $ para referirnos a los pesos de las neuronas de una determinada capa, siendo j la capa.\n",
    "\n",
    "- Además, las unidades de activación (neurona) serán definidas como $ a_{i}^{(j)} $ de la capa j y unidad i.\n",
    "\n",
    "- En sí, el nombre \"neurona\" no deja de ser un nombre extraordinariamente interesante para referirnos a una \"función\".\n",
    "\n",
    "## Notación 2\n",
    "\n",
    "Practiquemos un poco la notación. Sea:\n",
    "\n",
    "$ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $\n",
    "\n",
    "¿Qué nos dice la neurona $ a_{1}^{2} $?\n",
    "\n",
    "- Pues, implica que, es la primera unidad de la capa dos, realizando la activación de dicha unidad. Por cierto, con \"activación\" nos referimos al valor calculado por la neurona, es decir, su resultado. \n",
    "\n",
    "¿Qué nos dice $ \\Theta^{(j)} $?\n",
    "\n",
    "Como habíamos definido, $ \\Theta^{(j)} $ se encarga de parametrizar la red neuronal. En sí, es una matriz de ondas, la que controla el mapeo de la función de las capas.\n",
    "\n",
    "## El mapeo de la función de las capas\n",
    "\n",
    "¿Qué es el mapeo de la función de capas? \n",
    "\n",
    "Recordemos lo que hicimos en la regresión lineal, o en la regresión logística. ¿No es que intentamos ajustar una función a los datos? Pues sí, y esa es la base del \"mapeo\".\n",
    "\n",
    "Ésto lo define Hans Lehnert Merino, en [“Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales\"](https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y).\n",
    "\n",
    "- \"Lo que busca un método de machine learning es encontrar un mapeo entre\n",
    "una entrada X y una salida Y. Esto es precisamente lo que realiza una red neuronal: buscar una aproximación de una función mediante un modelo parametrizado\"\n",
    "\n",
    "Lehnert Merino, H. L. M. (2019, abril). Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales. Universidad Técnica Federico Santa María. Recuperado de https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Los cálculos\n",
    "\n",
    "Así que, teniendo definido los puntos anteriores, presentaremos los cálculos:\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Definiremos las matrices de mapeo para aclarar mejor la fórmula (recordamos que debemos trasponerlas para realizar los cálculos). Entonces, definimos de la segunda capa de la red neuronal, pero, con la primera capa de neuronas.\n",
    "\n",
    "La matriz de mapeo acotada (1) $ \\Theta^{(1)} = \\begin{pmatrix} \\Theta_{1}^{(1)} & \\Theta_{2}^{(1)} & \\Theta_{3}^{(1)} \\end{pmatrix} $, en donde:\n",
    "\n",
    "- $ \\Theta_{1}^{(1)} = \\begin{pmatrix} \\Theta_{10}^{(1)} \\\\ \\Theta_{11}^{(1)} \\\\ \\Theta_{12}^{(1)} \\\\ \\Theta_{13}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{2}^{(1)} = \\begin{pmatrix} \\Theta_{20}^{(1)} \\\\ \\Theta_{21}^{(1)} \\\\ \\Theta_{22}^{(1)} \\\\ \\Theta_{23}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{3}^{(1)} = \\begin{pmatrix} \\Theta_{30}^{(1)} \\\\ \\Theta_{31}^{(1)} \\\\ \\Theta_{32}^{(1)} \\\\ \\Theta_{33}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "De forma que la matriz de mapeo completa, que define la segunda capa, se construye a partir de reemplazar las matrices anteriores en la matriz acotada expuesta en (1). Esta matriz quedará de 3 x 4 dimensiones en este caso particular.\n",
    "\n",
    "Definimos de la tercera capa en la red neuronal, pero, con la segunda capa de neuronas:\n",
    "\n",
    "$ \\Theta_{1}^{(2)} = \\begin{pmatrix} \\Theta_{10}^{(2)} \\\\ \\Theta_{11}^{(2)} \\\\ \\Theta_{12}^{(2)} \\\\ \\Theta_{13}^{(2)} \\end{pmatrix} $\n",
    "\n",
    "Y la matriz de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $\n",
    "\n",
    "Siendo definidas nuestras neuronas en la capa oculta como:\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "$ a_{2}^{(2)} = g(\\Theta_{20}^{2}X_{0}+\\Theta_{21}^{2}X_{1}+\\Theta_{22}^{2}X_{2} + \\Theta_{23}^{2}X_{3}) $\n",
    "\n",
    "$ a_{3}^{(2)} = g(\\Theta_{30}^{3}X_{0}+\\Theta_{31}^{3}X_{1}+\\Theta_{32}^{3}X_{2} + \\Theta_{33}^{3}X_{3}) $\n",
    "\n",
    "Y una vez realizados su cálculos, se calcula la hipótesis en la tercera capa de la red neuronal, pero con la segunda capa de neuronas:\n",
    "\n",
    "$ h_{\\Theta}(X) = a_{1}^{(3)} = g(\\Theta_{10}^{2}X_{0}+\\Theta_{11}^{2}X_{1}+\\Theta_{12}^{2}X_{2} + \\Theta_{13}^{2}X_{3}) $\n",
    "\n",
    "Bastante jaleo, ¿no es cierto? Evidentemente, el procedimiento de transponer se realiza en base a la necesidad de operar las matrices (contenidos que se exploran en álgebra matricial).\n",
    "\n",
    "## ¿Cómo entendemos una neurona particular?\n",
    "\n",
    "Por ejemplo, la neurona $a_{1}^{2}$ es igual a la función sigmoide o a la función de activación sigmoidal, aplicada a la siguiente combinación lineal de sus entradas.\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "Y así, sucesivamente, para cada neurona.\n",
    "\n",
    "## La dimensionalidad\n",
    "\n",
    "Si una red neuronal tiene:\n",
    "\n",
    "- Un número de $ s_{j} $ unidades en la capa j.\n",
    "\n",
    "- Un número de $ s_{j+1} $ unidades en la capa $ j + 1 $.\n",
    "\n",
    "Entonces, $\\Theta^{(j)}$ tendrá una dimensión de $ s_{j+1} \\times (s_{j} + 1) $.\n",
    "\n",
    "## Las conexiones\n",
    "\n",
    "En sí, todos los cálculos en una red neuronal se encuentran intrínsicamente relacionados unos con otros. \n",
    "\n",
    "Los cálculos de la segunda capa se explican a través de la primera capa:\n",
    "\n",
    "- La primera capa corresponden a las entradas (input), X.\n",
    "\n",
    "- La segunda capa realiza el cálculo sobre las variables $ X $ de la primera capa, parametrizando con $ \\Theta_{i}^{(1)} $ y resultando en $ a_{i}^{(2)} $, aplicando la combinación lineal de sus entradas y la función sigmoide.\n",
    "\n",
    "- La tercera capa realiza el cálculo sobre las variables $ a_{i}^{(2)} $ originadas en la capa anterior, parametrizando con $ \\Theta_{i}^{(2)}$.\n",
    "\n",
    "Así que, tras hablar de lo que hacen las tres unidades ocultas para calcular sus valores, hablaremos del último espinal: la última unidad.\n",
    "\n",
    "## La salida\n",
    "\n",
    "La última unidad, calcula $h_{\\theta}(X)$ que es Y, de forma que también podemos escribirlo como:\n",
    "\n",
    "- La salida es igual a $h_{\\theta}(X)=Y=a_{1}^{3}$, que es la función sigmoide evaluada en la combinación de parámetros.\n",
    "\n",
    "De esta forma, la función $ h_{\\theta}(X)$ mapea los valores de entrada $ X $, que, en algunos espacios o disposiciones $ Y $, definirá, con cierta precisión, un conjunto de hipótesis, llegando a diferentes funciones de mapeo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La búsqueda de la eficiencia\n",
    "\n",
    "En las últimas seldas, vimos la definición matemática sobre cómo representar y calcular las hipótesis utilizadas por las redes neuronales. \n",
    "\n",
    "A continuación, veremos <b>cómo realizar este cálculo de forma eficiente</b>, con la implementación vectorizada.\n",
    "\n",
    "En segundo lugar, Andrew NG. desea empezar a darnos una intuición sobre por qué las representaciones de las redes neuronales son una buena idea para ayudarnos con hipótesis complejas no lineales. Es decir, similar a lo que vimos en regresión logística en situaciones donde la hipótesis podía asemejarse a una circunferencia, o a curvas cualesquiera.\n",
    "\n",
    "## La implementación vectorizada\n",
    "\n",
    "<img src=\"5.png\"></img>\n",
    "\n",
    "Consideremos la red neuronal de arriba. \n",
    "\n",
    "Anteriormente, habíamos dicho que la secuencia de pasos que necesitamos para calcular la salida de una hipótesis en estas ecuaciones:\n",
    "\n",
    "- Calcular los valores de activación de las unidades ocultas con los valores de entrada.\n",
    "\n",
    "- Utilizando las salidas anteriores, calculamos para la última unidad de la espinal, obteniendo el resultado de la red neuronal $h_{\\theta}(X)$.\n",
    "\n",
    "Entonces, ahora es cuando definiremos la implementación vectorizada, debiendo introducir términos adicionales.\n",
    "\n",
    "Los términos subrayados los redefiniremos, por lo que, sea la combinación lineal ponderada de la funciones de activación:\n",
    "\n",
    "- Recordemos de [notebooks anteriores](https://github.com/adinamarca/notebooks/blob/main/PY/ML/3_regresion_logistica/notebook.ipynb). \n",
    "\n",
    "En dicho notebook, definimos $ h(\\Theta^{T}X) $, donde $ Z = \\Theta^{T}X $. \n",
    "\n",
    "Utilizaremos una definición similar para las hipótesis de las redes neuronales, definiendo por cada combinación lineal de cada neurona como $ Z_{i}^{j} $:\n",
    "\n",
    "- Donde $ i $ indica el número de la neurona.\n",
    "\n",
    "- Donde $ j $ corresponde al número de capa.\n",
    "\n",
    "De esta forma, redefinimos:\n",
    "\n",
    "- Sea $ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $, redefinimos $ Z_{1}^{2} = \\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3} $, de forma que quedamos con $ a_{1}^{(2)} = g(Z_{1}^{2}) $.\n",
    "\n",
    "- Repitiendo el paso anterior, quedamos con $ a_{2}^{(2)} = g(Z_{2}^{2}) $.\n",
    "\n",
    "- Y $ a_{3}^{(2)} = g(Z_{3}^{2}) $.\n",
    "\n",
    "Observamos que, estas nuevas definiciones, corresponden de forma sospechosamente similar a la operación $ \\Theta^{(i)}*X $ (en forma análoga a $\\Theta^{T}X$, pero estando $ \\Theta $ ya transpuesta).\n",
    "\n",
    "Mediante esta observación, seremos capaces de vectorizar este cálculo de la red neuronal:\n",
    "\n",
    "- Recordamos nuestro vector de características como $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Definimos nuestro vector de combinaciones lineales (que definen, cada uno, una función de mapeo a través de los parámetros $ \\Theta $) como $ Z^{(2)} = \\begin{pmatrix} Z_{1}^{2} \\\\ Z_{2}^{2} \\\\ Z_{3}^{2} \\end{pmatrix} $.\n",
    "\n",
    "¡Necesitamos un valor más! Que es la unidad de sesgo (bias unit). Para solucionar este problema, lo que haremos será sumar $ a_{0}^{2} $:\n",
    "\n",
    "- La unidad de oscilación la definimos como $ a_{0}^{2} = 1$.\n",
    "\n",
    "Ahora tendremos que $ a^{(2)} $ será un vector de cuatro dimensiones.\n",
    "\n",
    "Finalmente, para calcular el valor real de la salida de las hipótesis:\n",
    "\n",
    "- Queda $h_{\\Theta}(X)=a_{1}^{(3)}=g(\\Theta_{10}^{(2)}a_{0}^{(2)}+\\Theta_{11}^{(2)}a_{1}^{(2)}+\\Theta_{12}^{(2)}a_{2}^{(2)}+\\Theta_{13}^{(2)}a_{3}^{(2)})$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Funciones no lineales complejas\n",
    "\n",
    "En esta y la próxima celda trabajaremos con un ejemplo detallado que muestra cómo una red neuronal puede calcular una función no lineal compleja de entrada.\n",
    "\n",
    "<img src=\"6.png\"></img>\n",
    "\n",
    "Consideremos el problema de la imagen anterior, donde tenemos $ X_{1} $ y $ X_{2} $ como variables de entrada, lo que nos define que tenemos solo dos valores posibles: valores binarios.\n",
    "\n",
    "<img src=\"7.png\"></img>\n",
    "\n",
    "Podemos considerar la imagen anterior como una versión simplificada de un problema más complejo. En este sentido, se debería aprender un límite de decisión no lineal, de forma que debamos separar los ejemplos positivos y negativos.\n",
    "\n",
    "## Puertas lógicas\n",
    "\n",
    "<img src=\"8.png\"></img>\n",
    "\n",
    "~ Imagen por [Alejandro García](http://aletecnocampello.blogspot.com/2015/12/puertas-logicas.html\n",
    ").\n",
    "\n",
    "Antes de continuar, es recomendable leer acerca de las puertas lógicas (y no, no es para nada necesario aprenderse la nomenclatura).\n",
    "\n",
    "Una puerta lógica, o compuerta lógica, es un dispositivo electrónico con una función booleana u otras funciones como sumar o restar, incluyen o excluyen según sus propiedades lógicas (extraído desde [Wikipedia](https://es.wikipedia.org/wiki/Puerta_l%C3%B3gica)).\n",
    "\n",
    "- [Puerta lógica XOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XOR). Implica una salida verdadera (1 o TRUE) si una, y solo una de las entradas a la puerta es verdadera.\n",
    "\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XOR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 0\n",
    "\n",
    "- [Puerta lógica XNOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XNOR). Es la inversa de la puerta lógica XOR.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XNOR \\; X_{2} $ |\n",
    "| 0 | 0 | 1\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica OR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_OR). Es verdadero si al menos una entrada es verdadera.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;OR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica AND en Wikipedia](https://es.wikipedia.org/wiki/Puerta_AND). Es verdadero si y solo si las dos entradas son verdaderas.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;AND \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica NOT en Wikipedia](https://es.wikipedia.org/wiki/Puerta_NOT). Es verdadero si la entrada es falsa.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $\\;NOT \\; X_{1} $ |\n",
    "| 0 | 1\n",
    "| 1 | 0\n",
    "\n",
    "## Redes neuronales en funciones no lineales complejas\n",
    "\n",
    "Entonces, ¿cómo podemos hacer que una red neuronal se ajuste a este problema?\n",
    "\n",
    "Específicamente, lo que está calculando Y es equivalente a: $ Y = X_{1} \\;XNOR \\; X_{2} $. Es decir, tendremos ejemplos positivos siempre que ambas entradas sean positivas o ambas sean falsas.\n",
    "\n",
    "<img src=\"9.png\"></img>\n",
    "\n",
    "En este sentido, queremos saber si podemos hacer que una red neuronal sea capaz de ajustarse a este tipo de conjuntos de aprendizaje.\n",
    "\n",
    "## ¿Qué es la hipótesis no lineal compleja? \n",
    "\n",
    "Cuando nos referimos con hipótesis no lineales complejas, podemos utilizar el mismo ejemplo de la puerta lógica XNOR. En dicho ejemplo sería imposible trazar una recta que realizase el límite de decisión correctamente a ambos conjuntos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ajuste a puerta lógica AND\n",
    "\n",
    "Con el fin de construir una red que se ajuste al ejemplo XNOR, vamos a comenzar con un ejemplo ligeramente más simple: con una puerta lógica AND.\n",
    "\n",
    "<img src=\"10.png\"></img>\n",
    "\n",
    "Concretamente, digamos que tenemos las entradas $ X_{1} $ y $ X_{2} $ que, nuevamente, son binarias {$1 \\; , \\; 0$}.\n",
    "\n",
    "Entonces, ¿podremos hacer que una sola red neuronal se ajuste a este problema? En orden para ello, ¡consideraremos las unidades de oscilación!\n",
    "\n",
    "<img src=\"11.png\"></img>\n",
    "\n",
    "Ahora, asignaremos algunos valores a los pesos de los parámetros de la red.\n",
    "\n",
    "<img src=\"12.png\"></img>\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-30, 20, 20] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-30 + 20X_{1} + 20X_{2}) $.\n",
    "\n",
    "A veces, es conveniente dibujar los pesos de los parámetros en el diagrama de la red neuronal. \n",
    "\n",
    "Solo para recordarnos, la función de activación sigmoidea $ g(Z) $ comienza en $0$, se eleva lentamente, pasando por $ 0.5 $ y asintotando en $ 1 $.\n",
    "\n",
    "En este sentido:\n",
    "\n",
    "- Si $ Z = 4.6 $, entonces la función sigmoidea será de 0.99, y por otro lado, si evaluamos en $ -4.6 $ la función sigmoidea será de 0.01, lo cual es muy cercano a 0.\n",
    "\n",
    "<img src=\"13.png\"></img>\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -30 | 0\n",
    "| 0 | 1 | -10 | 0\n",
    "| 1 | 0 | -10 | 0\n",
    "| 1 | 1 | 10 | 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La importancia de los pesos\n",
    "\n",
    "Notar como, con los pesos de los parámetros, fuimos capaces de ajustar a este problema concreto, una sola neurona, dando el resultado esperado en la <b>tabla de verdad</b>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ajuste a puerta lógica OR\n",
    "\n",
    "Esta vez, construyamos una neurona para la puerta lógica OR.\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-20, 25, 25] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-20 + 25X_{1} + 25X_{2}) $.\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -20 | 0\n",
    "| 0 | 1 | 5 | 1\n",
    "| 1 | 0 | 5 | 1\n",
    "| 1 | 1 | 30 | 1\n",
    "\n",
    "## Las neuronas individuales\n",
    "\n",
    "De esta forma, concluimos cómo las neuronas individuales pueden utilizarse para funciones lógicas como AND o OR, entre otras.\n",
    "\n",
    "En la siguiente celda abordaremos ejemplos más complejos, llegando a trabajar con más de una neurona, formando una red neuronal capaz de usarse para calcular funciones más complejas como XOR o XNOR."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Un paso más allá\n",
    "\n",
    "En la última sección vimos como una red neuronal puede usarse para calcular las puertas lógicas $ AND $ y $ OR $.\n",
    "\n",
    "En esta celda trabajaremos en el ejemplo de cómo calcular hipótesis complejas no lineales.\n",
    "\n",
    "## Ajuste a puerta lógica NOT\n",
    "\n",
    "<img src=\"14.png\"></img>\n",
    "\n",
    "En este sentido, podríamos hacer que una red neuronal calcule una negación, lo que significa una puerta lógica $ NOT X_{1} $. Escribiremos una hipótesis asociada a esta red.\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}] = [5, -20] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(5-20X_{1}) $.\n",
    "\n",
    "Veamos entonces los <b>dos posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | EVALUACIÓN | OUTPUT\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 5 | 1\n",
    "| 1 | -15 | 0\n",
    "\n",
    "Entonces, cuando $ X_{1} = 0 $:\n",
    "\n",
    "- Estaremos calculando $ h_{\\Theta}(X) = g(5) = \\; aprox. \\; 1 $.\n",
    "\n",
    "Cuando $ X_{1} = 1 $:\n",
    "\n",
    "- Estaremos calculando $ h_{\\Theta}(X) = g(-15) = \\; aprox. \\; 0 $.\n",
    "\n",
    "Si observamos, estaremos calculando la puerta lógica de una negación, dado que al dar el valor $True$ al INPUT, el OUTPUT nos devuelve $False$. Por otro lado, si le brindamos $False$, nos devuelve $True$ ($True = 1$ y $False = 0$).\n",
    "\n",
    "Así que, para realizar una puerta lógica NOT, la idea general es colocar un gran peso negativo delante de la variable que se desea anular.\n",
    "\n",
    "De esta forma, hagamos ejemplo más complejo que implique una combinación de puertas lógicas.\n",
    "\n",
    "## Ajuste a la puerta lógica ($NOT \\; X_{1}$) $AND$ ($NOT \\; X_{2}$)\n",
    "\n",
    "Antes, definamos las negaciones.\n",
    "\n",
    "Si $ (NOT \\; False)  \\; AND \\;  (NOT \\; False) $, entonces $ (True) \\; AND \\; (True) = True $.\n",
    "\n",
    "Es decir, solo si tenemos dos $ NOT \\; False $, tendríamos dos $ True $, y $ True \\; AND \\; True $. Por lo que definiremos dos grandes pesos negativos encima de un positivo.\n",
    "\n",
    "La idea aquí es pensar:\n",
    "\n",
    "- Un bias negativo nos dará un sesgo negativo. Por lo que si lo anulamos con pesos positivos, tendremos $ True $.\n",
    "\n",
    "- Un bias positivo nos dará un sesgo positivo. Por lo que si lo anulamos con pesos negativos, tendremos $ False $.\n",
    "\n",
    "Definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [20, -25, -25] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(20 - 25X_{1} - 25X_{2}) $.\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ \\; X_{1} $ | $ \\; X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| $ NOT \\;  0 $ | $ NOT \\;  0 $ | -30 | 0\n",
    "| $ NOT \\;  0 $ | $ NOT \\;  1 $ | -5 | 0\n",
    "| $ NOT \\;  1 $ | $ NOT \\;  0 $ | -5 | 0\n",
    "| $ NOT \\;  1 $ | $ NOT \\;  1 $ | 20 | 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ¿Y el ajuste a XNOR?\n",
    "\n",
    "Recordemos la puerta lógica XNOR:\n",
    "\n",
    "- [Puerta lógica XNOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XNOR). Es la inversa de la puerta lógica XOR.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XNOR \\; X_{2} $ |\n",
    "| 0 | 0 | 1\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "El gráfico se vería de la siguiente manera:\n",
    "\n",
    "- Azul: ejemplos negativos.\n",
    "- Rojo: ejemplos positivos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos matplotlib (gráficos)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importamos pandas (manipulación de datos) y numpy (álgebra avanzada)\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x181261c0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <path clip-path=\"url(#pdfc62d22c2)\" d=\"M 42.321307 217.756364 \r\nL 48.321307 211.756364 \r\nM 42.321307 211.756364 \r\nL 48.321307 217.756364 \r\n\" style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\"/>\r\n    <path clip-path=\"url(#pdfc62d22c2)\" d=\"M 42.321307 20.083636 \r\nL 48.321307 14.083636 \r\nM 42.321307 14.083636 \r\nL 48.321307 20.083636 \r\n\" style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\"/>\r\n    <path clip-path=\"url(#pdfc62d22c2)\" d=\"M 346.684943 217.756364 \r\nL 352.684943 211.756364 \r\nM 346.684943 211.756364 \r\nL 352.684943 217.756364 \r\n\" style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\"/>\r\n    <path clip-path=\"url(#pdfc62d22c2)\" d=\"M 346.684943 20.083636 \r\nL 352.684943 14.083636 \r\nM 346.684943 14.083636 \r\nL 352.684943 20.083636 \r\n\" style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma4e886edb0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(37.369744 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.194034\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(98.242472 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.066761\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(159.115199 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.939489\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(219.987926 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.812216\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(280.860653 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#ma4e886edb0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(341.733381 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mf048179899\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"175.221818\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 179.021037)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"135.687273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 139.486491)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"96.152727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 99.951946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"56.618182\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 60.417401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf048179899\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pdfc62d22c2\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "xnordata = {'x':[0, 0, 1, 1], 'y':[0, 1, 0, 1], 'color':['r','b','b','r']}\n",
    "xnor = pd.DataFrame(data=xnordata)\n",
    "plt.scatter(x=xnor.x, y=xnor.y, c=xnor.color, marker='x')"
   ]
  },
  {
   "source": [
    "## ¿Cómo separamos el límite de decisión?\n",
    "\n",
    "En este problema, necesitaremos un límite de decisión no lineal para separar los ejemplos positivos y negativos, dado que una recta no sería posible.\n",
    "\n",
    "## Ajuste a la puerta lógica XNOR\n",
    "\n",
    "Tomaremos las siguientes puertas lógicas:\n",
    "\n",
    "- Ajuste a la puerta lógica $X_{1}$ AND $X_{2}$. Pesos en rojo.\n",
    "\n",
    "- Ajuste a la puerta lógica (NOT $ X_{1} $) AND (NOT $X_{2}$). Pesos en morado.\n",
    "\n",
    "- Ajuste a la puerta lógica $X_{1}$ OR $X_{2}$. Pesos en gris.\n",
    "\n",
    "Y asignaremos cada una de ellas a una neurona particular con las mismas entradas y pesos ($ X_{1} \\; y \\; X_{2}$).\n",
    "\n",
    "<img src=\"15.png\"></img>\n",
    "\n",
    "~ [Link util para dibujar redes neuronales](http://alexlenail.me/NN-SVG/index.html).\n",
    "\n",
    "Entonces, pasemos la red neuronal a la tabla de verdad.\n",
    "\n",
    "| $CAPA \\: \\: DE \\: \\: ENTRADA$ | $CAPA \\: \\: DE \\: \\: ENTRADA$ | $CAPA \\: \\: OCULTA$ | $CAPA \\: \\: OCULTA$ | $CAPA \\: \\: DE \\: \\: SALIDA$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ a_{1}^{2} $ | $ a_{2}^{2} $ | $ h_\\Theta(X) $ |\n",
    "| 0 | 0 | 0 | 1 | 1\n",
    "| 0 | 1 | 0 | 0 | 0\n",
    "| 1 | 0 | 0 | 0 | 0\n",
    "| 1 | 1 | 1 | 0 | 1\n",
    "\n",
    "De esta forma, con esta red neuronal, que tiene una capa de entrada, una capa oculta y otra de salida, terminamos con un límite de decisión no lineal complejo, que calcula la función XNOR.\n",
    "\n",
    "## Otra forma de representar la red neuronal\n",
    "\n",
    "Tenemos que la entrada la podíamos representar como $ \\Theta^{1} $, donde las matrices para AND, NOR y OR eran:\n",
    "\n",
    "- $AND$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-30 & 20 & 20\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $NOR$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "20 & -25 & -25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $OR$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-20 & 25 & 25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Entonces, podemos combinar las matrices para obtener el operador lógico XNOR (que nos brinda $True$ si $X_{1}$ o $X_{2}$ son $0 \\; o \\; 1$):\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "X_{0} \\\\ X_{1} \\\\ X_{2}\n",
    "\\end{bmatrix}\\rightarrow\\begin{bmatrix}\n",
    "a_{1}^{(2)} \\\\ a_{2}^{(2)}\n",
    "\\end{bmatrix}\\rightarrow\\begin{bmatrix}\n",
    "a_{1}^{(3)}\n",
    "\\end{bmatrix}\\rightarrow h_{\\Theta}(X)$\n",
    "\n",
    "Para la transición entre la primera capa y la segunda, usaremos la matriz $ \\Theta^{(1)} $ que combina los valores para $AND$ y $NOR$:\n",
    "\n",
    "$ \\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30 & 20 & 20 \\\\\n",
    "20 & -25 & -25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Y para la transición entre la segunda y la tercera capa, usaremos la matriz $\\Theta^{(2)}$ que utiliza los valores para $OR$:\n",
    "\n",
    "$ \\Theta^{(2)} = \\begin{bmatrix}\n",
    "-20 & 25 & 25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Escribamos todos los valores para nuestros nodos:\n",
    "\n",
    "$ a^{(2)} = g(\\Theta^{(1)}x) $\n",
    "\n",
    "$ a^{(3)} = g(\\Theta^{(2)}a^{(2)}) $\n",
    "\n",
    "$ h_{\\Theta}(X) = a^{(3)} $\n",
    "\n",
    "Y así es como obtenemos la puerta lógica XNOR usando una capa oculta con dos nodos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}