{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado en el video de <b>DOT.CSV</b>: [Redes Neuronales](https://www.youtube.com/watch?v=W8AeOXa_FqU) y curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Redes neuronales\n",
    "\n",
    "Las redes neuronales fueron desarrolladas como simulación de las neuronas o de las redes de neuronas en el cerebro. \n",
    "\n",
    "## Neuronas\n",
    "\n",
    "<img src=\"2.png\"></img>\n",
    "\n",
    "Las neuronas - <i>como la que está adjunta en la imagen</i> - son una perfecta analogía para entender la representación de la hipótesis de las redes neuronales.\n",
    "\n",
    "Las primeras cosas que llaman la atención es que, las neuronas, tienen un cuerpo celular morado, con ramificaciones, llamadas \"dentritas\". Estas dentritas son similares a nuestras entradas de la función, 'inputs' o X (características).\n",
    "\n",
    "Además, estos cables de entrada reciben inputs de otras ubicaciones.\n",
    "\n",
    "Finalmente, tenemos que estos cables de entrada se conectan a un cable de salida, denominado axón, cuya función es enviar señales a otras neuronas. Este 'output' vendría siendo nuestras salidas 'h(x)'.\n",
    "\n",
    "Cobra sentido, ¿no parece?\n",
    "\n",
    "## Unidad computacional\n",
    "\n",
    "En analogía a las neuronas biológicas, y en un nivel simple, una neurona es una <b>unidad computacional</b> que tiene un número de entradas. Es, a través de estos cables de entrada, que se reciben características con las cuales se realizan algunos cálculos, y luego, se envían estos resultados mediante su axón (output) a otros nodos o a otras neuronas del cerebro.\n",
    "\n",
    "## Neurona: unidad de logística\n",
    "\n",
    "En una red neuronal artificial, lo que implementamos en una computadora es un modelo muy simple respecto a lo que realizan las neuronas biológicas.\n",
    "\n",
    "Modelaremos una neurona como una unidad logística. Así que, en el caso de la representación del modelo:\n",
    "\n",
    "• El círculo amarillo representaría una función análoga del cuerpo de una neurona (los cálculos sobre los input que darán un output).\n",
    "\n",
    "• Nuestro resultado, $ h_ \\theta (x) $, vendría siendo el output gracias al axón (en analogía biológica).\n",
    "\n",
    "<img src=\"3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función de activación\n",
    "\n",
    "Por lo general, lo que representa el cálculo $ h_{\\theta}(x) $ es:\n",
    "\n",
    "- Dada las entradas $ X_{1}, X_{2} $ y $ X_{3} $ se aplica una función (que puede ser, por ejemplo, una regresión logística) $ h_{\\theta} $, resultando en $ h_{\\theta}(x) $.\n",
    "\n",
    "En este sentido, recordemos que de notebooks pasados...\n",
    "\n",
    "Nuestra función está parametrizada por nuestro vector de parámetros:\n",
    "\n",
    "$ \\theta = \\begin{pmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} $\n",
    "\n",
    "Y definida por nuestro vector de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias)\n",
    "\n",
    "Hasta ahora, en los notebooks solo se ha hablado acerca de los parámetros $ \\theta $ como $\\theta_{i}$. Sin embargo, y por lo general, en terminología de las redes neuronales, a estos parámetros se les conoce como 'pesos' del modelo:\n",
    "\n",
    "- En inglés, 'weight'.\n",
    "\n",
    "Por lo que es usual encontrar el vector de parámetros expresados en función de $ W $ en vez de $ \\theta $.\n",
    "\n",
    "## Red neuronal\n",
    "\n",
    "En el diagrama anterior definimos una neurona, que es la base de una red neuronal.\n",
    "\n",
    "Una red neuronal consiste en diferentes neuronas unidas. Específicamente, tenemos:\n",
    "\n",
    "- Capa de entrada X (donde se introducen las variables o características).\n",
    "\n",
    "    - $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Capas intermedias o ocultas (neuronas).\n",
    "\n",
    "    - $ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $, siendo el superíndice el número de capa y $ a_{0}^{2} = 1 $ (neurona de oscilación).\n",
    "\n",
    "- Capa de salida Y (resultado final de la hipótesis).\n",
    "\n",
    "    - $ h_{\\Theta} $ que es el valor o valores que calcula la hipótesis.\n",
    "\n",
    "Más adelante, veremos redes neuronales con más de una capa oculta, pero básicamente cualquier capa que no sea de entrada o salida será una capa oculta.\n",
    "\n",
    "## Notación 1\n",
    "\n",
    "Sí, lo sé. La notación es poco entendible al principio, pero créeme, te acostumbras, por lo que la odias al principio y luego la amas por su simplicidad.\n",
    "\n",
    "Sigamos.\n",
    "\n",
    "Arriba, si eres atento a los detalles, cambiamos $ \\theta $ por $ \\Theta $, y también, por supuesto, introducimos neuronas denotadas como $ a $, ¿por qué?\n",
    "\n",
    "Para explicar los cálculos específicos representados por una red neuronal, introducimos un poco más de notación:\n",
    "\n",
    "- Usaremos $ \\Theta^{(j)} $ para referirnos a los pesos de las neuronas de una determinada capa, siendo j la capa.\n",
    "\n",
    "- Además, las unidades de activación (neurona) serán definidas como $ a_{i}^{(j)} $ de la capa j y unidad i.\n",
    "\n",
    "- En sí, el nombre \"neurona\" no deja de ser un nombre extraordinariamente interesante para referirnos a una \"función\".\n",
    "\n",
    "## Notación 2\n",
    "\n",
    "Practiquemos un poco la notación. Sea:\n",
    "\n",
    "$ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $\n",
    "\n",
    "¿Qué nos dice la neurona $ a_{1}^{2} $?\n",
    "\n",
    "- Pues, implica que, es la primera unidad de la capa dos, realizando la activación de dicha unidad. Por cierto, con \"activación\" nos referimos al valor calculado por la neurona, es decir, su resultado. \n",
    "\n",
    "¿Qué nos dice $ \\Theta^{(j)} $?\n",
    "\n",
    "Como habíamos definido, $ \\Theta^{(j)} $ se encarga de parametrizar la red neuronal. En sí, es una matriz de ondas, la que controla el mapeo de la función de las capas.\n",
    "\n",
    "## El mapeo de la función de las capas\n",
    "\n",
    "¿Qué es el mapeo de la función de capas? \n",
    "\n",
    "Recordemos lo que hicimos en la regresión lineal, o en la regresión logística. ¿No es que intentamos ajustar una función a los datos? Pues sí, y esa es la base del \"mapeo\".\n",
    "\n",
    "Ésto lo define Hans Lehnert Merino, en [“Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales\"](https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y).\n",
    "\n",
    "- \"Lo que busca un método de machine learning es encontrar un mapeo entre\n",
    "una entrada X y una salida Y. Esto es precisamente lo que realiza una red neuronal: buscar una aproximación de una función mediante un modelo parametrizado\"\n",
    "\n",
    "Lehnert Merino, H. L. M. (2019, abril). Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales. Universidad Técnica Federico Santa María. Recuperado de https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los cálculos\n",
    "\n",
    "Así que, teniendo definido los puntos anteriores, presentaremos los cálculos:\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Definiremos las matrices de mapeo para aclarar mejor la fórmula (recordamos que debemos trasponerlas para realizar los cálculos). Entonces, definimos de la segunda capa de la red neuronal, pero, con la primera capa de neuronas.\n",
    "\n",
    "La matriz de mapeo acotada (1) $ \\Theta^{(1)} = \\begin{pmatrix} \\Theta_{1}^{(1)} & \\Theta_{2}^{(1)} & \\Theta_{3}^{(1)} \\end{pmatrix} $, en donde:\n",
    "\n",
    "- $ \\Theta_{1}^{(1)} = \\begin{pmatrix} \\Theta_{10}^{(1)} \\\\ \\Theta_{11}^{(1)} \\\\ \\Theta_{12}^{(1)} \\\\ \\Theta_{13}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{2}^{(1)} = \\begin{pmatrix} \\Theta_{20}^{(1)} \\\\ \\Theta_{21}^{(1)} \\\\ \\Theta_{22}^{(1)} \\\\ \\Theta_{23}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{3}^{(1)} = \\begin{pmatrix} \\Theta_{30}^{(1)} \\\\ \\Theta_{31}^{(1)} \\\\ \\Theta_{32}^{(1)} \\\\ \\Theta_{33}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "De forma que la matriz de mapeo completa, que define la segunda capa, se construye a partir de reemplazar las matrices anteriores en la matriz acotada expuesta en (1). Esta matriz quedará de 3 x 4 dimensiones en este caso particular.\n",
    "\n",
    "Definimos de la tercera capa en la red neuronal, pero, con la segunda capa de neuronas:\n",
    "\n",
    "$ \\Theta_{1}^{(2)} = \\begin{pmatrix} \\Theta_{10}^{(2)} \\\\ \\Theta_{11}^{(2)} \\\\ \\Theta_{12}^{(2)} \\\\ \\Theta_{13}^{(2)} \\end{pmatrix} $\n",
    "\n",
    "Y la matriz de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $\n",
    "\n",
    "Siendo definidas nuestras neuronas en la capa oculta como:\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "$ a_{2}^{(2)} = g(\\Theta_{20}^{2}X_{0}+\\Theta_{21}^{2}X_{1}+\\Theta_{22}^{2}X_{2} + \\Theta_{23}^{2}X_{3}) $\n",
    "\n",
    "$ a_{3}^{(2)} = g(\\Theta_{30}^{3}+\\Theta_{31}^{3}X_{1}+\\Theta_{32}^{3}X_{2} + \\Theta_{33}^{3}X_{3}) $\n",
    "\n",
    "Y una vez realizados su cálculos, se calcula la hipótesis en la tercera capa de la red neuronal, pero con la segunda capa de neuronas:\n",
    "\n",
    "$ h_{\\Theta}(X) = a_{1}^{(3)} = g(\\Theta_{10}^{2}a_{0}^{(2)}+\\Theta_{11}^{2}a_{1}^{(2)}+\\Theta_{12}^{2}a_{2}^{(2)} + \\Theta_{13}^{2}a_{3}^{(2)}) $\n",
    "\n",
    "Bastante jaleo, ¿no es cierto? Evidentemente, el procedimiento de transponer se realiza en base a la necesidad de operar las matrices (contenidos que se exploran en álgebra matricial).\n",
    "\n",
    "## ¿Cómo entendemos una neurona particular?\n",
    "\n",
    "Por ejemplo, la neurona $a_{1}^{2}$ es igual a la función sigmoide o a la función de activación sigmoidal, aplicada a la siguiente combinación lineal de sus entradas.\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "Y así, sucesivamente, para cada neurona.\n",
    "\n",
    "## La dimensionalidad\n",
    "\n",
    "Si una red neuronal tiene:\n",
    "\n",
    "- Un número de $ s_{j} $ unidades en la capa j.\n",
    "\n",
    "- Un número de $ s_{j+1} $ unidades en la capa $ j + 1 $.\n",
    "\n",
    "Entonces, $\\Theta^{(j)}$ tendrá una dimensión de $ s_{j+1} \\times (s_{j} + 1) $.\n",
    "\n",
    "## Las conexiones\n",
    "\n",
    "En sí, todos los cálculos en una red neuronal se encuentran intrínsicamente relacionados unos con otros. \n",
    "\n",
    "Los cálculos de la segunda capa se explican a través de la primera capa:\n",
    "\n",
    "- La primera capa corresponden a las entradas (input), X.\n",
    "\n",
    "- La segunda capa realiza el cálculo sobre las variables $ X $ de la primera capa, parametrizando con $ \\Theta_{i}^{(1)} $ y resultando en $ a_{i}^{(2)} $, aplicando la combinación lineal de sus entradas y la función sigmoide.\n",
    "\n",
    "- La tercera capa realiza el cálculo sobre las variables $ a_{i}^{(2)} $ originadas en la capa anterior, parametrizando con $ \\Theta_{i}^{(2)}$.\n",
    "\n",
    "Así que, tras hablar de lo que hacen las tres unidades ocultas para calcular sus valores, hablaremos del último espinal: la última unidad.\n",
    "\n",
    "## La salida\n",
    "\n",
    "La última unidad, calcula $h_{\\theta}(X)$ que es Y, de forma que también podemos escribirlo como:\n",
    "\n",
    "- La salida es igual a $h_{\\theta}(X)=Y=a_{1}^{3}$, que es la función sigmoide evaluada en la combinación de parámetros.\n",
    "\n",
    "De esta forma, la función $ h_{\\theta}(X)$ mapea los valores de entrada $ X $, que, en algunos espacios o disposiciones $ Y $, definirá, con cierta precisión, un conjunto de hipótesis, llegando a diferentes funciones de mapeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La búsqueda de la eficiencia\n",
    "\n",
    "En las últimas seldas, vimos la definición matemática sobre cómo representar y calcular las hipótesis utilizadas por las redes neuronales. \n",
    "\n",
    "A continuación, veremos <b>cómo realizar este cálculo de forma eficiente</b>, con la implementación vectorizada.\n",
    "\n",
    "En segundo lugar, Andrew NG. desea empezar a darnos una intuición sobre por qué las representaciones de las redes neuronales son una buena idea para ayudarnos con hipótesis complejas no lineales. Es decir, similar a lo que vimos en regresión logística en situaciones donde la hipótesis podía asemejarse a una circunferencia, o a curvas cualesquiera.\n",
    "\n",
    "## La implementación vectorizada\n",
    "\n",
    "<img src=\"5.png\"></img>\n",
    "\n",
    "Consideremos la red neuronal de arriba. \n",
    "\n",
    "Anteriormente, habíamos dicho que la secuencia de pasos que necesitamos para calcular la salida de una hipótesis en estas ecuaciones:\n",
    "\n",
    "- Calcular los valores de activación de las unidades ocultas con los valores de entrada.\n",
    "\n",
    "- Utilizando las salidas anteriores, calculamos para la última unidad de la espinal, obteniendo el resultado de la red neuronal $h_{\\theta}(X)$.\n",
    "\n",
    "Entonces, ahora es cuando definiremos la implementación vectorizada, debiendo introducir términos adicionales.\n",
    "\n",
    "Los términos subrayados los redefiniremos, por lo que, sea la combinación lineal ponderada de la funciones de activación:\n",
    "\n",
    "- Recordemos de [notebooks anteriores](https://github.com/adinamarca/notebooks/blob/main/PY/ML/3_regresion_logistica/notebook.ipynb). \n",
    "\n",
    "En dicho notebook, definimos $ h(\\Theta^{T}X) $, donde $ Z = \\Theta^{T}X $. \n",
    "\n",
    "Utilizaremos una definición similar para las hipótesis de las redes neuronales, definiendo por cada combinación lineal de cada neurona como $ Z_{i}^{j} $:\n",
    "\n",
    "- Donde $ i $ indica el número de la neurona.\n",
    "\n",
    "- Donde $ j $ corresponde al número de capa.\n",
    "\n",
    "De esta forma, redefinimos:\n",
    "\n",
    "- Sea $ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $, redefinimos $ Z_{1}^{2} = \\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3} $, de forma que quedamos con $ a_{1}^{(2)} = g(Z_{1}^{2}) $.\n",
    "\n",
    "- Repitiendo el paso anterior, quedamos con $ a_{2}^{(2)} = g(Z_{2}^{2}) $.\n",
    "\n",
    "- Y $ a_{3}^{(2)} = g(Z_{3}^{2}) $.\n",
    "\n",
    "Observamos que, estas nuevas definiciones, corresponden de forma sospechosamente similar a la operación $ \\Theta^{(i)}*X $ (en forma análoga a $\\Theta^{T}X$, pero estando $ \\Theta $ ya transpuesta).\n",
    "\n",
    "Mediante esta observación, seremos capaces de vectorizar este cálculo de la red neuronal:\n",
    "\n",
    "- Recordamos nuestro vector de características como $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Definimos nuestro vector de combinaciones lineales (que definen, cada uno, una función de mapeo a través de los parámetros $ \\Theta $) como $ Z^{(2)} = \\begin{pmatrix} Z_{1}^{2} \\\\ Z_{2}^{2} \\\\ Z_{3}^{2} \\end{pmatrix} $.\n",
    "\n",
    "¡Necesitamos un valor más! Que es la unidad de sesgo (bias unit). Para solucionar este problema, lo que haremos será sumar $ a_{0}^{2} $:\n",
    "\n",
    "- La unidad de oscilación la definimos como $ a_{0}^{2} = 1$.\n",
    "\n",
    "Ahora tendremos que $ a^{(2)} $ será un vector de cuatro dimensiones.\n",
    "\n",
    "Finalmente, para calcular el valor real de la salida de las hipótesis:\n",
    "\n",
    "- Queda $h_{\\Theta}(X)=a_{1}^{(3)}=g(\\Theta_{10}^{(2)}a_{0}^{(2)}+\\Theta_{11}^{(2)}a_{1}^{(2)}+\\Theta_{12}^{(2)}a_{2}^{(2)}+\\Theta_{13}^{(2)}a_{3}^{(2)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones no lineales complejas\n",
    "\n",
    "En esta y la próxima celda trabajaremos con un ejemplo detallado que muestra cómo una red neuronal puede calcular una función no lineal compleja de entrada.\n",
    "\n",
    "<img src=\"6.png\"></img>\n",
    "\n",
    "Consideremos el problema de la imagen anterior, donde tenemos $ X_{1} $ y $ X_{2} $ como variables de entrada, lo que nos define que tenemos solo dos valores posibles: valores binarios.\n",
    "\n",
    "<img src=\"7.png\"></img>\n",
    "\n",
    "Podemos considerar la imagen anterior como una versión simplificada de un problema más complejo. En este sentido, se debería aprender un límite de decisión no lineal, de forma que debamos separar los ejemplos positivos y negativos.\n",
    "\n",
    "## Puertas lógicas\n",
    "\n",
    "<img src=\"8.png\"></img>\n",
    "\n",
    "~ Imagen por [Alejandro García](http://aletecnocampello.blogspot.com/2015/12/puertas-logicas.html\n",
    ").\n",
    "\n",
    "Antes de continuar, es recomendable leer acerca de las puertas lógicas (y no, no es para nada necesario aprenderse la nomenclatura).\n",
    "\n",
    "Una puerta lógica, o compuerta lógica, es un dispositivo electrónico con una función booleana u otras funciones como sumar o restar, incluyen o excluyen según sus propiedades lógicas (extraído desde [Wikipedia](https://es.wikipedia.org/wiki/Puerta_l%C3%B3gica)).\n",
    "\n",
    "- [Puerta lógica XOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XOR). Implica una salida verdadera (1 o TRUE) si una, y solo una de las entradas a la puerta es verdadera.\n",
    "\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XOR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 0\n",
    "\n",
    "- [Puerta lógica XNOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XNOR). Es la inversa de la puerta lógica XOR.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XNOR \\; X_{2} $ |\n",
    "| 0 | 0 | 1\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica OR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_OR). Es verdadero si al menos una entrada es verdadera.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;OR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica AND en Wikipedia](https://es.wikipedia.org/wiki/Puerta_AND). Es verdadero si y solo si las dos entradas son verdaderas.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;AND \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica NOT en Wikipedia](https://es.wikipedia.org/wiki/Puerta_NOT). Es verdadero si la entrada es falsa.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $\\;NOT \\; X_{1} $ |\n",
    "| 0 | 1\n",
    "| 1 | 0\n",
    "\n",
    "## Redes neuronales en funciones no lineales complejas\n",
    "\n",
    "Entonces, ¿cómo podemos hacer que una red neuronal se ajuste a este problema?\n",
    "\n",
    "Específicamente, lo que está calculando Y es equivalente a: $ Y = X_{1} \\;XNOR \\; X_{2} $. Es decir, tendremos ejemplos positivos siempre que ambas entradas sean positivas o ambas sean falsas.\n",
    "\n",
    "<img src=\"9.png\"></img>\n",
    "\n",
    "En este sentido, queremos saber si podemos hacer que una red neuronal sea capaz de ajustarse a este tipo de conjuntos de aprendizaje.\n",
    "\n",
    "## ¿Qué es la hipótesis no lineal compleja? \n",
    "\n",
    "Cuando nos referimos con hipótesis no lineales complejas, podemos utilizar el mismo ejemplo de la puerta lógica XNOR. En dicho ejemplo sería imposible trazar una recta que realizase el límite de decisión correctamente a ambos conjuntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste a puerta lógica AND\n",
    "\n",
    "Con el fin de construir una red que se ajuste al ejemplo XNOR, vamos a comenzar con un ejemplo ligeramente más simple: con una puerta lógica AND.\n",
    "\n",
    "<img src=\"10.png\"></img>\n",
    "\n",
    "Concretamente, digamos que tenemos las entradas $ X_{1} $ y $ X_{2} $ que, nuevamente, son binarias {$1 \\; , \\; 0$}.\n",
    "\n",
    "Entonces, ¿podremos hacer que una sola red neuronal se ajuste a este problema? En orden para ello, ¡consideraremos las unidades de oscilación!\n",
    "\n",
    "<img src=\"11.png\"></img>\n",
    "\n",
    "Ahora, asignaremos algunos valores a los pesos de los parámetros de la red.\n",
    "\n",
    "<img src=\"12.png\"></img>\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-30, 20, 20] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-30 + 20X_{1} + 20X_{2}) $.\n",
    "\n",
    "A veces, es conveniente dibujar los pesos de los parámetros en el diagrama de la red neuronal. \n",
    "\n",
    "Solo para recordarnos, la función de activación sigmoidea $ g(Z) $ comienza en $0$, se eleva lentamente, pasando por $ 0.5 $ y asintotando en $ 1 $.\n",
    "\n",
    "En este sentido:\n",
    "\n",
    "- Si $ Z = 4.6 $, entonces la función sigmoidea será de 0.99, y por otro lado, si evaluamos en $ -4.6 $ la función sigmoidea será de 0.01, lo cual es muy cercano a 0.\n",
    "\n",
    "<img src=\"13.png\"></img>\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -30 | 0\n",
    "| 0 | 1 | -10 | 0\n",
    "| 1 | 0 | -10 | 0\n",
    "| 1 | 1 | 10 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La importancia de los pesos\n",
    "\n",
    "Notar como, con los pesos de los parámetros, fuimos capaces de ajustar a este problema concreto, una sola neurona, dando el resultado esperado en la <b>tabla de verdad</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste a puerta lógica OR\n",
    "\n",
    "Esta vez, construyamos una neurona para la puerta lógica OR.\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-20, 25, 25] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-20 + 25X_{1} + 25X_{2}) $.\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -20 | 0\n",
    "| 0 | 1 | 5 | 1\n",
    "| 1 | 0 | 5 | 1\n",
    "| 1 | 1 | 30 | 1\n",
    "\n",
    "## Las neuronas individuales\n",
    "\n",
    "De esta forma, concluimos cómo las neuronas individuales pueden utilizarse para funciones lógicas como AND o OR, entre otras.\n",
    "\n",
    "En la siguiente celda abordaremos ejemplos más complejos, llegando a trabajar con más de una neurona, formando una red neuronal capaz de usarse para calcular funciones más complejas como XOR o XNOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un paso más allá\n",
    "\n",
    "En la última sección vimos como una red neuronal puede usarse para calcular las puertas lógicas $ AND $ y $ OR $.\n",
    "\n",
    "En esta celda trabajaremos en el ejemplo de cómo calcular hipótesis complejas no lineales.\n",
    "\n",
    "## Ajuste a puerta lógica NOT\n",
    "\n",
    "<img src=\"14.png\"></img>\n",
    "\n",
    "En este sentido, podríamos hacer que una red neuronal calcule una negación, lo que significa una puerta lógica $ NOT X_{1} $. Escribiremos una hipótesis asociada a esta red.\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}] = [5, -20] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(5-20X_{1}) $.\n",
    "\n",
    "Veamos entonces los <b>dos posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | EVALUACIÓN | OUTPUT\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 5 | 1\n",
    "| 1 | -15 | 0\n",
    "\n",
    "Entonces, cuando $ X_{1} = 0 $:\n",
    "\n",
    "- Estaremos calculando $ h_{\\Theta}(X) = g(5) = \\; aprox. \\; 1 $.\n",
    "\n",
    "Cuando $ X_{1} = 1 $:\n",
    "\n",
    "- Estaremos calculando $ h_{\\Theta}(X) = g(-15) = \\; aprox. \\; 0 $.\n",
    "\n",
    "Si observamos, estaremos calculando la puerta lógica de una negación, dado que al dar el valor $True$ al INPUT, el OUTPUT nos devuelve $False$. Por otro lado, si le brindamos $False$, nos devuelve $True$ ($True = 1$ y $False = 0$).\n",
    "\n",
    "Así que, para realizar una puerta lógica NOT, la idea general es colocar un gran peso negativo delante de la variable que se desea anular.\n",
    "\n",
    "De esta forma, hagamos ejemplo más complejo que implique una combinación de puertas lógicas.\n",
    "\n",
    "## Ajuste a la puerta lógica ($NOT \\; X_{1}$) $AND$ ($NOT \\; X_{2}$)\n",
    "\n",
    "Antes, definamos las negaciones.\n",
    "\n",
    "Si $ (NOT \\; False)  \\; AND \\;  (NOT \\; False) $, entonces $ (True) \\; AND \\; (True) = True $.\n",
    "\n",
    "Es decir, solo si tenemos dos $ NOT \\; False $, tendríamos dos $ True $, y $ True \\; AND \\; True $. Por lo que definiremos dos grandes pesos negativos encima de un positivo.\n",
    "\n",
    "La idea aquí es pensar:\n",
    "\n",
    "- Un bias negativo nos dará un sesgo negativo. Por lo que si lo anulamos con pesos positivos, tendremos $ True $.\n",
    "\n",
    "- Un bias positivo nos dará un sesgo positivo. Por lo que si lo anulamos con pesos negativos, tendremos $ False $.\n",
    "\n",
    "Definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [20, -25, -25] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(20 - 25X_{1} - 25X_{2}) $.\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ \\; X_{1} $ | $ \\; X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| $ NOT \\;  0 $ | $ NOT \\;  0 $ | -30 | 0\n",
    "| $ NOT \\;  0 $ | $ NOT \\;  1 $ | -5 | 0\n",
    "| $ NOT \\;  1 $ | $ NOT \\;  0 $ | -5 | 0\n",
    "| $ NOT \\;  1 $ | $ NOT \\;  1 $ | 20 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Y el ajuste a XNOR?\n",
    "\n",
    "Recordemos la puerta lógica XNOR:\n",
    "\n",
    "- [Puerta lógica XNOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XNOR). Es la inversa de la puerta lógica XOR.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XNOR \\; X_{2} $ |\n",
    "| 0 | 0 | 1\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "El gráfico se vería de la siguiente manera:\n",
    "\n",
    "- Azul: ejemplos negativos.\n",
    "- Rojo: ejemplos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos matplotlib (gráficos)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importamos pandas (manipulación de datos) y numpy (álgebra avanzada)\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e5dacddb08>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xnordata = {'x':[0, 0, 1, 1], 'y':[0, 1, 0, 1], 'color':['r','b','b','r']}\n",
    "xnor = pd.DataFrame(data=xnordata)\n",
    "plt.scatter(x=xnor.x, y=xnor.y, c=xnor.color, marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo separamos el límite de decisión?\n",
    "\n",
    "En este problema, necesitaremos un límite de decisión no lineal para separar los ejemplos positivos y negativos, dado que una recta no sería posible.\n",
    "\n",
    "## Ajuste a la puerta lógica XNOR\n",
    "\n",
    "Tomaremos las siguientes puertas lógicas:\n",
    "\n",
    "- Ajuste a la puerta lógica $X_{1}$ AND $X_{2}$. Pesos en rojo.\n",
    "\n",
    "- Ajuste a la puerta lógica (NOT $ X_{1} $) AND (NOT $X_{2}$). Pesos en morado.\n",
    "\n",
    "- Ajuste a la puerta lógica $X_{1}$ OR $X_{2}$. Pesos en gris.\n",
    "\n",
    "Y asignaremos cada una de ellas a una neurona particular con las mismas entradas y pesos ($ X_{1} \\; y \\; X_{2}$).\n",
    "\n",
    "<img src=\"15.png\"></img>\n",
    "\n",
    "~ [Link util para dibujar redes neuronales](http://alexlenail.me/NN-SVG/index.html).\n",
    "\n",
    "Entonces, pasemos la red neuronal a la tabla de verdad.\n",
    "\n",
    "| $CAPA \\: \\: DE \\: \\: ENTRADA$ | $CAPA \\: \\: DE \\: \\: ENTRADA$ | $CAPA \\: \\: OCULTA$ | $CAPA \\: \\: OCULTA$ | $CAPA \\: \\: DE \\: \\: SALIDA$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ a_{1}^{2} $ | $ a_{2}^{2} $ | $ h_\\Theta(X) $ |\n",
    "| 0 | 0 | 0 | 1 | 1\n",
    "| 0 | 1 | 0 | 0 | 0\n",
    "| 1 | 0 | 0 | 0 | 0\n",
    "| 1 | 1 | 1 | 0 | 1\n",
    "\n",
    "De esta forma, con esta red neuronal, que tiene una capa de entrada, una capa oculta y otra de salida, terminamos con un límite de decisión no lineal complejo, que calcula la función XNOR.\n",
    "\n",
    "## Otra forma de representar la red neuronal\n",
    "\n",
    "Tenemos que la entrada la podíamos representar como $ \\Theta^{1} $, donde las matrices para AND, NOR y OR eran:\n",
    "\n",
    "- $AND$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-30 & 20 & 20\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $NOR$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "20 & -25 & -25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $OR$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-20 & 25 & 25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Entonces, podemos combinar las matrices para obtener el operador lógico XNOR (que nos brinda $True$ si $X_{1}$ o $X_{2}$ son $0 \\; o \\; 1$):\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "X_{0} \\\\ X_{1} \\\\ X_{2}\n",
    "\\end{bmatrix}\\rightarrow\\begin{bmatrix}\n",
    "a_{1}^{(2)} \\\\ a_{2}^{(2)}\n",
    "\\end{bmatrix}\\rightarrow\\begin{bmatrix}\n",
    "a_{1}^{(3)}\n",
    "\\end{bmatrix}\\rightarrow h_{\\Theta}(X)$\n",
    "\n",
    "Para la transición entre la primera capa y la segunda, usaremos la matriz $ \\Theta^{(1)} $ que combina los valores para $AND$ y $NOR$:\n",
    "\n",
    "$ \\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30 & 20 & 20 \\\\\n",
    "20 & -25 & -25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Y para la transición entre la segunda y la tercera capa, usaremos la matriz $\\Theta^{(2)}$ que utiliza los valores para $OR$:\n",
    "\n",
    "$ \\Theta^{(2)} = \\begin{bmatrix}\n",
    "-20 & 25 & 25\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Escribamos todos los valores para nuestros nodos:\n",
    "\n",
    "$ a^{(2)} = g(\\Theta^{(1)}x) $\n",
    "\n",
    "$ a^{(3)} = g(\\Theta^{(2)}a^{(2)}) $\n",
    "\n",
    "$ h_{\\Theta}(X) = a^{(3)} $\n",
    "\n",
    "Y así es como obtenemos la puerta lógica XNOR usando una capa oculta con dos nodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación multiclase\n",
    "\n",
    "Ahora, utilizaremos las redes neuronales para hacer clasificación multiclase, en donde podríamos tener más de una categoría de las cuales podemos estar intentando distinguir.\n",
    "\n",
    "En la última celda estábamos tratando con el problema de reconocer la escritura de dígitos a mano, y de hecho, eso era un problema de clasificación multiclase, puesto habían diez categorías posibles:\n",
    "\n",
    "- Reconocer los dígitos del 0 al 9.\n",
    "\n",
    "La forma en que queremos hacer clasificación multiclase, con una red neuronal, es a través de una extensión del método 'uno vs/ todos', que fue el método visto en la regresión logística, del notebook anterior.\n",
    "\n",
    "<img src=\"16.png\"></img>\n",
    "\n",
    "Así que, digamos que tenemos un ejemplo de visión de computador, en donde, reconoceremos cuatro categorías de objetos:\n",
    "\n",
    "- Un peatón.\n",
    "\n",
    "- Un auto.\n",
    "\n",
    "- Una motocicleta.\n",
    "\n",
    "- Un camión.\n",
    "\n",
    "En esta situación, necesitaremos una red neuronal con cuatro unidades de salida, para que la red neuronal devolviera un vector de cuatro números.\n",
    "\n",
    "<img src=\"17.png\"></img>\n",
    "\n",
    "Así que, ahora la salida es un vector de cuatro valores, y lo que intentaremos hacer es lograr que:\n",
    "\n",
    "- La primera unidad clasifique si la imagen es un peatón.\n",
    "\n",
    "- Si no lo es, la segunda unidad clasifica si es un auto.\n",
    "\n",
    "- Y si nuevamente no corresponde, la tercera unidad clasificará si es una motocicleta.\n",
    "\n",
    "- Si nuevamente sucede que no coincide, la cuarta unidad deberá consultar si es un camión.\n",
    "\n",
    "Es decir, y en resumen:\n",
    "\n",
    "- Si es peatón.\n",
    "\n",
    "$ [1, 0, 0, 0] $\n",
    "\n",
    "- Si es un auto.\n",
    "\n",
    "$ [0, 1, 0, 0] $\n",
    "\n",
    "- Si es una motocicleta.\n",
    "\n",
    "$ [0, 0, 1, 0] $\n",
    "\n",
    "- Si es un camión.\n",
    "\n",
    "$ [0, 0, 0, 1] $\n",
    "\n",
    "Así que, es similar al método 'uno vs/ todos', del que hablábamos cuando estábamos describiendo la regresión logística. En este sentido, aquí tenemos cuatro clasificadores de regresión logística, donde cada uno está intentando reconocer una de las cuatro clases entre las que queremos distinguir.\n",
    "\n",
    "<img src=\"18.png\"></img>\n",
    "\n",
    "En esencia, la red neuronal se vería como arriba, con cuatro unidades de salida, y estas son las que queremos que sean $ h_{\\Theta} $ cuando tengamos diferentes imágenes.\n",
    "\n",
    "## Representamos el conjunto de entrenamiento\n",
    "\n",
    "Así que, cuando tengamos un conjunto de entrenamiento con diferentes imágenes de peatones, autos, motocicletas y camiones:\n",
    "\n",
    "- Lo que haremos será representar $ Y(i) $ como $ [1, 0, 0, 0] $, $ [0, 1, 0, 0] $, $ [0, 0, 1, 0] $, $ [0, 0, 0, 1] $, dependiendo de la imagen que le corresponda en $ X(i) $.\n",
    "\n",
    "Por lo que, una forma de representar el conjunto de entrenamiento sería:\n",
    "\n",
    "- Un par $ [X(i), Y(i)] $.\n",
    "\n",
    "Donde $ X(i) $ es una imagen de los cuatro objetos.\n",
    "\n",
    "Y $ Y(i) $ será uno de los vectores $ [1, 0, 0, 0] $, $ [0, 1, 0, 0] $, $ [0, 0, 1, 0] $, $ [0, 0, 0, 1] $, dependiendo de la imagen que le corresponda en $ X(i) $.\n",
    "\n",
    "De esta forma, esperamos que podamos encontrar una forma de hacer que nuestra red neuronal devuelva algún valor de uno de los cuatro objetos, siendo $ Y(i) $ uno de estos vectores. De esta forma, $ h_{\\Theta}(X) $ e $Y(i)$ serán vectores de cuatro dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Al fin la hora de programar! (Cortesía de DOT.CSV)\n",
    "\n",
    "En este sentido, DOT.CSV nos enseñará a programar una red neuronal DESDE CERO. \n",
    "\n",
    "Pero como ya sabemos, y lo más normal, es que estas implementaciones no se realicen desde cero, sino que se utilicen bibliotecas como SKLEARN, PYTORCH, u otras. Y pues, claro, SKLEARN ya la hemos utilizado anteriormente (como en la regresión logística)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema\n",
    "\n",
    "Clasificaremos a un grupo y al otro.\n",
    "\n",
    "<img src=\"19.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento numérico\n",
    "import numpy as np\n",
    "\n",
    "# Para manipulación de data\n",
    "import pandas as pd\n",
    "\n",
    "# Expande numpy\n",
    "import scipy as sc\n",
    "\n",
    "# Para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crearemos la nube circular de datos\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (características: 2) corresponde a: \n",
      " \n",
      " [[ 0.06858903 -1.00369209]\n",
      " [-0.09049054 -0.49259069]\n",
      " [-0.00221191 -1.0026593 ]\n",
      " ...\n",
      " [-0.27375991 -0.95578922]\n",
      " [ 0.66455449  0.74914011]\n",
      " [-0.93749696 -0.36077657]] \n",
      " \n",
      " Y (binaria: un círculo u el otro) corresponde a: \n",
      " \n",
      " [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Para ver de qué va la función\n",
    "# make_circles?\n",
    "\n",
    "# Número de registros (observaciones)\n",
    "n = 1000\n",
    "\n",
    "# Cuántas características tendremos sobre los registros de los datos (variables). Utilizaremos dos para hacer más simple la visualización, pero en ML se suele contar con un número muchísimo mayor de variables\n",
    "p = 2\n",
    "\n",
    "# Un ejemplo sería, de 500 personas, tenemos dos características que analizaremos\n",
    "\n",
    "# Generamos el dataset (factor: distancia entre círculos, noise: ruido o variabilidad para añadir dificultad)\n",
    "X, Y = make_circles(n_samples=n, factor=0.5, noise=0.05)\n",
    "\n",
    "Y = Y[:, np.newaxis]\n",
    "\n",
    "X, Y_e = make_circles(n_samples=n, factor=0.5, noise=0.01)\n",
    "\n",
    "# Vemos los datos\n",
    "print(\"\"\"X (características: 2) corresponde a: \\n \\n {} \\n \\n Y (binaria: un círculo u el otro) corresponde a: \\n \\n {}\"\"\".format(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Apreciemos como encontramos las variables en dos columnas:\n",
    "\n",
    "- $X[:,0]$ corresponde a la primera columna.\n",
    "\n",
    "\n",
    "- $X[:,1]$ corresponde a la segunda columna.\n",
    "\n",
    "\n",
    "- La coordenada $(X[n,0] \\:, \\:X[n,1])$ corresponde a un punto de alguno de los dos círculos con $ n \\: \\epsilon \\: \\mathbb{R} $.\n",
    "\n",
    "\n",
    "- $ Y[n] $ corresponde al valor booleano de si una determinada coordenada $(X[n,0] \\:, \\:X[n,1])$ corresponde al círculo exterior o al interior, con $ n \\: \\epsilon \\: \\mathbb{R} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Pero qué son los array de Numpy? ¿Por qué no ocupamos Pandas?\n",
    "\n",
    "Hasta el momento, puede haber cierta confusión. Pero hagamos un ejercicio:\n",
    "\n",
    "- $X[:,0]$ corresponde a la columna con la primera variable (entrada $X_{1}$).\n",
    "\n",
    "- $X[:,1]$ corresponde a la segunda variable (entrada $X_{2}$).\n",
    "\n",
    "- $Y$ corresponde al output (salida $h_{\\Theta}(X)$).\n",
    "\n",
    "Tanto X como Y están almacenados en dos variables distintas, donde X contiene las dos variables de entrada en dos columnas, e Y contiene la salida.\n",
    "\n",
    "El punto es que no está todo contenido en un solo DataFrame, que es a lo que estamos acostumbrados. En ese sentido, podríamos almacenar todo en un DataFrame.\n",
    "\n",
    "En sí, el uso de una librería o la otra es decisión arbitraria, pero veamos la comparación entre una u otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X_1       X_2  Y\n",
      "0  0.935347 -0.225024  0\n",
      "1 -0.421967  0.315725  1\n",
      "2  0.698766  0.660232  0\n",
      "3  1.080685 -0.216482  0\n",
      "4  0.342939 -1.051592  0\n",
      "[ 0.93534727 -0.42196745  0.69876636  1.08068536  0.34293923] [-0.22502362  0.31572524  0.66023205 -0.21648188 -1.05159197] [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Definimos el DataFrame\n",
    "df_datos = pd.DataFrame({'X_1': X[:,0], 'X_2': X[:,1], 'Y': Y_e})\n",
    "\n",
    "# Imprimimos los primeros cinco datos del DataFrame\n",
    "print(df_datos.head())\n",
    "\n",
    "# Comparamos con Numpy (seleccionamos los 5 primeros datos)\n",
    "print(X[:,0][0:5], X[:,1][0:5], Y_e[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas y Numpy\n",
    "\n",
    "Es decir, no considerando el rendimiento, Pandas nos ofrece mayores posibilidades de indexación, como otros métodos útiles. De hecho, ¡Pandas está hecho en base a Numpy! Pero, en estricto rigor:\n",
    "\n",
    "- Los DataFrames de Pandas pueden almacenar información homogénea y son mutables (modificables).\n",
    "\n",
    "- Los array de Numpy pueden almacenar información heterogénea y son mutables (modificables).\n",
    "\n",
    "El uso de una librería o la otra depende de lo que deseemos hacer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra definición de REDDIT\n",
    "\n",
    "\"I like to think of pandas as SQL tables and numpy as C arrays. Both on steroids.\n",
    "\n",
    "If you need 3+dimensions, you go for numpy. For 2d arrays you probably want pandas but numpy with structured columns and dtypes can also work.\n",
    "\n",
    "If you need query-like operations you go for pandas. Otherwise for heavy computations you will probably prefer numpy but vectorized calculations can also work on pandas.\n",
    "\n",
    "The best way to understand is pick a simple problem and solve it with both libraries.\"\n",
    "\n",
    "~ FROM [Reddit](https://www.reddit.com/r/Python/comments/7awtbb/what_are_the_advantages_of_pandas_dataframe_vs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordando\n",
    "\n",
    "En este sentido, vemos que nuestro array almacena:\n",
    "\n",
    "- $X[:, 0]$, que es la primera columna con la primera variable.\n",
    "\n",
    "- $X[:, 1]$, que es la segunda columna con la segunda variable.\n",
    "\n",
    "- $Y$, que, debería ser la tercera columna del DataFrame, pero la tenemos separada en otra variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficando la solución\n",
    "\n",
    "Lo que necesitamos es que nuestra red neuronal separe, en dos clases diferentes, las nubes de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEJCAYAAABR4cpEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABYa0lEQVR4nO3dd3xT5f7A8c9J0jbdTdPSUlpkyVIQsaAMRaRwvbgqKuLAgRuVKzhREBXBoiBeVH7iFXEvFHBexYJslCIgAioUBRmF7nSP5Dy/P3op1ma1TU6S9nm/XrxeNOc5J9+Tk+Sb80xFCCGQJEmSpBbQ+ToASZIkKfDJZCJJkiS1mEwmkiRJUovJZCJJkiS1mEwmkiRJUovJZCJJkiS1mMHXAfjS0aNHPXq8uLg48vPzPXpMrclz8L1Ajx8C/xwCPX7w3jkkJSXZfVzemUiSJEktJpOJJEmS1GIymUiSJEktJpOJJEmS1GIymUhSAJJT6kn+pk335pIkLRz7I5eP53yJ5XgJkeYILr53JDn7j7P2vc0EhQRx0cQRnHJ6MrvX/cq2lbsoyinBZrPSfWBX/nnncCJNEfXH+vylb/nh021UllVhSohmzIOj6T2ke91zZHxB8fESwk3hXD7lQjr1SfHhWUttjeIvswYvXLiQbdu2ER0dzbx58xptF0KwZMkStm/fTkhICBMnTqRLly4A7NixgyVLlqCqKiNGjCA9Pd2t55RdgxuT5+AZJQVlZC5ZS8HRYnav/42iHIvT8kHGIGqrahs9rg/W03dYL9JuPpelGZ/z5+6jqDb15PYgPRGmcKrKqqiuqKl/PDYphvtev41TTk/23Ek1gT9cg5YI9PhB+67BfnNncv7553PhhRfy8ssv292+fft2jh07xoIFC9i3bx+vvfYas2fPRlVVFi9ezLRp0zCbzUydOpXU1FSSk33zIZJav+qKaoqPlxCbZCIopPFHaMeq3bz92MfkHy50+5j2EgmArcbG9m93sf3bXfa319qw5JY0erzwaDGvP/QBN2dcTae+8g5F8j6/SSa9e/cmNzfX4fatW7dy3nnnoSgK3bt3p7y8nKKiIvLy8khMTCQhIQGAwYMHk5WVJZOJ5HFCCN57cjk7MndTVlQO4n9tF4qCKSGaKx++iPUf/cBPq/egWlXXB/SyAzsPMfuqBXQ5oyP/ev02KkurqCqtQmfQEZdixhCk93WIUiviN8nElcLCQuLi4ur/NpvNFBYWUlhYiNlsbvD4vn37fBGi1MqtfG0Na97ZRI2du4jKkkoW3LrYB1E5V11ewy+bsrkv9XGsNTastVYQddVnXfqmMOm124iKi3B9IElyIWCSib2mHUVRHD5uT2ZmJpmZmQBkZGQ0SE6eYDAYPH5MrbWlcxBCcHDPYRCCU05LqX/fWPJL2bDsB6Jiwznn0gFUllby4t2Lyfp6h8PqKH9XVVbd4G9bjY19Ww8w+ewZDL40lSmv3UlQSJDHni/Q30eBHj9ofw4Bk0zMZnODxqSCggJMJhNWq5WCgoJGj9uTlpZGWlpa/d+ebpySjXb+wZ1zOLI3h/+7+y2OZh9HVVXCo8MYeNEZ7N6wj7w/C+obuUPCgxE2YfduxFMUnYJOr8NWa/Paczhirbaybun3bFyRRViUkUhzBKPvSmPolQMc/ihzR6C/jwI9fmjDDfCupKam8vXXXzNkyBD27dtHWFgYJpOJqKgocnJyyM3NJTY2lk2bNjFp0iRfhyv5MVVVWXDLYo79kVf/WFlhOavf3tSobHV5TaPHPKnnOV2Z+vEkVr21gY9mfUZVebXrnU5QAA/1xbTV2igtKKe0oJzXJr/LG498SKfTkonvZGbA6H78tmU/7VLiGDTmLMKjwzzzpFKr4jfJ5IUXXmDPnj2UlpZy5513MnbsWKxWKwCjRo3izDPPZNu2bUyaNIng4GAmTpwIgF6vZ8KECcyaNQtVVRk+fDgpKbL3imTf/u0HWXz/uw0SiTd1PK0DYdGh/L79IDWVje9u2ner6zgy4oah6PU61n/0A7VVNvTBOtp1NJPUoz3fL9vK0ezjDfbT6XWMvGUYP2Xu4tgfeSjYr/JtLmu1lextB8jedoDNy36sf/y9p5ZzxgW9uevlGwk2eq5aTAp8fjPOxBfkOJPGWvM5lBSUMfX8WZQVVWgSR++h3bn/rTswBBvYs3Evr9z7Fpbc0vrt7TrF8ejHkzAlRjfY7+/xqzaVNx9dypbPt1NbU0tohJGb5ozjrH/0oaayhuxtB6itsbLytbUc3XuMouMWhOrdj/U56Wdx10s3ONwe6O+jQI8ftK/mksnEg+Qb0D/YO4dySwXTRz1LwZEijz2PIVhPaISR2moroVGhBIcYsNZaSejcjqsfu7TRCPQ9G/fyxcuZVFgqiUmIZuyjF5PULdGt+N0lhEBRFPZm/c72lT/z0+o95GTnNhjo6BEKRMaGYww30qF7Ine+eAOhkcb6zYH+Pgr0+EEmE03JZNJYazyHqvJqFty2mN3rfvPI8XV6HR16JHL1o5dy+rCeLWqotseT10AIwZ+7j/Dpv7/h+B95oCiUFpRSWlDu0QSjN+iY/NYd9DmvJxD476NAjx9kA7wktchv32fz/EuLyM8porqiGkOQnoKjxc3q0hsZG064KZywSCOhUaHEtIsipVcHzknvT0y7KI8nEW9QFIVTTk9m0n9uqX/MWmvj+09/5Oc1v3D8j3ws+RYKjzif7sUVm1Xl+etf4cZnxnL+dYNbGrYUgGQykVqNQ78e5f/ufcvlPFj2KHoFQ5AevUFP+1MT+Met5zPosrO8EKXvGYL0DL1yIEOvHFj/2HfvbOKDmcupakHvNVUVvDPjE7qf3QVbmUpxiYXY9jEBkXSllpPJRGo1Pp7zZbMSCcC4aZdx4W3DPRxR4Bh+/WCqyqvYtGwrFSWVGCOM6PQK+YcKqbBUun2c2iorU89/Bp1eQafXk9wjkfGzrqJb/07eC17yC7LNxINkPav2aqutrHpzPSsXr6Ugpwia0QyQ3CuJp1c+5De/oH19DU404gMc2XeMjKtepCS/rNnHi0uO5Zk1jwZUV2JfXwNPkG0mkuRCZVkV7z6xjN++30/h0SKsNc0bOW4INnDTnLGce9XZHo4wsP01qXY4NZH0yRey8vV1FB4tJiQsiKCQIEqLyqittLp1vPzDhWz8ZAtnjuzDkgc/4Oj+4+gNOrr178wNs68KqCQjOSbvTDxI/prxPiEET13yPL/v+LPZxwgKMZDQOZ6LJqYxeEyqB6PzDH+8BrXVVo7szSGmXRQxCdEUHbMw/+ZXOfjzYbf2Dw4LQlF0VP9thP/Ai/tx9ys3eyPkFvHHa9BU8s5Ekuyw1tr49N9fs3nZj+T9WeB6h79RdJDULZGz/tmXwVcMIOGUOHR6uWq1u4JCDA3GzZgSo3nyqwd4YPBT5B9yvW5LTYX93nT7tx+kvLiC8Bg5RUugk8lE8mvVlTXkHcznw1mfsfO7X5q8f0h4MH2H9+b6p64gpl2UFyJsuxRFIWPNY7z35DKyfzxA8XFLk9tWKkoqKbfIZNIayGQi+a1PX/iajcu2UnzMQnVl07usJnZtx4Pv3UVch1gvRCdB3R3LjbPH1v/9ws3/cbgqpD2VZVWUFlXQ7hRvRCdpSd7nS35p+7e7+GzBSo7/nle3tnkTW/YSOscze/UjMpFo7NbnryW5Z+MpYhwS8Oy4l1BV369MKbWMTCaSX8k/XMia9zbz2pT33O6lFZMYRXzHWMKjwwiPCaPHgK5Meet29Hq5LK3WIkzhTFs+mTGTLyIkLNitfarKqpl95QI2fpyFzar9mi6SZ8jeXB4ke4A0nxCCNx9dyo9f76Qkr9T1Dv8TERvOgm0z0el1dXNPAacP7B3Q16G1vI8O7DvI9FHPUphT7PZ+Xfp1ZOrSewkOdS8ReUtruQZa9uaSdyaSX9ixajebPslqUiJRdArnjj0bvUGPoigkdmlHYpd2XoxSaooIUzizVj3CgIv7EZtswhDs+k7x9x1/suC21zSITvI0mUwkv7BxaVZd24ibDMEGRtw4lLGPXuLFqKSWCosK5Z5Xbub5zTP4vz1ziD/F7HKfn9f8xiuT3qa8WJt1ZyTP8JveXDt27GDJkiWoqsqIESNIT09vsP2zzz5j/fr1QN2yq4cPH2bx4sVERERw9913YzQa0el06PV6MjIyfHAGUks0qa5cB2k3ncs1j6d7LR7JsxRFIdgYxLPrp/HujGWseXcz1hrHI+g3L9vK1q9+4trHL+OCG87VMFKpufwimaiqyuLFi5k2bRpms5mpU6eSmppKcnJyfZlLL72USy+9FICtW7fy5ZdfEhERUb99xowZREXJcQSBxlpj5f/ueZNfN2e7VT4yNoIzRvTm6scu9XJkkjfodDrGz7ySXoNP5aXblzhdari2qpb3Z37KoDEDCI0wOiwn+Qe/SCbZ2dkkJiaSkFC3HvbgwYPJyspqkEz+auPGjQwZMkTLECUP2r/jIGvf20xkbDiW3BK2frXTrf2i4iN57JNJsl2kFTjrwr6ck34mm5dvc1quprKWe/o+xv1v307vIT00ik5qDr9IJoWFhZjNJ+tSzWYz+/bts1u2urqaHTt2cMsttzR4fNasWQCMHDmStLQ0u/tmZmaSmZkJQEZGBnFxcZ4Iv57BYPD4MbXm7XNYdP9brHp3fX19uKJzb6ZevUFP6qgzOH1gb5dlA/06BHr84N45THv/frZ8tY2M6150OijVWmNlzriFzP7mMc44z/X194S2cg08+nyaPZMT9m51HU0H/uOPP9KjR48GVVwzZ84kNjYWi8XC008/TVJSEr17N37TpaWlNUg0nu42J7sTOnf8jzy++2Bjg4ZVoTrvmW4IMdDljI6cdm4PLv3XKLdiC/TrEOjxg/vn0GVgR17e9Qz3D34Cy3EnPfkETB/9DE989QAde3fwYKT2taVr0FR+3TXYbDZTUHBy8r6CggJMJpPdshs3bmTo0KENHouNrRvlHB0dzYABA8jOdq/+XdLWD1/soLSgaXM3JXdvz2PL/kX65AvR6fzi7Sp5WFCIgdQLz3BZzmZVmXnpfD6c9ZkcMe+H/OLT2bVrV3JycsjNzcVqtbJp0yZSUxtPDV5RUcGePXsabKuqqqKysrL+/zt37qRjx46axS65x1pj5ZeNvzVpn5DwEM5J7++liCR/Mm7aZZx2bneCjM4rS2qqavnqlVW8Pe1jjSKT3OUX1Vx6vZ4JEyYwa9YsVFVl+PDhpKSksHLlSgBGjRoFwJYtWzjjjDMwGk/27LBYLMydOxcAm83G0KFD6devn+bnIDlWU1XLlLOfcPuuJCwqlPbd2jEo/SxGThjm5egkfxAcGsxD79/N/u0HWPLwRxzac8RxYQFr3tvM6DtHEN/R9bgVSRtyOhUPkvWs9r1wy2ts/+Znt8oqOoVFv84hJCyk2c8X6Nch0OOHlp/DpuVbePVf7zltUwsODWL49UO45vF0jy+5LK+BY37dZiK1XpVlVexY6V4iATj93B4tSiRS6zD48oE88cX9GCMcvxdqKmtZ9dZ61r6/WcPIJEdkMpG8auHEN3F076voFHT6ul+Uil6h42kdmPzW7RpGJ/mzTn1TGPPAaKdlrNU2Mt9Yr1FEkjN+0WYitS6///QnK+b9F0teCQd2O14jvMuZpzB5yW3kZB+nXad4uRKi1MgF44eyadlWDuw85LDMoT1HeeaqBfxr8W2ERYVqGJ30V/LORPKoP/cc4cXbFvPT6j0c+PkwOOnBmXBKPJGxEXQf2FUmEsmuoBADj30yiQhTuNNyv27ez9PpL1BRUqlRZNLfyWQiedSnL3xN4dFit8qGhAV5NxipVQgODcbcwf64s786svcYT178PEf25mgQlfR3MplIHlVaWO5Wuaj4SP5x+3AvRyO1FpHmCNeFgGO/5zJv/CIvRyPZI5OJ5FHu9DNP7NqOyyf/k/ZywkbJTVc+fLFbdycABUeK+HD2p16OSPo7mUwkjykvriD/zwKnZa6ens7TKx/mghvkrM+S+zr3TeGh9ycyeEwq+iDXKzZmLpE9vLQmk4nkETVVtcy9/v+ctpcEhRg4++IzCAqRnQilpkvs0o47Foyn65mnuCxbU1nLxk+yNIhKOkEmE8kj/u/uN/l9x59Oy3Tqk4K5Q6xGEUmt1ZgHRrucwwvgk+e+pLqiWoOIJJDJRPKAomMWsn/8w2mZ8Jgw7nhxvEYRSa1Zz0HdOOfSs1yOKSk4XMSDQ58me9sBbQJr42QykVqsOLfEZS+uIVcMID5FTsontZyiKNz6/LU89P5ERt06jJgEx2OULLklzLv+FWqqajWMsG2SyURqNiEEa97bxDvTP3Y6IV9IWDCj7xqhYWRSW9D5jI5c98QYMtY+RkxCtMNyFSWVrHl3o4aRtU0ymUjN9uGsz3h3xjKyfzzgtJw52YQp0fGHvS0ysJ8oMohWniII5+ug26PnKHoO4V5n7NYtNMLIrMyHnTbM71rXtLV0pKaT3WqkZjn8Ww4bPt5CTaXr6gNTQoz3A/JbtYSxlGB+RKeUo6AC1egoQK/Ure8SwvdUi75UcD06CtGVbiYSI1UMxUZHBGEY2E+Y8gkG/sDAUcCGQIeVZCxiOja6+fQsfS3CFM5dC2/kgUFP2d2ef7hQ44jaHr9JJjt27GDJkiWoqsqIESNIT09vsH337t08++yztGtXN9Dt7LPP5sorr3RrX8lzbFYbj47I4PgfeS7XbweIiA1n1K1tbYErAZQTyUuEKd8AAmfLbeiUCkKV7zGK7wFQaiFcB2FieYNyfz+Ggo1gDmJiGvniXcD1+IvWzGa1OdxWbqnQMJK2yS+SiaqqLF68mGnTpmE2m5k6dSqpqakkJyc3KNerVy8eeeSRZu0recbc8a9wbH+uW2VTeidx0d1p9Btxmpej8hcqUczCqGxAoa5LalPWbGqULNzcV88x4pTrsYgp1DLA/SdsZaorahxuKz5WwmMjM/jHLedz3rhzNIyq7fCLZJKdnU1iYiIJCQkADB48mKysLLcSQkv2lZru920H3SpnTjbx1H8fRKdv3c1yOvIxcBCVEGKVh1CoaFIC8QRFAQM5xPIgNsIpFf/CRhesJANGl/u3FpGmCIwRRqrKquxuP/xLDm9P/wRjRAgDLz5T4+haP79IJoWFhZjNJ7uNms1m9u3b16jc3r17efDBBzGZTIwfP56UlBS39wXIzMwkMzMTgIyMDOLi4jx6HgaDwePH1JrLc3DjizKmXTS3zr6Odgm+mXtLi+ugVH6MrnI+UIqCikBB8XFjeF1SKSdGmX3iEcCECOqPGj4NdNq9N33xWYiLi6P3oO5s+3anwzI1lTV8+dIqRt800umx2sRn2dPPp9kzOWFvGfq/r+ncuXNnFi5ciNFoZNu2bTz33HMsWLDArX1PSEtLIy0trf5vT6+P3BbWjQ42BlFVZn9UsU6v0LF3B/71+m3Eto/x2Wvhzeugo4BQ3idc+bjBHYivE8lfnQxLAIUotZmIog3kiRXU3alY8fZH31efhdtfvI5/9f/VaZXXsYN5LmNrC5/l5nK0BrxfJBOz2UxBwckJAgsKCjCZGs4QGhYWVv///v37s3jxYkpKStzaV/IMIQS1NVaH24ddN5ibZo/VMCLtKJRjUh4kiF9w1aDuj3RUkaBciECHSjiCdtSInuiVEgAqxMXUcLaPo2y50AgjHXq05/ftjqtjbbVWSgvLiIx1b1p7yT1+UaHdtWtXcnJyyM3NxWq1smnTJlJTUxuUKS4urr8Lyc7ORlVVIiMj3dpX8oy3pn1MZYn9+miA5B7tNYxGCypB7CaIHUQpswhW9qAo3kkkf7/BFgJUoaCKxtuaQ1Hq/ukUFYNSSpCynzDlS4zKeozKemKUJ4ng1ZY/kR8IjXDeTlRbZWX6P55l/w732v8k9/jFnYler2fChAnMmjULVVUZPnw4KSkprFy5EoBRo0bx/fffs3LlSvR6PcHBwdx3330oiuJwX8mzsrcdYM27mxxujzCFMSj9LA0j8i49fxCjPIWBgzhde9gNQoAgGIVaFEXUP1ZHoZYUysX1hLIGhQoMId3Jr7oGQd0ddjQPYiTL40nsr8fTKRWEsRSrSKGKf3r2iTQ2/PrB/PHTn06X8C3KsTBn7Es88O5ddB/QRcPoWi9F2Gt0aCOOHj3q0eO15nrWWVcuYO/3+x3ulzr6DO59dYI3Q3Nby6+DIFa5g2Blb4tjEQJstCNffEgQ2wnlG1RMVHA1KvarY/8ev0IlscrdBCm//+W4egQCBdWjSUYVERSL6S2u8vL1Z2Hl62tZ98H3HPs9l9oqx1Wz5mQTGd89SnBocIPHfR2/J2jdZuIX1VySf9uzca/TRAJw3rjAr28/Qc9hDDg/X1fqqqmCqBG9yRdvU3cH0p8SplLGnQ4Tid1jEUqRmEuFmkaN6E216E+xeJI88QX54hUq1H9iEw2nq2nuT0SdUkaYsrR5O/uRUROGMfObh4hLdr7kQcHhIjYs/UGjqFo3v6jmkvzbK/e85bJM65muwko0c1BwPJranrqqrCBAj0o8FeKfVHCtx6JSiaWEaY2m4rLRkxJ6oogywsVbBCn7UUUwgiDClHXNeq66TgaBT1EUeg3uTk6280G221ft4YIbztUoqtZLJhPJqcqyKsqKnU8vD1BWFPjTVShYiFUmY+B3t6uOVKHDSi9KxK3Y6IqefKx0AEK8GuvfCSIoY2J9slGowCAKMLAHnSIQwv0R9TrKMbAPK6d6L2CNXPXIxWxevpXKUscdR/Zt+Z0Pnv6UsY9egk4nK2uaS75yknNCoFqdN0BHt4ti6FUDNQrI8wzsIYpnMCt3EaS4l0iEAJuIJF8soVC8jJUzEURhpQtaJxK78RFGoXgRi3iKMvVqqkU/t6u+FAVimObdADUSFhXKA2/fic7g+KuusrSKla+t4Z3pyzSMrPWRdyaSQ6pNZf5Nrzr9EopJjCbtxqGYkwJzbE8EiwlTVqBTSt3eRwioEWdQxHNAsMvyvqOjmnOp5lxAEC1mYGSdW8lSrxwnnjGUihup4jKvR+pN3VI7ExoRQnmx495dNqvK7vW/YbPa0Bva9oSZzSXvTCSH3nn8E3774XeH2085vQNPfnk/l9w7SsOoPEehmFDlv24nkrpGdSMFYi5F/Bv/TiR/p2DhSazC8ZofDUoroFcKiVbmE8clzVpzxZ8knZroskx1ZTW11Y57fknOyWQi2SWEYNvKXS7K4HSFO3+mJ5tYZSJ6xb2uk0JAmRhPrvgaK4E6KFahmGewigT391DAoCslVpmCWbmeuqlYAs89r9yMMcJ59aMpMQZjuO+rKAOVTCZSA5s//5HZVy5g6vmzKcopdlo2NDLwZqTVUUCM8ghxyu0EKe6PM1KJopwbvBiZNmwkUSRmUSXOplak1PX8cqMtpW4SycNEMt/7QXpBTEI0T3/7kNMy6VMCe7Cmr8k2E6nejlW7ef2BD7DklbgsqzfouPLhizWIypNUTMoUgpSmTaOhijDKxfVAkHfC0piNbhSLOfV/RfMEocp6l/spCoTyLaViMoH41RHTLgZ9kA5brf0OJe898Qk65Qr6nN9L48haB3lnItX75j9r3EokhmA9E+ZeE3DTUISy9H/To7hHCKgV7SkSGVTQOiewBD0WHqNG9HHvDoUa4pXLiGQBgVblFRRi4JTTHa9zdGx/Hv+Z8h77tx3QLqhWRCYTqZ6jRYX+KjTSyILtTzP0ysDrChymrGjC+BEDeeItCsT71NLXu4H5nJEiMROVMJcl6xrmywlTlhHFsxrE5lm3v3C90wXbLLklfPHytxpG1HrIZCLVM3dw0b1XgSlv3U54tOsvHf+jYuC4y1InpkEpFC+g0lGDuPyDIAZbE85XUSBE2YBCYA1WjU0yER0f6bRMhZOZsSXHZDKR6l097TKSe9ifxA0AAX/u9uzkmFqJ5Gncmf1XxUiuWIGV070flJ8pEROxipPX31W1l44KFCxejsqzio+XUFNV67RM0qnu93aTTpLJRKoXn2Jm7nczCHXShfLPPUc0jMgzgthGqLLaaRXXyTEkbwPhmsXmT6z0pUC8Sql6O1XqYLeqBCNY6P3APCi2fQxRZueLYhXnlvJbVrZGEbUeMplIDej0OiodLMsLcNY/+mgYTcsoVBDJv4lVHkDn4ouxlm7kii9RidcmOD8liKCcaynhQazC+frhigKhynqC2aBRdC0XFGJg0Bjn6+5s+3onT1w+l1Vvue7hJp3kN/37duzYwZIlS1BVlREjRpCent5g+/r16/n0008BMBqN3HrrrXTq1AmAu+++G6PRiE6nQ6/Xk5GRoXH0rcPGT7J474nlDrcbgg30vaC3hhG1RA0m5X6CFecz4AoBtXShSMwH5DQaJ6iYqBIXEM5HTu9Q6ubxmo6NbthIpFTchI2u2gXaDBdNHMmK579BtTmu9izJL2X1mxs5/9rBcnoVN/lFMlFVlcWLFzNt2jTMZjNTp04lNTWV5OST3fjatWvHE088QUREBNu3b+fVV19l9uzZ9dtnzJhBVFSUL8JvFarKq3nviWVOZ//tltoJJUAWPw/lC7emUheARTyFwHmjbFtUxl0YWYMB51O46xSBjn0EsQ8Dv1IsngL8d0p3Q5AeY3iI05UYAYrzSijOLQnYeee05hfVXNnZ2SQmJpKQkIDBYGDw4MFkZWU1KNOjRw8iIurqOk899VQKCgp8EWqr9fOaPU4TiSHYwJ0vBsYIcKVmNRHKYrfq/K10wUYH7wcVkBSqxUCEcP8HhEHJI0J504sxeUbaTa6TXXhUKJGmttl+1hx+cWdSWFiI2Wyu/9tsNrNv3z6H5VevXs2ZZ57Z4LFZs2YBMHLkSNLS0rwTaCtmjAh1vFEHD70/EVMAzMMVwWJ0ZctRFNdrsAihUCLuBwLjbssXSrkPRVgxsh6dG68pgC4AeniNeXA0JYVlrPtgM6rVTrc1Bfpe0LvRcr6SY36RTOwtQ++oOmXXrl189913PPXUU/WPzZw5k9jYWCwWC08//TRJSUn07t24bj8zM5PMzEwAMjIyiItz3sDYVAaDwePH1Mq5lw3iecMiu2uXhEWEcurpXfz/3EQFessaFLXMvfKKkZjYIaD4xcegnv+9j+ai2o6jWK5FcWOsjiG4HTq/O4fGHnztbswJJj6Z92WjbcawEG6ZdR2RJuc9v/yZ1u8jv/gUmc3mBtVWBQUFmEyN6ykPHjzIokWLmDp1KpGRJ+u4Y2Pr1nmOjo5mwIABZGdn200maWlpDe5a8vPdmzHWXXFxcR4/plZWv73BYYNkRUklqz5cz8gJ52kcVdMYyCZWyXN7lLtNRJJfUOzVmJrDP99HevQ8g1m5DZ3ieEljIaCk+kwirFY/PIfGIuLsD8Ctqqhm83+z6D8qcHov/p233kdJSfbHovlFm0nXrl3JyckhNzcXq9XKpk2bSE1tOM13fn4+c+fO5Z577mlwMlVVVVRWVtb/f+fOnXTs2HZGLntK5pJ1jdYX/6vIOP//hWYjARX3quKEgEoRmOuw+IqNTthwPLcV1PXuCuczjSJqucO/5tjfIODLlzK1DSbA+cWdiV6vZ8KECcyaNQtVVRk+fDgpKSmsXLkSgFGjRvHxxx9TVlbGa6+9Vr9PRkYGFouFuXPnAmCz2Rg6dCj9+vXz1akEpE3LssjZn+e0TMIp/l1lAfyvR5YOgfNWEFXoqOXMVjGlvLZ0qCIGXM26HEBNUDn7HfdUyz1UgBAiYHow+ppfJBOA/v37079//waPjRp18pfjnXfeyZ133tlov4SEBJ577jmvx9earX57k9M+9zq9QlJ31yvV+ZqeP1EocvldVi0GYCGDgPrW8xOqEuN0uxBQIUYFzBwCFRbHPRgrXXQdlhryi2ouybdczRas6HUc/8P5nYs/COJX9IrrSfp0SiUykTRPjeiPEM4H8dVyhkbRtEx5cQVFxx33PFMUJSDe9/5CJhMJU6LzdgZbjY3v3tmkUTTNV0tXt9bkUInxeiytVSUXuXydTcr9YCvULqhm2rvldyy5pQ63B4cFExTiN5U3fk8mE4lrHk93+aFxNW23P3BnfQ2biKJcjNcgmtbKQJGYi8D+ZKB1651UoLeMQKFY29CayJxsIjTK8fiqCksF5XI6erfJZCLxy6Z91FY7XjUvvqPZrRHDvhak7HPaLVgVEVjEE1g5VbugWiFBFDXC+YJhCrXEKE85LeNrHXt3oGNvx0suqDbB6w+8b3ccnNSYTCZtnKqqfLN4rcPtik7h1uevJSIgppVw3IlACCgX11BDf4dlJPdVM9hlmaYskewr971+GzqD46/BnOzj5B/y/yo7fyCTSRu37v3vyT3geGDTyJvPo+c53TSMqHkUSp02qQuMlHOlZvG0dlWMxCbMTsuIAOjkEBYVSkovx3cnVeXVfP6iXMbXHTKZtGGqqpL5xnqEav82/pzLz+K6J8doHFXTKZRhVu50WMVVN0DxH+Cgnl9qOkEEFnGf04Z4IYzaBdQCYx4YjSHYcZvhz2t/oSTfcUO9VEcmkzastKDM6YckJCwwJrkL510MivMVIKsYqlE0bUcNgxBOvkL0ymEU/L+KqO/wXuicrJ5WeLSY3386pGFEgUkmkzYsLCqMIGOQw+3rP/ieX7/fr2FEzWNQ/nC6XVEgmL0aRdN2BLENxUk7lU6BWOU+7QJqpp3f/eJyXXihOj5PqY5MJm2YIVhPTaXjD5FqE3z3jv8vyaoTzqsgVBFKtWx497gw5QuXk2rqyUHBzVmcfeTgLtd3HRuWbtEgksDmVjLZu3cvX3zxBT/99FOjbStWrPB0TJJGDv1ylKoKx+u9A04XzPIHIXyHQXF89yQE1HAWVnppGJV0gkItCjW+DsOpU1O7Ou3RBVBhkVOruOIymaxbt45nnnmGPXv2sHDhQp555hmqqk4O5Fm+3PGa4ZJ/qyytotbF7b2/9+QKU75A53QKFT3F4knN4mlLKsSlqMJ1pwYV/15Ou9fgbnQf0MVpmbDowOhM4Esuk8ny5ct57LHHeOihh3jxxReJjIzkySefpLy8btU1OaAncHXpdwrhMfbXczihtNC/qyh0HHW6XaDDgPM2Fal5akmlQlyFVcS6mMbGv9sbFEVhylt3cN64cxyWcTbtilTHZTIpLCykW7e6X6fBwcHcc8899O7dmxkzZlBcXCynZw5gQSEGl/NyWWsdj4z3B3oXvYV0Si0GDmgTTBtUxq0UipeclgnjPY2iaT5rjZUDPztuO6m1+vfnwB+4TCYxMTHk5DRcQGb8+PEMHDiQGTNmYJUvcsDK3naAQ784/mUfaY5g5M3+u7piMNtQcN7mYxMmagJkFttApaPI4TZFgQhlqYbRNM+K+V/z527H3cvVGserS0p1XCaT1NRUNmxo3KNn7NixnH/++TKZBLCPMz53uLqiolMYOeE82ndN0DYoNymUEK3McNmbqFb0QCVem6DaqBA2O70OCv7feH107zGn24uOl1DuZO0TyY3FscaPdzzD6uWXX87ll1/ukUB27NjBkiVLUFWVESNGkJ6e3mC7EIIlS5awfft2QkJCmDhxIl26dHFrX8k+1cHIdwB9kI5L7h2pYTRNE8Yn6BXn9dhCKJRxq0YRtV1WTkUInCQUFQN7sNJby7CaxNXcc9UV1QSFOB6TJbm4M1FV1a1/LaWqKosXL+bRRx9l/vz5bNy4kcOHDzcos337do4dO8aCBQu4/fbb65fvdWdfyb7h1zmerM9aY2Pn6j0aRtM0OgpclhEEEcJmDaJp21xN+qgoEKm8rlE0zXPZ5AudrpcWEhpMsJMBvpKLO5NrrrnGrYN8+OGHLQoiOzubxMREEhLqqlQGDx5MVlYWycnJ9WW2bt3Keeedh6IodO/enfLycoqKisjLy3O5r2Tf6cN6EmQ0UFtlp6pSwIGfD9Mv7XTtA3NDBZcTJr5CUZyNwK4hnPdQRRiVXKFhdG1LCI5nnT5B58fTqpQVlfP6Qx84rPKFus+K5JzTZPLSS857aXhKYWEhZvPJGUjNZjP79u1rVCYuLq5BmcLCQrf2PSEzM5PMzEwAMjIyGhzPEwwGg8eP6U1xcXH0GngqO9f90nijAudclOrH5xOHKDkbrJudzk2rUyqJMqwnPPoOzSJrqUB7H+ks36G4aJ826G3ExfjnOS288032bfnd4fZgYxB3zbsJc1yshlG1nNbvI6fJJD6+ccOlqqpYLBaio6PR6TwzG4u9sSp/73LsqIw7+56QlpZGWlpa/d/5+Y6nXm+OuLg4jx/T2/qO7G0/mQjY+NkWErv75xdAnWdop6ShKM47gVitZRQE0HUJpPdRBK8Rrmx0WkUEIGxHqchfQAXXahOYm1RV5fedztddOeuffRHBasBckxO89T5KSrI/Zb/bCxxXVFTw+uuvs3HjRlRVRa/XM3jwYCZMmEBYmPOBb66YzWYKCk7WgRcUFGAymRqV+esLc6KM1Wp1ua/kWK8hp6LTK6i2xkl51RvrOP/aQZiT/Pn1dD1o1koHDeJoexQqMCqrXfaog7rxPqGsokL4VzIBZx0HQKfXcfE9/tsRxZ+4fWuxZMkSqqqqmDdvHu+88w5z586lpqaG119vecNa165dycnJITc3F6vVyqZNm0hNTW1QJjU1lXXr1iGEYO/evYSFhWEymdzaV3LMlBCN3qC3u628uJJVb/r3RI8Cx71whACriKdUTNIworZDz5/ocP+Xb92Ej/41Gl6n09Gxt+MfG6pNZfXb/v0Z8Bdu35ns2LGDl156iZCQurl4kpKSmDhxIvfee2+Lg9Dr9UyYMIFZs2ahqirDhw8nJSWFlStXAjBq1CjOPPNMtm3bxqRJkwgODmbixIlO95XcozfoMQTpHa4BHxTi9lvEJyrEZUTwtt1fl4oCCBs6jqLifFVAqelsJKISg45ct8rXXQP/m6j8lnnXsnvDXipL7c/xtmeDXL7AHW5/UwQHB1NSUtKgHaWkpASDwTNfNv3796d//4bThI8aNar+/4qicOut9scM2NtXcs+iSW9TWWZ/FLkxMoQRN/j3olLlTMAoPidIKba73aAUEslrFIl/axtYGyCIoUakoucrl1VdNhFJmbhem8CaKCwqFMXJ4ljBxsBYJM7X3M4EF1xwAU8//TQXXXQR8fHx5OXl8eWXXzJixAhvxid5UdExC/u3O2581Ck6Is0RGkbUHApCiQCKHZZwNX+X1HwlPIBO5BFCltNlky3iCWo4S9vgmkCvt1/VC1BaVM6vm/fRc9CpGkYUeNy+5xwzZgzp6en88MMPvPXWW/zwww9cdtllXHnlld6MT/Ki4twSKpxMEVFRWsmmZVs1jKh5dE4SCYCKnD7ce3TU4nwsUi1d/TqRAJiTHXcyKTxSxDuPL/PIAO3WrEkN8ElJSUyfPp358+czffp0OnTowBtvvOHF8CRv6tA9kWBn67wL+P7TbdoF1FzC8a/KukZ4+10ZpZbTUUCY8qnTu5Ii8YKmMTXH+dcMclrVlftnAYd/zXG4XWpCMtm4cSNdu3Zt8FiXLl3sTgIpBYZgYxAx7ZwvXFRb7XzxLH9QjfN2HR0FKMhJ+rzByGr0iuNqRBsxCCI1jKh5juw7hnAyV11QsIGQUNl24ozbyURRlEa3eaqqysWxAlyPs7s63R6T4Hy9E39Qzi0OF2dSFDDqdmFS7gc/Xz42EKnEOF0YyyoCo2fl7vW/Od3evls7EjrL2aedcTuZ9OzZkw8++KA+oaiqytKlS+nZU85ZE8jSbjrXaU+cM9NO0y6YZlKJxYrzudiC+I1QvtAoorajBseN0kJAJWM0jKZ5dn73C8d+d9y9ObFrPHf/303aBRSg3O7NdfPNN5ORkcEdd9xRP0zfZDLx8MMPezM+ycs+/fc3Dn9Z6gwKZ/jpRI9/V7dmiePZohVFJZhdVAr//3ILJFHKvx23lwDVnKtpPM3x5cJMuzNAQN1SDONnXokpMUbboAKQ28nEbDYzZ84csrOzKSgowGw2061bN4/NzyX5xr6tjtdHV62CP3YcoNfg7hpG1DwKJU631zXEJ2oUTdsRhLPqIQM6ClFpp1k8zVFWVO5wm61W5c2pH/Hkfx8kLCpUw6gCT5MygU6no3v37gwaNIju3bvLRNIKtEtxPjL8wC7HS5n6C4Uy9FhclqrkEk3iaSuC2IHOSccGgQ5/mz7FHr3B+fdY7sEC1n3wg0bRBC6ZDdq4GzPGotM7G/3r39Op1FEQLqatVYlEpb1G8bQFgmhlntP2NpV2qPj/3eDxA67nF6sq9/+lh31NJpM2LqlbIrc+f53D7Z8t+Nbve+wJwrFxitMyOkowKXfJLsIeYuAX9BxyuF0ApeJm7QJqJmutjSoH0wmdEJMQxXnjBmkUUeCSyURiyBUDMCfH2N1WfMzCf+57V9uAmsEiHkUVjn8mKwqEKL9gUu7WMKrWSiVKmeVyPi4RAHcllaXO7zjikmMZfecIYtvHaBNQAJPJRAIgpafjabg3r/iR6grnv958TSUWG67XXQniIHoOeD+gVkqhErMyniBctKUpidTSTZugWqDouOOOG8aIEJ76+kH+cdv52gUUwGQykQC4ec7VDt8Nqk3lty37tQ2oGcrEPU4H0EFdF2E9x7QJqBWK4FWClCNO70qEANU4AQjRLK7mqq10PJBV2ARV5fanpZcak8lEAupGukeZHU97Yav1/1451VxAqRjndO1FIUBPDnpXv6wlu4KV7e4V1AfGfGhH9x93uE1VVYdrnEiN+byrTllZGfPnzycvL4/4+HgmT55MRETDac/z8/N5+eWXKS4uRlEU0tLSGD16NAAfffQRq1atIiqqbo6pa665Rq5t0kwRpnBK8kobb1Cg+8Au2gfUDBXcSYTyPYo4YHe7okC08m9sIoYa+mER0wHHE0VKfyXQkeeylKIAamBM+5/5+nqH2+I7mkk61f/bffyFz5PJihUr6NOnD+np6axYsYIVK1Zw/fUNF9HR6/WMHz+eLl26UFlZySOPPELfvn1JTq6bQuOiiy7i0ksv9UX4rUqNg1t+RacQHh2mcTTNp4aMQ6nMcFoVo1eKMYp1WPmAchz3ZpNOCuMTdDge4HeCVSRA8AjAfycJzT2QzyuT3ubgbsezJgwek4pOLytv3OXzVyorK4thw4YBMGzYMLKyshqVMZlMdOlS98s4NDSUDh06UFgYGL98AomjW3phE+zZ4HwiPH+i6EJd9jSC/02xojj+ZSqdpOco4coSl6+rKgxUiKtA578ThAohWHjPm+zfdsDpTMEFR4u1C6oV8PmdicViwWSq64VjMpkoKXE+LUZubi5//PEH3bqd7CnyzTffsG7dOrp06cINN9zQqJrshMzMTDIzMwHIyMggLi7OQ2dRx2AwePyYWoqKjaC82P44jA9mfs7CH4doHFHz6DkHUWlCEUUuywYrh4kz+9c187v3kVqD3nI5inB+VyIAEXoLYWF3+t85/MWfvx4hZ5/jthIAFOh5Vje/PQd3aH0NNEkmM2fOpLi4uNHj48aNa9JxqqqqmDdvHjfddBNhYXXVLqNGjapf7fHDDz/krbfeYuLEiXb3T0tLIy0trf7v/HzXI1+b4sQEmIFq4KVn8ukL39jd9ucvhzl29BiGYJ///nApLq4DNbYBGJXv0CnOq1qEqKEgPxtBjDbBucHf3keRPEuYUoSzSQaEgAoxitKKcVCR73fn8Fc5h45R5aKre6fTUuh/cR+/PQd3eOsaJCXZ71yhyTfD9OnTHW6Ljo6mqKgIk8lEUVFRfUP631mtVubNm8e5557L2WefXf94TExM/f9HjBjBnDlzPBZ3W3PppFF8uXAV1hpro21CFTw95t9MXXpvQCwSVMJUakQ/ongBhRqH1TMKNch1TpwzKt+5rt4iglKm4jTj+InCo0U46/KX1D2BJ1Y8iGL075kf/I3P20xSU1NZu3YtAGvXrmXAgAGNyggheOWVV+jQoQMXX3xxg21FRSerMrZs2UJKSmAsxuOPDMEGxkwe7XD7Hzv+5LUp72kYUUsoVDEai3jIZcl45WpilZsIZYX3wwoAIawmRnmEGOUBongSHc5HiatCR754h0BIJAA6FxM7nj6sJ/HJzidAlRrzeZ1Feno68+fPZ/Xq1cTFxTFlyhQACgsLWbRoEVOnTuW3335j3bp1dOzYkQcffBA42QX4nXfe4cCBAyiKQnx8PLfffrsvTyfgjXvkcj5d8DXVDnp27V7/G9WVNQFxdwJ162nYiMFAsd3tilI3TWQwBwjiBcL4jELx74BYatYbIniFMOVTdEpdAhHgxrQpUSjYnI7v8Sd9z+9NVHyk/W7wwKHdRzWOqHVQhL/P4udFR4969k3jz/XE7oqLi2P9Z5uZd8MiaqvstzcMuvws7nzxBo0jc9/fr0Mw3xOjPIpOcW/gZaU4H4t4wkvRuear95FCGWblNgxKTpP3LVVvoZzx9X/782fhv4tWs2zuV9RU2n9/9xrcjbmrn/Tb+N2ldZuJz6u5JP/Ta/CpPPHlFIe1Fln//cnpgkL+poZzyBdvoQqjy+lWAILY5/2g/JCew+ho3pePCICpU6CuynzDx1kOEwkgByo2k0wmkl1VZdUOGymtVVa+/3SbtgG1kEoyueJrysSVLhOKniNEM5VgNuO0pbaVsZGE2ozZAKyiPZX80wsReZ7NqlLu5IdQSu8krnjoIg0jaj1kMpHsKityvu7Hxo+3aBSJZ5VzD6oIdZpQFAVCdZuJUWZgUqYA/j1jsqfoOIwe9+aiEsKAKsKpFV0oEfcGTBuTIUiPzeq4unP8zCsDarYHf+LzBnjJ/wgh+OS5L52WseSVoqpqQC7dXKv0w6hsdllOp9QQLLYTzjuUc4sGkfmClRDWY2QVRmWDWzMH2EQsFnE/KklYOYVA+U1aU1nDFy9/S0m+/YZ3gPhTZC+u5pLJRGrk4K7D5DiZTRUgLCo0IBMJ4HTd8r9TFAjnE2rFadRwjhej0poghK+JVl5AodqtJAJgEzEUiNdQifVueB5WVlTO05e/QE52rtNyFcWVxCbGaBNUKxOY3waSV9msKsLmvK2g+9mBMYuwPTaaNsWETqkgWslAz+9eikh7kWQQo8xBp7ifSITQUSlGB1wiAfho9mcuE4miU4g0h2sUUesjk4nUSOe+KSR0jndaZvOKH3njkQ81isizSsXtCNG0m3K9Ukys8kArWEPeRjSPEaZ843YSgbrpUirFUMq41XuhedGRva4XROtxdlei4+3PwCG5JpOJ1IhOr+PGjLEEG4MclqkormTjsq1k/3hAu8A8RCWBcjEGJxPG2qVXColTxhLKMu8EpoEonsOobGxSIgGwkUwJ0wjErwwhBDnZTqptFTjrn3257/XbtAuqFQq8d4akiR4Du3L5A867e9ZU1PDJ3K80isizyriLcnEbNaInteIUrMLs1hgUvVJGpPIi0TxMjDKdcN5GcTHdiH+owqRMIVT5usmJRBVRVIhLgMCY9eDvdm/YS7WDAbgASd0SmPSfWwiNNGoYVesjk4nk0KgJwwiNcP4B27PhN/ZvP6hRRJ6kUM51FIpXKBBvki/eRXVz5mCdIgjV/YBRWU+Espg45Rr0+OtrYCWEr4gnnRBlWxPaR6BG9KZSDKdQzKaCq70bppdYa20sm/sV1urGk5eeEJtk0jCi1ksmE8khQ7CBB969kwiTk373Aubf9Crbv92lXWBeYaRIzMEq4t26QzlBUeraU+KUG4lXLiWKWQTzA74a7KgjDyNfEMZi4pQrSFDSiFGeRa9r2lrmghAKxUIsYgZWTvdStN736n3vsN9JVaw52cRVj1yiXUCtmEwmklPdzurM/W/f6XSm1dKCMr76v1UaRuUdVnqQL5b+b+xE09QllRLCdN8So0zHpNyP96e2VwlhLRG8TDDfE8ErmJU7idHNJVJ5G4NSUDeRZTMm862ls+fD1VhpYZnLFUKve+JyOvVJ1iii1k2OM5Fc6tLvFHoM7MIvm7IdlrE4GQgWaIrFTGJ4HAMHmvVFXDfYcRvRPEUJjwHV6KjERsKJEnb3M/IVoco36C02omhPKXf/bdEuGyGsIZg9qOgwKj+gJwedUosqlqMgUBQb4H4CEQJUEYpOqURR6v62EUexyGj6ifuZr/5vNaUFzueQ27n6F8668AyNImrdZDKR3PLAO3cx64oF/O6gfcRZnXSgsdGRAvEfjHxDuFiKQTnY5KSiKGBkA8YGc1bVTXZfSw8sYgYqdaOtdeRiUh7+X/ISYIMw3S6CxEEKxMtAEEZWEKW8gkJV/Zf+X2PSKU1//esSRzvyeQe9OIRRrMFKN6oZCs2Yo8uf1FZb2bbyZ5flErq20yCatkEmE8kthmADMz6fwr1nTrO7DoRqc29698ARRBUXU8XFhIsFRLCsWQmlIYFCNSHsJJ4rsWFGEIyB4/V3FH9lIJsoniZY+Qk9xQ2O15w7psbRGMgXbwPB2OhKOV1bflA/kXeogOLjFqdlIuMiuOD6IRpF1Pr5PJmUlZUxf/588vLyiI+PZ/LkyURERDQqd/fdd2M0GtHpdOj1ejIyMpq0v+QZ9pb0BSg6ZqG0sIzI2Nb32pdzD3pRSghb0CvOv6DcpSgCg4vp3hVFJUxZ65Hn+zubMFIoFkKATB3fFEIIPnvhG6orHLdZte8azz2v3oIxvPWdv6/4PJmsWLGCPn36kJ6ezooVK1ixYgXXX3+93bIzZsxotEZ8U/aXWi4sKpQKi/1xFUvnfMFNs8ei07e2fh06SngMnThODI8SrOz3dUDNJgTUih4U8wQq7X0djldsXv4jWV/9hLAzKjUsOpSXf54dsPPK+TOfv6JZWVkMGzYMgGHDhpGVlaXp/lLTdDg1weG2te9u5v5znmTl6975Ne1rKgkUi+eoFqfXT2MfSOuUCgG1dKWQl1ttIgHY8vl2h3fQVeXV7NmwV+OI2gaf35lYLBZMprpBQyaTiZKSEodlZ82aBcDIkSNJS0tr8v6ZmZlkZmYCkJGRQVxc0yb8c8VgMHj8mFpzdQ73LLiV2/rc73AYRWFOMSvm/Zdeqd0584I+XorSOe9ehzjgA1TbAVBLQVSgK3sEHXleer7mq7tEChANih41eDS68IeI80SDiwu+/CwIJ813qlVl95q9nD9mqNNjtIXPssefT4snmTlzJsXFxY0eHzduXJOOERsbi8Vi4emnnyYpKYnevXs3KY60tLT6JAR4fH1kf1732l2uziE4xsCQKwaw8WPHd4DllkqWzvuMlL6++fWrzXWI+N8/CGMs4coH6JUCAIRQqP8q/9/39t97X3mTECAIplqci4Wp1H/MbUBVgSYx+OqzYK2xciT7qNMy4eZQl7G1hc9yczlaA16TZDJ9+nSH26KjoykqKsJkMlFUVNSoTeSE2NjY+vIDBgwgOzub3r17u72/5Dm3zb+OrV/95LSBc2/WHwghULT6BvWhCq6iSpxHqPgcMFJB3YhqPYcJFx8CNvQcJIjDgPeSihAGaulGpbiEStre0rPZW/9g8YPvk3ew0GEZQ7CeETeeq2FUbYfP20xSU1NZu7aujn3t2rUMGDCgUZmqqioqKyvr/79z5046duzo9v6SZ9msqt3Gzb+qLKnklXvfosbJBHutiUoC5dxKOdcjiEYQjZXTsPAUFmZRyDscF5mUiruoEd0RomnjOJy1zajCQJV6FnniIwrFK20ykQghePOxpRzd53x24NF3jXA535zUPD5vM0lPT2f+/PmsXr2auLg4pkyZAkBhYSGLFi1i6tSpWCwW5s6dC4DNZmPo0KH069fP6f6S9xiC9MQkRpN7wPkt9PcrtrFn4z7uf/MOOvVN0Sg6f2aggqupEFej5w8ieBsdZdSKZPTKMfQcRYcFvVKO8repWGrphE7UoqeuCkclCiuJqLSnXFyDlR6+OCG/cXTfcY7/4aTdSoF7/3MLqRf21S6oNkYRIpD6o3jW0aPO61abqi3Vs+5a/xsv3PwqtVWuR153PiOFJ758wBPhuSXQr0OcOZTawocJIhsQ1NKFEvEQYCCEtYCOKs4D/PcXtpbX4Mi+Y6x6Yz2r3tzgsEy7Tmae2/C428cM9PcQtNI2E6n1Of3cHjy7fjpvTv2In1btdloNc+z3PEoKyogyt74BjV6hhGMRT3Cyy9zJRpYqLvRFRH7rnemfsH7pD1SVVTstl9Krg0YRtV0+bzORAlds+xgmv3E7D70/EUXnuFW5proWvZNZhyVHFP6aSKSTrLU2XrprCd++sc5lIgG44qG2146kNfkJl1qs99Ae3P3KzQ4Thq2mboEiSfKUV//1Nlmf73Br2ZjkXu1J6uZ4sK3kGTKZSB4xYPQZPLfxcYIcrBv/3TubqCpv2gJNkmRPaWEZ+7b+4VZZU2I0Nz9zdZvoou5rss1E8hhzBxMRpjCKchpPhmirtTGp/+PEdTBx+nk9Gff4ZXJ+JKlZLHmllBY5X6fklD7JDL1yIMOuHURIaGCuXR9oZDKRPMrcIdZuMgGoLq/myN5jHP8jD4HguifGaBydFMiEEPyyKZuDuw/hpImOTn1TmPH5lFY44ah/k8lE8qgRNwwl20UVhLXWxs9rf20zI+Sllqssq+L5GxZxcNdhpzMvAFw743KZSHxAvuKSRw26/Cxik2JclsvZf5xX7n2LyjLZjiK59u6MZezd8rvLRKIz6Eju0XpnRPZnMplIHqUoCudcdpbrgmrdCPl/T3jN+0FJAc1aY+XHr3e6VTbp1ATCY8K8HJFkj0wmksddNHEE7Tq5N/X1bz9k85/736XMRYOq1Ha98/gnDhdk+6vw6FAefG+iBhFJ9shkInlchCmcyUtuI6VXEjoXgxVVm2DDh1uYedl8jmYf0yhCyd+V5JdycNdhaipr2OBkuYMTErrEM/nN24mJl7OG+4psgJe8IunURJ7+9mEseSXMGD3XYQ+vE479nscHMz9lypt3aBSh5I+stTYWTXqbfVm/U26pJDTSSK2LmafNHUw8/ulkIkzhGkUp2SPvTCSvio6P4o4F492aFST3UAH7tx/kwM+HaMPzj7ZpH876jKwvd1B0zEJNZQ2WXMcrpwJExUVwwQ1DZCLxA/LORPK6XoNOpffgU9mzcZ/Tcjl7jzPr8hfQGXQkdmnHXS/dQIfusmdOW7L3h/0u18o5odfgU7l5ztUkdI73clSSO+SdiaSJsy/tj07v+vbEZlWprbJyaM9Rnr1mobxDaSOOH8hj8f3vc3ive8tCKAoykfgZmUwkTQy+YgDtuzZtsr3i4yWseW+TlyKS/MWejXt55qqXWPfh91irbW7tE5diJiZBNrb7E59Xc5WVlTF//nzy8vKIj49n8uTJREQ0XPfi6NGjzJ8/v/7v3Nxcxo4dy0UXXcRHH33EqlWr6td+v+aaa+jfv7+m5yC5FmwM4l+Lb+U/k99l37Y/QHVvvy2f72D4dUO8G5zkU8vm/peinGK3yxuC9PQZ1pOQsBDvBSU1mc+TyYoVK+jTpw/p6emsWLGCFStWcP311zcok5SUxHPPPQeAqqrccccdDBw4sH77RRddxKWXXqpp3FLTJXSOZ9qK+ygtLOPzl77lm/+scTmFeLCDWYil1qPoWLHbZTue1oGzLuzDpf/6h/cCkprF59VcWVlZDBs2DIBhw4aRleW8T/nPP/9MYmIi8fGyrjRQRcZGcO3jlzNs3Dkuy+5c8wuTzpzG0ozPZftJK2GtsbLzuz3sXL2H797dRMGRIrf26z3kVGZ+8xDpk/8pZ5z2Qz6/M7FYLJhMJgBMJhMlJc67Am7cuJEhQxpWe3zzzTesW7eOLl26cMMNNzSqJjshMzOTzMxMADIyMoiLc2+UtrsMBoPHj6k1Lc/hgcV3Y040s+GTH8g9lG/3LkW1qljySvnipUw2fbKVf2+eRWxijNPjBvp1CPT4wfE5bPw0i0WT36TgqHsJ5ARFr3DF5Es0e11a8zXwFkVo8HNv5syZFBcXN3p83LhxvPzyy7zxxhv1j918880sWbLE7nGsVit33HEH8+bNIyYmBoDi4uL69pIPP/yQoqIiJk50b0qFo0fd6znirri4OPLz8z16TK356hzu7P0wlSWuJ33s1DeFGV9McfrLNNCvQ6DHD43PodxSwcK73mD3xr0IW9O/cswdTDz/wxMejNC51ngNPCUpKcnu45rcmUyfPt3htujoaIqKijCZTBQVFdUnBnu2b99O586d6xMJ0OD/I0aMYM6cOZ4IWdJYeFSYW8nk4K5DfDj7My6b9A/CokI1iExqKSEE865/hf3bDzZrf0OwnkGXp3o4KsnTfF7xmJqaytq1awFYu3YtAwYMcFjWXhVXUdHJ2+UtW7aQkpLinUAlr7ox4yq3xqEIFb5+5Tsm9pnKinn/RQjBno37+PrV1Rz+LUeDSKWmEEKwcOKb7N/R9EQSZAwisWs7Rtx0Hlc8NNoL0Ume5PM2k/T0dObPn8/q1auJi4tjypQpABQWFrJo0SKmTp0KQHV1NTt37uT2229vsP8777zDgQMHUBSF+Pj4RtulwND3/N7cMGssK+b/l5KCMlSr877DwiZY/sLXbM/cTc7+41RX1BAWvZIzLujNY+9N1ihqyZ7aGitPXjGX37Zkg6JQkl/istfeX8WlxHLPoptJ6ZmEPkgvF1ALEJq0mfgr2WbSmK/PQQhBbVUtr93/Hj98tr3pB1Dg1meu49zrB7ou66d8fQ1aoqq8mvsHPUlZoftLCuj0Cu27JdAv7TTOHNWHbv07+TyBBPI1OKFVtplIkrsURSE4NJjb/z2eg7sPc2x/XtMOIOC92csYet0An38htUVvT//E7URiMBoYddN5jLjpXMwdTPJ6BTift5lIkj2GID3njj2b0Ehjk/etKKnk5TuXyHEpGiorKmfRpHf44dNtbu8TGmHk6mmXEZccKxNJKyDvTCS/dfHdI0noHM+yuf/l6N6mLZyV9eVP3NTxPnQ6hdgkE8OuOYdL7h0lv7Q8TLWpvPnoUtZ9sBm1iV1+g0KCEELIa9JKyDsTya8NGN2PZ1ZP5erHLiWhU5xb66LUE3UrOeYfKmT5vK9ZPu+/XouzrXpnxieseXdTkxMJQFLXdjKRtCIymUgBYfRdI5izfhoXjG/epI+qTeWb19ZQUlDm4cjaJiEEv23JbtaszopeIblne27MGOuFyCRfkdVcUsBQFIWL7xnJ9m93N2mW2ROqyqqZNeYFnvnuUTm3UzNY8uqmOjr+Rz7P37iIylLXg0xP0OkVknsm0S21Mz0GdiF1dD8MQXpvhSr5gEwmUkAxJ5m4cfZVrJj/NUU5xZQUljVpeo5j+/OYM+5l9Hodw8cPZcDoM7wYbWATQmCtsbFr/a8seehDqsqqCAkNpqSwzK1xI1HmCBS9gikxhiseGE3fC3p7P2jJZ2QykQLOmSNPp1/aaZQXVzDvhkX83sRpOn7dlA3A7vV7iUuJ5dbnr+Hb19ZRU1VL17M6cdFdIwgODfZG6H6vrKiczxas5Nfvs8k7VEhVaWWD9pDqihq3jtOtf2ce/nAiVeXVRJojZNtIGyAHLXqQHOikvd93/MmiSW9z7I9cEBAVF8l5Vw3ii0Ur3V6A6++69j+FSf+5hZiEaM8G6yZfXYPCnGKevWYhOdnHW3ScsCgj7/75f5SUOZ8B3J8F2ufAHjloUZKaoEu/jjz19QOs+/AHSvJKOW/cOfTq34MNn3xPcW7zvsz2bzvI/UOewqDX065zHF3O6MjVj13WaieW/GXTXpY8spS8g/motmZm4P9JOjWBxz+bQrAxGGRfhzZFJhMp4IWEhTDy5vMaPBbfKa7ZyQTAWmXFipU/dx3hz11H+PHrnxn76CUMuWIAekPgNhyrqkpNRQ2Fxy1sXraVjZ9kUXC4aWuLODLgon7cNv9auZxuGyWTidQqnXf12fy567DbdfyulBaUsfj+93lvxnLMySaEEKhWlfiOZsY/fSXtTvG/hZSEEGz8eAtbvvgJvUEhyBjMb5uzseSXIFp2A4K5g4minGJUVRAaZaTPsJ7c9fKNspdcGyaTidQqnXf1ORQfL+H7T3+kwlJJWHQYtdU15P1ZiFCb30xYWVbF4V9PTnWfsz+XaWlzuHX+NRQcKebnNb+i2mycOrALI8YP9Wi7ixACoQp0esdf2EXHLFjySkju0Z4lj3zID59uo7ba6rEYAK6efgkX3noB21b+zJ97jtBvxGl06XeKR59DCjyyAd6DZKOdf/jrOaiqSnVFDcbwEBRF4fBvOWzP3MVnL3xDTWWtV+MIMhqITzHToXt7+g7vSW21ld9++J0O3RMYdev5hEbYn3csLi6Oo4eOsn7pFnL25zLwon5sWr6VXzfvo6aqlvjkuruh5J7t6/epqaxhwW2v88umvVhrbOgMOoQqWpQ4/07RwcCLz2Tiwptclg3091Ggxw/aN8DLZOJB8g3oH1ydgxCCf9/yGjtW7W7WErKeEBRiILFbO04b2p0Lrh9KQud4Ni3fypp3NmGttpLzRy6VpVUIVaA36LFZbQ32DwkLJjI2nOLckrqkAS7XgGmK+I5m7nrpBr7+z3cc+z0PvUFHr8GnctUjlzi9Mzoh0N9HgR4/tMFksnnzZpYuXcqRI0eYPXs2Xbt2tVtux44dLFmyBFVVGTFiBOnp6QCUlZUxf/588vLyiI+PZ/LkyURERLj13DKZNNZWzkG1qaz78Hs+efYryorKW9yLqSUUvYIhSE9tlWero5oVi05h8JhUrn7sUqLjHS+h7Uqgv48CPX7QPpn4vLUsJSWFBx54gF69ejkso6oqixcv5tFHH2X+/Pls3LiRw4cPA7BixQr69OnDggUL6NOnDytWrNAocimQ6fQ6zr92MAu2z+TB9+7iwjuGE9fR7JNYhE34RSLRB+m588Xx3P7C9S1KJFLb5PNkkpyc7DDTnZCdnU1iYiIJCQkYDAYGDx5MVlYWAFlZWQwbNgyAYcOG1T8uSe5QFIXeQ7pzzfR0xj12KTHtTn6JGoINpPRqT5sYvK1At7M6M/DiM30diRSgAqI3V2FhIWbzyV+NZrOZffv2AWCxWDCZTACYTCZKShyPLcjMzCQzMxOAjIwM4uI8253TYDB4/Jhaa8vn8M8b0zj9nF58PPcLKkoqGXz5AIaNHUTGdQvY/NmPJ6vCFIhtbyIkNJic/S0bLe4TCiR2bseEWeNYv2wLlSWV9B7cgzH3jSYoJMgjTxHo76NAjx+0PwdNksnMmTMpLi5u9Pi4ceMYMGCAy/3tNes0Z66ftLQ00tLS6v/2dH2irGf1Dy05h1BzCOOfuaL+78LCQm5/8Xr6/eM01n3wPdYaGz3O6col94zEEGxgy5c7+ODpFdRW1tJz0Kmcdm53ljz0oadOpUWSeyZSVlxJeXE5YdFhnDnyNGqrrHTqk8L51w4iODSYHueebKO0lFqg1DPPHejvo0CPH1rpdCrTp09v0f5ms5mCgoL6vwsKCurvRqKjoykqKsJkMlFUVERUlKzrlTxv4MVn2q0CGnhRPwZe1K/+77xDBUTHR2LJa/itrNMrxHeMI9IUzsE9R6it8ky3ZEWvEB4dRmikkU59ksk/XESwMZgbZl1Jco/22Kw2qitqCI00yskWJa8KiGqurl27kpOTQ25uLrGxsWzatIlJkyYBkJqaytq1a0lPT2ft2rVu3elIkrfEp5jpc34vvv90G9aaukb10MgQ/nnHBVx234UA7N9+kA9nfUrO/uOUFpQ3GAui0+uIbheJMdJIcY4FIeruzKvLqxs8T0h4MJff/09GTRjmdHoXvUHfaucUk/yLz7sGb9myhddff52SkhLCw8Pp1KkTjz32GIWFhSxatIipU6cCsG3bNt58801UVWX48OGMGTMGgNLSUubPn09+fj5xcXFMmTJFdg1uAXkOLSeEYN2HP7D1q5/q1k25fjBnjDjNbjlrjY2vXlnNvi37CQkPYfSdF3D2qFTy8/NRVZXcA/kYw0P4+Nkv+WXTPioslcQkRHHBDUMbzUfmT3x9DVoq0OOHNjjOxJdkMmlMnoPvOYq/oqSSkvxS4pJjMQT7d6VCa70GgaRVtplIktRyYVGhsspK8ls+H2ciSZIkBT6ZTCRJkqQWk8lEkiRJajGZTCRJkqQWk8lEkiRJarE23TVYkiRJ8gx5Z+JBjzzyiK9DaDF5Dr4X6PFD4J9DoMcP2p+DTCaSJElSi8lkIkmSJLWYTCYe9Nfp7QOVPAffC/T4IfDPIdDjB+3PQTbAS5IkSS0m70wkSZKkFpPJRJIkSWoxOWtwC2zevJmlS5dy5MgRZs+eTdeuXe2Wu/vuuzEajeh0OvR6PRkZGRpH6pi757Bjxw6WLFmCqqqMGDGC9PR0bQN1oKysjPnz55OXl0d8fDyTJ0+2u56NP14DV6+pEIIlS5awfft2QkJCmDhxIl26dPFNsHa4in/37t08++yztGvXDoCzzz6bK6+80geR2rdw4UK2bdtGdHQ08+bNa7Td319/cH0Oml4DITXboUOHxJEjR8SMGTNEdna2w3ITJ04UFotFw8jc58452Gw2cc8994hjx46J2tpa8cADD4hDhw5pHKl9b7/9tli+fLkQQojly5eLt99+2245f7sG7rymP/74o5g1a5ZQVVX89ttvYurUqT6KtjF34t+1a5d45plnfBSha7t37xb79+8XU6ZMsbvdn1//E1ydg5bXQFZztUBycrLDhWIChTvnkJ2dTWJiIgkJCRgMBgYPHkxWVpZGETqXlZXFsGHDABg2bJjfxOWKO6/p1q1bOe+881AUhe7du1NeXk5RUZGPIm7In98T7urdu7fTVVn9+fU/wdU5aElWc2lk1qxZAIwcOTLguh0WFhZiNpvr/zabzezbt8+HEZ1ksVgwmUwAmEwmSkpKHJb1p2vgzmtaWFhIXFxcgzKFhYX15+tL7r4n9u7dy4MPPojJZGL8+PGkpKRoGWaL+PPr3xRaXQOZTFyYOXMmxcXFjR4fN24cAwYMcPsYsbGxWCwWnn76aZKSkujdu7eHI3X+/C05B2Gn97iiKJ4IzS3O4m/KMXx5Df7OndfU16+7M+7E1rlzZxYuXIjRaGTbtm0899xzLFiwQKsQW8yfX393aXkNZDJxYfr06S0+RmxsLADR0dEMGDCA7OxsTb/IWnoOZrOZgoKC+r8LCgo0/XXmLP7o6GiKioowmUwUFRURFRVlt5yvr8HfufOams3mBmt4a/26O+NO/GFhYfX/79+/P4sXL6akpMThNfI3/vz6u0vLayDbTLysqqqKysrK+v/v3LmTjh07+jiqpunatSs5OTnk5uZitVrZtGkTqampvg4LgNTUVNauXQvA2rVr7d5p+eM1cOc1TU1NZd26dQgh2Lt3L2FhYX7zZeZO/MXFxfW/7rOzs1FVlcjISF+E2yz+/Pq7S8trIEfAt8CWLVt4/fXXKSkpITw8nE6dOvHYY49RWFjIokWLmDp1KsePH2fu3LkA2Gw2hg4dypgxY3wc+UnunAPAtm3bePPNN1FVleHDh/vNOZSWljJ//nzy8/OJi4tjypQpREREBMQ1sPearly5EoBRo0YhhGDx4sX89NNPBAcHM3HiRIddt33BVfxff/01K1euRK/XExwczA033ECPHj18HPVJL7zwAnv27KG0tJTo6GjGjh2L1WoFAuP1B9fnoOU1kMlEkiRJajFZzSVJkiS1mEwmkiRJUovJZCJJkiS1mEwmkiRJUovJZCJJkiS1mEwmkuSncnNzGTt2LDabzdehSJJLcgS8JAWITZs28dVXX3HgwAG6devGE0884euQJKmeTCaSFCAiIiIYPXo0R48eZdeuXb4OR5IakMlEkjSSn5/PG2+8wS+//IIQgiFDhnDzzTezfPlyVq1aRU1NDf369WPChAkN5lQ6oW/fvgCsWrVK69AlySXZZiJJGlBVlTlz5hAXF8fLL7/MK6+8wpAhQ1izZg1r1qxhxowZvPTSS1RVVbF48WJfhytJTSaTiSRpIDs7m8LCQsaPH4/RaCQ4OJiePXuyYcMGLr74YhISEjAajVx77bVs2rRJNrpLAUcmE0nSQH5+PvHx8ej1+gaPFxUVER8fX/93XFwcNpsNi8WidYiS1CIymUiSBuLi4sjPz290x2EymcjLy6v/Oz8/H71eT3R0tNYhSlKLyGQiSRro1q0bJpOJd999l6qqKmpqavj1118ZMmQIX375Jbm5uVRVVfH+++8zaNCgRncwUNfuUlNTg81mQwhBTU1N/XTjkuRrcgp6SdJIfn4+r7/+Or/++iuKojBkyBBuuukmli1bVt+b64wzzmDChAlERESQm5vLPffcw/vvv49er2fNmjUsXLiwwTGHDRvG3Xff7aMzkqSTZDKRJEmSWkxWc0mSJEktJpOJJEmS1GIymUiSJEktJpOJJEmS1GIymUiSJEktJpOJJEmS1GIymUiSJEktJpOJJEmS1GL/DxpBklfLMVuXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficamos los datos de entrada: col1, col2, color = Y\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X[:,0], X[:,1], c=Y_e)\n",
    "plt.axis(\"equal\")\n",
    "ax.set_xlabel('col1')\n",
    "ax.set_ylabel('col2')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡A programar la red neuronal!\n",
    "\n",
    "Sabemos que, la unidad de procesamiento de la red neuronal es la neurona, y que podemos juntar varias, formando capas, de forma de obtener una codificación, y por ende, un procesamiento de información más compleja.\n",
    "\n",
    "## Otra forma de verlo\n",
    "\n",
    "Casi todas las fórmulas que hemos ocupado en nuestra red neuronal, han sido posibles de vectorizar (que fue lo que continuamente explicó Andrew NG.), y de hecho, esta es la razón por la cual se logra un gran rendimiento con las redes neuronales: la vectorización.\n",
    "\n",
    "Esto último quiere decir que, <b>dentro de una misma capa</b>, estamos realizando las mismas operaciones para todas las neuronas:\n",
    "\n",
    "- Mismas función de activación.\n",
    "\n",
    "- Mismas fórmulas de backpropagation.\n",
    "\n",
    "## ¿Módulos?\n",
    "\n",
    "La neurona es la unidad de procesamiento. ¿Y si consideramos que cada capa es un módulo? Pues sí:\n",
    "\n",
    "- Dentro de una misma capa, se realizan las mismas operaciones.\n",
    "\n",
    "## OOP\n",
    "\n",
    "Lo interesante de pensarlo así es porque da toda una nueva forma de ver el Deep Learning, dado que el módulo principal en este campo es la capa, y pues, cada capa se diferenciará de otra si realiza una operación distinta.\n",
    "\n",
    "Por ello, tomaremos esta perspectiva, y definiremos primero, en nuestra red neuronal:\n",
    "\n",
    "- Una clase, un tipo de objeto, que se refiera a una capa.\n",
    "\n",
    "En este sentido, necesitamos pensar en la capa desde el punto de vista de las conexiones:\n",
    "\n",
    "- Las capas cuentan con pesos (W).\n",
    "\n",
    "- Además, con los parámetros de sesgo o bias (b).\n",
    "\n",
    "En nuestro caso, la clase 'neural_layer' no ejecutará ninguna lógica, o ninguna función, a pesar que podríamos, pero en post de mantener simple el problema, solo la ocuparemos como:\n",
    "\n",
    "- Una estructura de datos.\n",
    "\n",
    "De esta forma, mantendremos solo los parámetros de la capa.\n",
    "\n",
    "Recomiendo [este curso para programación orientada a objetos en Python de Datacamp](https://learn.datacamp.com/courses/object-oriented-programming-in-python). \n",
    "\n",
    "<i>Prontamente realizaré la conversión del mismo curso, que escribí en OneNote a un notebook de Jupyter, pero en orden para ello necesitaré tiempo</i>. EDIT: [¡en camino!](https://github.com/adinamarca/notebooks/blob/main/PY/PGRM/Object_oriented_programming_in_python/notebook.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :(\n",
    "\n",
    "En esta celda debería ir código... Pero nuevamente deberé hacer una pausa para aprender redes neuronales, dado que olvidé gran parte de los fundamentos de OOP, y no deseo hacer por hacer, razón por la cual deberé retomarlos.\n",
    "\n",
    "```\n",
    "# Importamos la clase datetime desde el módulo datetime\n",
    "#from datetime import datetime\n",
    "\n",
    "# Y la fecha...\n",
    "# import datetime\n",
    "# print(datetime.now())\n",
    "```\n",
    "\n",
    "<b color=\"yellow\">OUT</b>: 2021-03-09 22:59:15 en 15:56 min. de video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :)\n",
    "\n",
    "¡Seguimos!\n",
    "\n",
    "```\n",
    "print(datetime.datetime.now())\n",
    "```\n",
    "<b color=\"yellow\">OUT</b>: 2021-03-16 19:44:15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Vamos a OOP!\n",
    "\n",
    "Ahora. Crearemos la clase ```neural_layer```, que no ejecutará lógica, o dicho de otro modo, no ejecutará ninguna función particular (solo tendrá atributos).\n",
    "\n",
    "- Pesos (W).\n",
    "\n",
    "- Bias (b).\n",
    "\n",
    "Algunas preguntas para realizar la clase:\n",
    "\n",
    "- ¿Qué parámetros hacen falta inicializar? \n",
    "\n",
    "- ¿Qué número de conexiones entran de la capa anterior?\n",
    "\n",
    "- ¿Cuántas neuronas hay en nuestra capa?\n",
    "\n",
    "- ¿Cuál es la función de activación?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE DE LA CAPA DE LA RED\n",
    "\n",
    "class neural_layer:\n",
    "\n",
    "    # Función que inicializa capas\n",
    "\n",
    "    def __init__(self, n_conn, n_neur, act_f):\n",
    "\n",
    "        # Función de activación\n",
    "\n",
    "        self.act_f = act_f\n",
    "\n",
    "        # La información anterior la utilizamos para inicializar los parámetros de la capa: creamos el vector de parámetros bias (tantos como neuronas tengamos)\n",
    "\n",
    "        # Para el bias necesitamos un vector columna, con n de columnas igual al n de neuronas (n_neur)\n",
    "\n",
    "        # Con np.random.rand nos devolverá un valor entre 0 y 1 aleatorio: nos interesa que la inicialización aleatoria sea en torno a la media 0 (normalizada y estandarizada)\n",
    "\n",
    "        # Necesitamos valores entre [-1, 1], por lo que realizamos la operación  *2 - 1\n",
    "\n",
    "        self.b = np.random.rand(1, n_neur) * 2 - 1\n",
    "\n",
    "        # Lo mismo para el vector de pesos (matriz de n_conn * n_neur)\n",
    "\n",
    "        # Recordemos que entre dos capas tenemos tantas conexiones como n de neuronas hubiera en la capa anterior y n de neuronas haya en la capa actual\n",
    "\n",
    "        self.W = np.random.rand(n_conn, n_neur) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, habremos definido una clase con la cual podemos crear capas para una red neuronal. No hemos implementado una lógica aún, sino que es una estructura de datos (información, parámetros, etc.), y nada más.\n",
    "\n",
    "Otra cosa que necesitamos implementar es la función de activación.\n",
    "\n",
    "<img src=\"20.png\"></img>\n",
    "\n",
    "Si recordamos, estas funciones introducen no linealidad, de forma que las sumas ponderadas de cada una de las neuronas no termine siendo una única recta (suma de rectas es una recta).\n",
    "\n",
    "Además, teníamos múltiples funciones de activación:\n",
    "\n",
    "- Sigmoide (la que implementaremos).\n",
    "\n",
    "- RELU.\n",
    "\n",
    "- Tanh.\n",
    "\n",
    "<img src=\"21.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES DE ACTIVACIÓN\n",
    "\n",
    "# La implementamos como función anónima, donde si recordamos el n de entradas se mapea y distorsiona en un rango que va desde 0 a 1\n",
    "sigm = lambda x: 1 / (1 + np.e ** (-x))\n",
    "\n",
    "# Variable generada de forma lineal, que va de -5 a 5 con 100 valores\n",
    "_x = np.linspace(-4, 4, 100)\n",
    "\n",
    "# La graficamos\n",
    "plt.plot(_x, sigm(_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adelantando trabajo\n",
    "\n",
    "La red neuronal implementa el algoritmo de backpropagation, y si recordamos, dentro del algoritmo, una de sus derivadas parciales, es la derivada de la función de activación. En ese sentido, en algún momento tendremos que definir la derivada de la función sigmoide, lo que definiremos a continuación (y por comodidad, la definiremos en un par de funciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm = (lambda x: 1 / (1 + np.e ** (-x)),\n",
    "        # Derivada de la función sigmoide\n",
    "        lambda x: x*(1 - x))\n",
    "\n",
    "# Ahora si deseamos acceder a la derivada de la función sigmoide\n",
    "plt.plot(_x, sigm[1](_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordando otras funciones de activación\n",
    "\n",
    "Podríamos buscar en Google cualquier función de activación que nos sea útil, e implementarla. En este sentido, recordemos la RELU, caracterizada por una especie de L inclinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El máximo entre 0 y x (función cuyo dominio positivo era el propio valor de entrada, y el dominio negativo era 0)\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "\n",
    "# La visualizamos\n",
    "plt.plot(_x, relu(_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a diseñar la red neuronal\n",
    "\n",
    "Hemos creado la clase que nos permite crear capas. ¡Manos a la obra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Capa n (layer n: ln)\n",
    "\n",
    "- Conexiones de entrada: p\n",
    "- Neuronas (n): 4\n",
    "- Función de activación: sigm\n",
    "\n",
    "\n",
    "'''\n",
    "l0 = neural_layer(p, 4, sigm)\n",
    "l1 = neural_layer(4, 8, sigm)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda anterior, es una forma válida de crear la red neuronal capa por capa, pero, debemos hacer el código elegante y bonito, y la clase ```neural_layer``` nos invita a hacer todo ésto de forma más iterativa... Y ese será el caso, por lo que lo haremos más cómodo.\n",
    "\n",
    "## Lo cómodo\n",
    "\n",
    "Ahora, definiremos la función que se encargará de crear nuestra red neuronal, por lo que la siguiente celda, se encargará de dicha función, estando a su vez, en función de la variable topology (topología de la red), donde definiremos el número de neuronas que tiene la red:\n",
    "\n",
    "- La primera será p (n de atributos de los datos que nos dará el n de neuronas de nuestra primera capa).\n",
    "\n",
    "- La siguiente capa 4, 8, ..., etc.\n",
    "\n",
    "- Y así, hasta la última capa, que es la neurona de salida (resultado binario).\n",
    "\n",
    "Bien, el n de neuronas que hemos elegido ha sido arbitrario, y prontamente veremos cómo decidir estos valores.\n",
    "\n",
    "Al igual que como hemos definido la topología de la red, podríamos definir un vector con las funciones de activación de cada red, o con los parámetros de aprendizaje de cada capa, etc. Sin embargo, y por simplicidad, el único hiperparámetro que ocuparemos será el número de neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación del código\n",
    "\n",
    "Le pasamos el vector topology y las funciones de activación que tendrán todas las capas (recordemos que no es necesario que todas las capas tengan la misma función de activación, y lo hacemos así por simplicidad).\n",
    "\n",
    "```\n",
    "\n",
    "def create_nn(topology, act_f):\n",
    "\n",
    "    # Vector donde conteneremos todas las capas de nuestra red neuronal (será la estructura de datos que sostenga a cada una de las capas de la red)\n",
    "    nn = []\n",
    "\n",
    "    # La función enumerate brinda tanto el índice del vector que estamos recorriendo (l) como el objeto (layer)\n",
    "\n",
    "    # Le decimos que recorra hasta el último valor para no sobrepasar el índice (l + 1), descartando el último valor\n",
    "    for l, layer in enumerate(topology[:-1]):\n",
    "\n",
    "        # Dentro del vector nn añadiremos un objeto neural_layer() cuya primera capa tenga el n de conexiones que pertenezca al vector topology en la posición l (índice que recorreremos por todo el ciclo for)\n",
    "\n",
    "        # Además, tendremos tantas neuronas como l + 1 \n",
    "\n",
    "        # Es decir, la primera capa oculta que añadiremos a nuestro vector nn será la correspondiente a la primera capa oculta:\n",
    "\n",
    "        # Neuronas: 4\n",
    "        # Conexiones de entradas: p\n",
    "        # Función de activación: act_f\n",
    "\n",
    "        # Nota: si dejamos l+1 causará un overflow en el vector\n",
    "\n",
    "        # Overflow: desbordamiento de búfer que ocurre cuando los datos que se escriben en un búfer corrompen aquellos datos en direcciones de memoria adyacentes a los destinados para el búfer, debido a una falta de validación de los datos de entrada (recorrer un vector en l + 1 de un índice l)\n",
    "\n",
    "        nn.append(neural_layer(topology[l], topology[l+1], act_f))\n",
    "\n",
    "        # Una vez recorrido todo el bucle for, dentro de la variable nn tendremos todas las capas creadas, por lo que estaría automatizado el proceso\n",
    "\n",
    "    return nn\n",
    "\n",
    "# Creamos la red\n",
    "\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "create_nn(topology, sigm)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_nn(topology, act_f):\n",
    "\n",
    "    nn = []\n",
    "\n",
    "    for l, layer in enumerate(topology[:-1]):\n",
    "\n",
    "        nn.append(neural_layer(topology[l], topology[l+1], act_f))\n",
    "\n",
    "    return nn\n",
    "\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "create_nn(topology, sigm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué hicimos?\n",
    "\n",
    "Creamos la estructura de la red: 6 capas ocultas correspondientes al vector topology.\n",
    "\n",
    "Con esto, tendríamos nuestra red neuronal creada, es decir, la estructura de datos que soporta toda nuestra red neuronal.\n",
    "\n",
    "¡Pero de momento no hemos implementado nada de lógica para entrenar este tipo de redes!\n",
    "\n",
    "Es decir, ahora mismo, no sirve. Por lo que ahora, pasaremos a ver todo el código que nos permite entrenar a nuestra red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programando la lógica de la red neuronal\n",
    "\n",
    "A continuación, definiremos la celda que entrenará a la red, con toda lógica programada.\n",
    "\n",
    "## Recordando el primer paso\n",
    "\n",
    "Recordemos que el paso de entrenar a la red neuronal tiene tres elementos fundamentales.\n",
    "\n",
    "El primero es, un paso hacia adelante. En este sentido, le mostraremos a la red neuronal:\n",
    "    \n",
    "- Un tipo de dato de entrada.\n",
    "        \n",
    "- Un dato de salida (resultado a obtener).\n",
    "\n",
    "Y allí, la red neuronal procesará hacia adelante por todas su capas y neuronas, ejecutando:\n",
    "\n",
    "- Sumas ponderadas.\n",
    "\n",
    "- Funciones de activación.\n",
    "\n",
    "Hasta llegar al final y brindar un valor. Ahora bien, en este caso, como no está entrenada, nos dará un valor aleatorio, pero lo normal, es que con el paso del entrenamiento, este valor se terminase asemejando a nuestro vector $Y$. \n",
    "\n",
    "El anterior, sería el primer elemento: el paso adelante de la información.\n",
    "\n",
    "## Recordando el segundo elemento\n",
    "\n",
    "El segundo elemento sería, una vez tenemos el resultado, lo comparamos con el vector $Y$ (resultado real o esperado). Allí, compararíamos usando la función de coste, la que nos brindará el cómo se diferencian ambos resultados.\n",
    "\n",
    "Lo anterior, nos generará un error, de forma que el error lo utilizaremos para hacer una propagación hacia atrás (el algoritmo de backpropagation), para así calcular las derivadas parciales, que son las que nos permitirán obtener la información necesaria para ejecutar la tercera pieza.\n",
    "\n",
    "## Recordando el tercer elemento\n",
    "\n",
    "La tercera pieza es el algoritmo del descenso del gradiente, que nos permitirá optimizar la función de coste y entrenar la red.\n",
    "\n",
    "## Retropropagación al pasado (extracto del vídeo de explicando las matemáticas de backpropagation)\n",
    "\n",
    "El aprendizaje, o lo que es el entrenamiento real, la optimización por la cual minimizaremos el coste de la red neuronal (el entrenamiento de la red) se realiza en base al algoritmo del descenso del gradiente. Sin embargo, para que el descenso del gradiente pueda funcionar, necesita el vector gradiente, que es el vector de las derivadas parciales de los parámetros con respecto al coste, y estas derivadas parciales no las da el algoritmo de Backpropagation.\n",
    "\n",
    "En este sentido:\n",
    "\n",
    "- Descenso del gradiente: todas las responsabilidades de las neuronas.\n",
    "\n",
    "- Backpropagation: proceso de retropropagar el error por cada neurona.\n",
    "\n",
    "<img src=\"22.png\"></img>\n",
    "\n",
    "Aún no he visto el algoritmo de backpropagation, pero deseo hacerme cierta intuición respecto a lo que estamos realizando antes de proceder a programar. Además, deseo volver a este notebook cuando verdaderamente conozca el algoritmo, de forma de someter a una función de costo mi intuición (no soy un buen bromista).\n",
    "\n",
    "Nos apoyaremos un poco en la explicación de DOT.CSV :).\n",
    "\n",
    "## El intento de entender la repropagación de errores\n",
    "\n",
    "Pues, intentemos.\n",
    "\n",
    "<img src=\"23.png\"></img>\n",
    "\n",
    "Primero, partamos por comprender las derivadas parciales.\n",
    "\n",
    "### El primer intento\n",
    "\n",
    "Dado que, al realizar el algoritmo de backpropagation estamos sometiendo a juicio a cada una de las capas y neuronas, debemos buscar responsables. En este sentido, nos realizamos estas preguntas:\n",
    "\n",
    "- ¿Qué parámetros podemos variar inmediatamente? Pues sí, la suma ponderada:\n",
    "\n",
    "    - ¿Cómo varía el coste cuando varíamos los pesos $W$?\n",
    "\n",
    "    - ¿Cómo varía el coste cuando varíamos los bias $b$?\n",
    "\n",
    "Partamos con la primera pregunta respecto a los pesos.\n",
    "\n",
    "### La derivada parcial del peso de la capa L\n",
    "\n",
    "Recordemos que los pesos, en primera instancia, están sometidos a la suma ponderada (1° función), en segundo lugar, a la función de activación (2° función), y en tercer lugar, al coste (3° función). En este sentido, ¡es una COMPOSICIÓN DE FUNCIONES la que debemos derivar! Por lo que necesitamos de la regla de la cadena:\n",
    "\n",
    "- Derivamos primero $W^{L}$ en torno a la suma ponderada.\n",
    "\n",
    "- Derivamos segundo la suma ponderada en torno a la función de activación.\n",
    "\n",
    "- Derivamos tercero la suma ponderada en torno al coste.\n",
    "\n",
    "### La derivada parcial del bias de la capa L\n",
    "\n",
    "Como sabemos, el bias, de igual forma que los pesos, están sometido a una composición de funciones, por lo que repetimos las mismas derivadas parciales:\n",
    "\n",
    "- Derivamos primero $b^{L}$ en torno a la suma ponderada.\n",
    "\n",
    "- Derivamos segundo la suma ponderada en torno a la función de activación.\n",
    "\n",
    "- Derivamos tercero la suma ponderada en torno al coste.\n",
    "\n",
    "### El álgebra y el cálculo\n",
    "\n",
    "Tras derivar estas derivadas parciales, obtenemos:\n",
    "\n",
    "<img src=\"24.png\"></img>\n",
    "\n",
    "Y aquí apreciamos algo interesante.\n",
    "\n",
    "<img src=\"25.png\"></img>\n",
    "\n",
    "Al realizar un poco de álgebra, obtenemos una simplificación que nos resulta en una nueva derivada: la derivada del coste y la suma ponderada, que da cuenta de cómo varía el error en función de la suma ponderada (Z).\n",
    "\n",
    "Es decir, lo que nos cuenta esta derivada es en qué grado se modifica el coste o el error, cuando se produce un pequeño cambio en la suma de la neurona. Si esta derivada es grande, es que ante un pequeño cambio en el valor de la neurona, este se verá reflejado en el resultado final.\n",
    "\n",
    "<img src=\"26.png\"></img>\n",
    "\n",
    "Y por el contrario, si la derivada es pequeña, da igual cómo variemos el valor de la suma, ya que no afectará el error de la red.\n",
    "\n",
    "<img src=\"27.png\"></img>\n",
    "\n",
    "¡La derivada nos dirá la responsabilidad de la neurona en el resultado final, y por tanto, el error!\n",
    "\n",
    "### El error imputado a la neurona\n",
    "\n",
    "Si la neurona, es responsable, habrá que ocupar esta información para realizar correcciones, y esta misma lógica, se le suele denominar como <b>el error imputado a la neurona</b>.\n",
    "\n",
    "<img src=\"28.png\"></img>\n",
    "\n",
    "Por lo tanto, y para simplificar, podemos reestructurar nuestra expresión inicial de esta manera: en función de los errores de la capa L.\n",
    "\n",
    "<img src=\"29.png\"></img>\n",
    "\n",
    "Así pues, por un lado tenemos la derivada del coste respecto al término bias es igual al error de las neuronas (dado que el bias es constante).\n",
    "\n",
    "<img src=\"30.png\"></img>\n",
    "\n",
    "Y la derivada del coste respecto al peso es igual al error de la neurona, multiplicado por la activación de la capa previa.\n",
    "\n",
    "<img src=\"31.png\"></img>\n",
    "\n",
    "Así, hemos deducido tres expresiones diferentes que nos permiten obtener las derivadas parciales que estamos buscando para la última capa.\n",
    "\n",
    "<img src=\"32.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función que se encargará de entrenar a nuestra red neuronal\n",
    "def train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que el paso de entrenar a una red neuronal tiene tres elementos fundamentales:\n",
    "\n",
    "- El primero es, <b>un paso hacia adelante de la información</b>:\n",
    "\n",
    "    - Le mostraremos a la red neuronal un tipo de dato de entrada y un dato de salida, y allí la red neuronal procesará hacia adelante, por todas su capas y neuronas, ejecutando sumas ponderadas, funciones de activación, etc. Hasta llegar a un valor.\n",
    "\n",
    "    - En este caso, dado que la red no estará entrenada, nos brindará un valor con alto error dado que el valor será aleatorio. Sin embargo, lo normal es que este vector se fuese asemejando al vector Y a lo largo del entrenamiento.\n",
    "\n",
    "- El segundo elemento es, una vez tenemos el resultado, comparar con el vector Y esperado. Y aquí, utilizaríamos la función de coste. Esto último, nos generará un error, de forma que este error se utilizará para realizar una <b>propagación hacia atras</b>: backpropagation. Así entonces, calcularíamos las derivadas parciales que nos permitirán ejecutar la tercera pieza.\n",
    "\n",
    "- La tercera pieza es el <b>algoritmo del descenso del gradiente</b>: se optimiza la función de coste y por ende, se entrena la red.\n",
    "\n",
    "    - Aquí es importante recordar el learning rate (parámetro de aprendizaje), que nos permite definir a qué ritmo aprende la red, y debe ser establecido cautelosamente, de otra forma, nuestra red neuronal puede converger o diverger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "create_nn(topology, sigm)\n",
    "\n",
    "# Definiremos la función de coste (error cuadrático medio), con Yp (Y PREDICHA) como Yr (Y REAL)\n",
    "\n",
    "# Definimos la media, dado que, posiblemente, tendremos muchos elementos en el vector\n",
    "\n",
    "Dado que en algún momento necesitaremos la derivada de l2_cost la definimos antes\n",
    "\n",
    "# [(MEDIA((ERROR)**2)), derivada de lo anterior]\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función que se encargará de entrenar a nuestra red neuronal\n",
    "def train():\n",
    "    pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordar el tipo de clase de neural_net\n",
    "neural_net = create_nn(topology, sigm)\n",
    "type(neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "# Forward pass, backward pass and gradient descent\n",
    "\n",
    "# Deberemos pasar nuestros datos de entrada y salida, así como la función de coste y el learning rate a nuestro descenso del gradiente\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5):\n",
    "\n",
    "    # Forward pass (pasar el vector de entrada capa por capa, ejecutando en cada capa la suma ponderada y la función de activación)\n",
    "\n",
    "    # A continuación, definimos la suma ponderada, donde multiplicamos matricialmente (@) X por la capa 1, tomando los parámetros W y b de la clase del objeto neural_net\n",
    "    z = X @ neural_net[0].W + neural_net[0].b\n",
    "\n",
    "    # Crear la activación de la capa 1 (guardándose en a, que es la salida de la capa 1)\n",
    "    a = neural_net[0].act_f(z)\n",
    "\n",
    "    # Y luego, si quisiéramos con la capa 2...\n",
    "    # ¡Repetición! Así que esto nos invita a hacerlo de forma iterativa\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5):\n",
    "\n",
    "    # Recorre todas las capas en nuestra red neuronal, utilizando el índice l para recorrer c/u de las capas\n",
    "\n",
    "    # Ahora, X serviría solo para la primera capa oculta, dado que luego tendríamos que tomar las salidas de la primera capa oculta para utilizarlas en la segunda, y etc. Por lo que necesitamos un vector que almacene esa información\n",
    "\n",
    "    # Lo que es útil, es que debemos guardar los pares de información donde guardemos tanto el valor de la suma ponderada, como el valor de la activación\n",
    "    # out = [(z0, a0), (z1, a1), etc.]\n",
    "    out = []\n",
    "\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        # nos referrimos al último vector de out\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f(z)\n",
    "        out.append((z, a))\n",
    "\n",
    "        # Con el código anterior tenemos listo el procesamiento hacia adelante de nuestra red neuronal\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5):\n",
    "\n",
    "    # En la primera iteración el vector estará vacío, por lo que debemos colocar datos referentes a la primera capa, donde la suma ponderada no existe al ser la capa de entrada: el output será el vector X\n",
    "    out = [(None, X)]\n",
    "\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f[0](z)\n",
    "        out.append((z, a))\n",
    "\n",
    "        # Con el código anterior tenemos listo el procesamiento hacia adelante de nuestra red neuronal\n",
    "        # Imprimimos el error dado que evaluamos en la función de costo el valor predicho vs. real\n",
    "    print(l2_cost[0](out[-1][1], Y))\n",
    "\n",
    "# Valor aleatorio\n",
    "train(neural_net, X, Y, l2_cost, 0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora vamos para atrás\n",
    "\n",
    "Por lo que entrenaremos nuestra red, ya que tenemos el error ya calculado (Y predicha menos Y real) y entonces, podemos aplicar el algoritmo de backpropagation.\n",
    "\n",
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5):\n",
    "    out = [(None, X)]\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f[0](z)\n",
    "        out.append((z, a))\n",
    "    print(l2_cost[0](out[-1][1], Y))\n",
    "train(neural_net, X, Y, l2_cost, 0.5)\n",
    "```\n",
    "\n",
    "\n",
    "La lógica implementada en la función la podríamos utilizar no solamente en forward pass, sino que también podríamos utilizarla en realizar una simple predicción (sin necesidad de entrenar a la red). En este sentido, podríamos separar la función en dos funciones diferentes, pero en nuestro caso, ¡lo haremos al estilo DOT.CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La nota del perdido\n",
    "\n",
    "Lo que se quiso decir en la celda anterior: la función anterior puede ser utilizada como una simple función de costo referente a predecir, sin necesidad de entrenar a la red. Por lo que, debería ser separada en dos funciones (el código está algo chapucero), pero se arreglará a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5, train=True):\n",
    "    out = [(None, X)]\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f[0](z)\n",
    "        out.append((z, a))\n",
    "    print(l2_cost[0](out[-1][1], Y))\n",
    "train(neural_net, X, Y, l2_cost, 0.5)\n",
    "\n",
    "#if train:\n",
    "    # Training:\n",
    "    # Backward pass: propagamos el error hacia atrás con el error imputado a la neurona\n",
    "    # Gradient descent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El error\n",
    "\n",
    "En este momento, al no estar entrenando la red, el error resultante es totalmente aleatorio, o por lo menos, hasta que implementemos los algoritmos de backpropagation y gradient descent (propagación de errores y descenso del gradiente).\n",
    "\n",
    "## Lo hecho\n",
    "\n",
    "Teniendo listo:\n",
    "\n",
    "- Parámetros de entrada.\n",
    "\n",
    "- Estructura la red neuronal en función del n° de capas y n° de neuronas (topología).\n",
    "\n",
    "- Cálculo de sumas ponderadas $Z$.\n",
    "\n",
    "- Cálculo de la función de activación sobre $Z$.\n",
    "\n",
    "- Función de error de $Y \\: predicha \\: en \\: base \\: a \\: Y \\: real$.\n",
    "\n",
    "## La nota del perdido (continuación)\n",
    "\n",
    "Ahora que la estructura básica está lista, y nos arroja una resultado, lo que necesitamos ahora es ajustar este resultado azaroso, puesto en sí, no tiene ninguna lógica, y no existen responsabilidades reales de cada una de las neuronas. En razón de ello, debemos saber qué neuronas son responsables del error, indagando con derivadas parciales el cambio cuando modificamos minímamente cada uno de sus parámetros (pesos y bias) en función del costo. Para ello, deberemos retropropagar los errores: las últimas capas que sean responsables del error, retropropagarán el error hasta las capas ocultas, encontrando aquellas neuronas responsables y modificando sus parámetros en función del costo a través de la iteración, gracias al descenso del gradiente.\n",
    "\n",
    "En este sentido, debemos calcular entre derivadas parciales, e iterar, gracias a los algoritmos de la propagación de error y descenso del gradiente.\n",
    "\n",
    "Puede ser algo confuso... Especialmente si no tenemos conocimiento de:\n",
    "\n",
    "- Derivadas en dos dimensiones y derivadas parciales en tres dimensiones.\n",
    "\n",
    "- Desconocimiento de la estructura de una red neuronal.\n",
    "\n",
    "- Función de costo.\n",
    "\n",
    "- Descenso del gradiente.\n",
    "\n",
    "- Conocimiento parcial del algoritmo de Backpropagation.\n",
    "\n",
    "- Función de activación (sigmoide que nos entrega probabilidades).\n",
    "\n",
    "- Linealidad al sumar rectas (razón por la que usamos la función de activación).\n",
    "\n",
    "- Álgebra matricial.\n",
    "\n",
    "- Programación.\n",
    "\n",
    "... Algunas, quizás no tan necesarias (y mucho se puede arguir al respecto), pero tener un conocimiento breve o parcial de cada una de ellas es sumamente útil. Y sí, quizás se me olvidaron algunas. Sin embargo, recomiendo previamente a entrar en redes neuronales, saber:\n",
    "\n",
    "- Regresión lineal.\n",
    "\n",
    "- Regresión logística.\n",
    "\n",
    "De esta forma, nos aseguramos tener una base, puesto ambos algoritmos están íntimamente relacionados con la estructura de una red neuronal.\n",
    "\n",
    "Aún más importante para el autodidacta, es plantearse la sgte. pregunta: ¿por qué al ser una red neuronal un algoritmo más complejo necesito aprender regresión lineal y logística? Pues, la respuesta ya es explícita, y seamos majaderos nuevamente: ESTÁN ÍNTIMAMENTE RELACIONADOS.\n",
    "\n",
    "Sería como aprender a restar sin saber sumar. Son conocimientos que previamente debemos adquirir, de otro modo, no comprenderemos la totalidad del algoritmo (o parcialmente, que es útil).\n",
    "\n",
    "A modo personal, me sería difícil implementar una red neuronal desde cero, pero creo podría hacerlo. Se habla mucho que las librerías nos facilitan la vida, y de ello no hay duda, pero al facilitarnos la vida, también nos evitan la parte pesada, y en ello, el experimentar, el fallar... Quizás por ello es importante intentar, al menos, comprender los algoritmos, puesto en esa medida, interiorizamos el contenido (aunque sea un poco), y no nos quedamos con lo mecánico de escribir un par de líneas de código y creer que ya lo sabemos todo, puesto en la práctica, nos mentiríamos a nosotros mismos.\n",
    "\n",
    "## ¡Vamos!\n",
    "\n",
    "Pues, como decíamos, la red neuronal aún no está siendo entrenada:\n",
    "\n",
    "- Hemos ido hacia adelante, pero no hacia atrás.\n",
    "\n",
    "Ahora, entrenaremos nuestra red, y para ello, necesitaremos del descenso del gradiente y la retropropagación de errores.\n",
    "\n",
    "## Training (al modo fitness)\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "El algoritmo de Backpropagation consiste en calcular las derivadas parciales que necesitamos para el algoritmo del descenso del gradiente. En este sentido, las derivadas parciales que necesitamos respecto al coste, son:\n",
    "\n",
    "- $W$.\n",
    "\n",
    "- $b$.\n",
    "\n",
    "Para implementar esto, necesitamos retropropagar el error hacia atrás, y para ello, necesitamos imputar los errores a las neuronas responsables ($\\delta$).\n",
    "\n",
    "En este sentido, íbamos calculando nuestras $\\delta$ según íbamos hacia atrás, puesto teníamos dos fórmulas que nos permitía calcularlo.\n",
    "\n",
    "#### (1) Cómputo del error de la última capa\n",
    "\n",
    "Una de las fórmulas era la que nos brindaba el error imputado a las neuronas en la última capa (caso especial al venir el error directamente de la función de coste).\n",
    "\n",
    "$\\Large\\delta^{L} = \\frac{\\partial C}{\\partial a^{L}}\\frac{\\partial a^{L}}{\\partial Z^{L}}$\n",
    "\n",
    "#### (1) Retropropagamos el error a la capa anterior\n",
    "\n",
    "$\\Large\\delta^{l-1} = W^{l}\\delta^{l}\\frac{\\partial a^{l-1}}{\\partial Z^{l-1}}$\n",
    "\n",
    "#### (2) Calculamos las derivadas de la capa usando el error\n",
    "\n",
    "Luego, tenemos esta fórmula que nos permite calcular los delta ($\\delta$) en función de los delta de la capa anterior.\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial b^{l-1}} = \\delta^{l-1}$\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial W^{l-1}} = \\delta^{l-1}a^{l-2}$\n",
    "\n",
    "Y pues, implementando estas fórmulas, tendríamos nuestras derivadas parciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferencias entre append (al final) e insert (arbitrario por el programador)\n",
    "\n",
    "h = [1]\n",
    "\n",
    "# append\n",
    "h.append(2)\n",
    "print('con append', h)\n",
    "\n",
    "h = [1]\n",
    "\n",
    "# insert\n",
    "h.insert(0, 2)\n",
    "print('con insert', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación\n",
    "\n",
    "A continuación, el código con una explicación anotada en comentarios (medios desordenados).\n",
    "\n",
    "```\n",
    "topology = [p, 4, 8, 16, 8, 4, 1, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yr - Yp))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5, train=True):\n",
    "    out = [(None, X)]\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f[0](z)\n",
    "        out.append((z, a))\n",
    "    #print(l2_cost[0](out[-1][1], Y))\n",
    "\n",
    "\n",
    "    if train:\n",
    "        \n",
    "        # Backward pass: propagamos el error hacia atrás con el error imputado a la neurona\n",
    "    \n",
    "        # Almacenamos todos los delta que vayamos calculando\n",
    "        deltas = []\n",
    "    \n",
    "        # Debido a que iremos hacia atrás, necesitamos un bucle for que vaya desde atrás hacia adelante\n",
    "        # Como habíamos dicho, tenemos dos casos diferentes: el caso en que nos encontremos en la última capa (caso especial) y en el resto de las capas\n",
    "        \n",
    "        for l in reversed(range(0, len(neural_net))):\n",
    "        \n",
    "            z = out[l+1][0]\n",
    "            a = out[l+1][1]\n",
    "        \n",
    "            # Tamaños de las salidas por capa si fuéramos hacia atrás\n",
    "            # print(a.shape)\n",
    "        \n",
    "            # Caso especial: estamos en la última capa\n",
    "            if l == len(neural_net) - 1:\n",
    "                \n",
    "                # Calcular delta última capa (cómputo del error de la última capa)\n",
    "                # l2_cost (derivada) para valores a (output últ. capa) e Y\n",
    "                # y lo otro, lo multiplicamos por la derivada de la func. act. de la últ. capa evaluada en a (punto de act. de neurona)\n",
    "                \n",
    "                deltas.insert(0, l2_cost[1](a, Y)* neural_net[1].act_f[1](a))\n",
    "            \n",
    "            # Gradient descent\n",
    "            else:\n",
    "                \n",
    "                # Calcular delta respecto a capa previa\n",
    "                # casi todo igual excp. a delta calculada previamente, de la capa anterior\n",
    "                # siendo multiplicado matricialmente por el vector de pesos correspondiente a la capa siguiente (conexiones entre la capa actual a la siguiente, puesto el error estará en la capa sgte. y queremos moverlo hacia atrás)\n",
    "                \n",
    "                deltas.insert(0, deltas[0] @ _W.T * neural_net[1].act_f[1](a))\n",
    "                \n",
    "                 # cuando hacemos referencia a la W, deberíamos guardarla temporalmente para hacer uso de ella en la prox. iteración\n",
    "            _W = neural_net[l].W\n",
    "                \n",
    "                # hacemos uso de los delta para optimizar los parámetros de la red\n",
    "                # el descenso del gradiente requiere que pasemos por c/u de las capas de la red neuronal\n",
    "                # en vez de hacer otro bucle for hacia adelante, mientras hacemos el pass hacia atrás, podemos ir actualizando los parámetros en función de los vectores gradientes\n",
    "                # lo único que necesitamos con b es restarle el valor de los parámetros b de nuestra capa con respecto al coste, y esto lo obteníamos en función de la derivada parcial que en función de delta, simplemente era delta\n",
    "                # cogemos delta y lo restamos\n",
    "                \n",
    "            neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * lr\n",
    "            neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * lr\n",
    "                \n",
    "    return out[-1][1]\n",
    "train(neural_net, X, Y, l2_cost, 0.5)\n",
    "print(\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50500178],\n",
       "       [0.50709694],\n",
       "       [0.50571081],\n",
       "       [0.50298137],\n",
       "       [0.50255053],\n",
       "       [0.50134898],\n",
       "       [0.5027774 ],\n",
       "       [0.51315118],\n",
       "       [0.51206711],\n",
       "       [0.50684907],\n",
       "       [0.50834261],\n",
       "       [0.50424982],\n",
       "       [0.50470214],\n",
       "       [0.50674519],\n",
       "       [0.50273873],\n",
       "       [0.51143733],\n",
       "       [0.51019371],\n",
       "       [0.5135279 ],\n",
       "       [0.51097407],\n",
       "       [0.5068092 ],\n",
       "       [0.50213732],\n",
       "       [0.50742365],\n",
       "       [0.50243153],\n",
       "       [0.50990153],\n",
       "       [0.50798254],\n",
       "       [0.49918779],\n",
       "       [0.49961365],\n",
       "       [0.50736323],\n",
       "       [0.49832704],\n",
       "       [0.50468349],\n",
       "       [0.51355967],\n",
       "       [0.51359521],\n",
       "       [0.50952222],\n",
       "       [0.50424444],\n",
       "       [0.5030662 ],\n",
       "       [0.51113881],\n",
       "       [0.51081107],\n",
       "       [0.49940774],\n",
       "       [0.50102807],\n",
       "       [0.50973761],\n",
       "       [0.50375889],\n",
       "       [0.50908052],\n",
       "       [0.50884882],\n",
       "       [0.50116463],\n",
       "       [0.50281904],\n",
       "       [0.50407635],\n",
       "       [0.499056  ],\n",
       "       [0.50347951],\n",
       "       [0.51086381],\n",
       "       [0.50551224],\n",
       "       [0.51359835],\n",
       "       [0.50855175],\n",
       "       [0.50051606],\n",
       "       [0.51245154],\n",
       "       [0.50470308],\n",
       "       [0.49813987],\n",
       "       [0.50234202],\n",
       "       [0.49844439],\n",
       "       [0.51004574],\n",
       "       [0.50649095],\n",
       "       [0.49796692],\n",
       "       [0.4995409 ],\n",
       "       [0.49935613],\n",
       "       [0.50264381],\n",
       "       [0.50635463],\n",
       "       [0.50454353],\n",
       "       [0.50826648],\n",
       "       [0.50233112],\n",
       "       [0.51019578],\n",
       "       [0.50871908],\n",
       "       [0.50639191],\n",
       "       [0.50469474],\n",
       "       [0.50266729],\n",
       "       [0.51159344],\n",
       "       [0.50225913],\n",
       "       [0.50670672],\n",
       "       [0.51296343],\n",
       "       [0.50641655],\n",
       "       [0.49821989],\n",
       "       [0.50324241],\n",
       "       [0.50607989],\n",
       "       [0.50142142],\n",
       "       [0.50225352],\n",
       "       [0.50260776],\n",
       "       [0.50983579],\n",
       "       [0.50877323],\n",
       "       [0.5000365 ],\n",
       "       [0.50637052],\n",
       "       [0.50894476],\n",
       "       [0.50464387],\n",
       "       [0.50995295],\n",
       "       [0.50815722],\n",
       "       [0.50758378],\n",
       "       [0.51306232],\n",
       "       [0.50242084],\n",
       "       [0.51006591],\n",
       "       [0.50213537],\n",
       "       [0.50161904],\n",
       "       [0.50196706],\n",
       "       [0.49913751],\n",
       "       [0.50707217],\n",
       "       [0.50090107],\n",
       "       [0.50999362],\n",
       "       [0.50448004],\n",
       "       [0.50730416],\n",
       "       [0.50625696],\n",
       "       [0.5094732 ],\n",
       "       [0.50610464],\n",
       "       [0.51362118],\n",
       "       [0.50356314],\n",
       "       [0.50375592],\n",
       "       [0.5023203 ],\n",
       "       [0.50596168],\n",
       "       [0.5082137 ],\n",
       "       [0.50314405],\n",
       "       [0.51002126],\n",
       "       [0.49821278],\n",
       "       [0.50617234],\n",
       "       [0.50738652],\n",
       "       [0.50732722],\n",
       "       [0.50315541],\n",
       "       [0.51145149],\n",
       "       [0.50572446],\n",
       "       [0.51004906],\n",
       "       [0.50799402],\n",
       "       [0.50297661],\n",
       "       [0.4990823 ],\n",
       "       [0.49904538],\n",
       "       [0.5098586 ],\n",
       "       [0.51370593],\n",
       "       [0.51322602],\n",
       "       [0.5015707 ],\n",
       "       [0.50776414],\n",
       "       [0.50075059],\n",
       "       [0.50882355],\n",
       "       [0.50526542],\n",
       "       [0.50391305],\n",
       "       [0.50948302],\n",
       "       [0.51182089],\n",
       "       [0.51128058],\n",
       "       [0.5066866 ],\n",
       "       [0.4986475 ],\n",
       "       [0.5042977 ],\n",
       "       [0.50888745],\n",
       "       [0.50256257],\n",
       "       [0.50318226],\n",
       "       [0.5087101 ],\n",
       "       [0.50217638],\n",
       "       [0.50374367],\n",
       "       [0.50243586],\n",
       "       [0.51359854],\n",
       "       [0.4995778 ],\n",
       "       [0.50435435],\n",
       "       [0.50740134],\n",
       "       [0.50349288],\n",
       "       [0.49813409],\n",
       "       [0.50406563],\n",
       "       [0.50090634],\n",
       "       [0.50216257],\n",
       "       [0.50766212],\n",
       "       [0.50566795],\n",
       "       [0.50222747],\n",
       "       [0.50763446],\n",
       "       [0.50808184],\n",
       "       [0.5022097 ],\n",
       "       [0.49823293],\n",
       "       [0.49942499],\n",
       "       [0.50901225],\n",
       "       [0.51007237],\n",
       "       [0.50743881],\n",
       "       [0.49817828],\n",
       "       [0.50815733],\n",
       "       [0.50498385],\n",
       "       [0.50812431],\n",
       "       [0.49896492],\n",
       "       [0.50510623],\n",
       "       [0.50121413],\n",
       "       [0.51332764],\n",
       "       [0.50537011],\n",
       "       [0.50837754],\n",
       "       [0.50208035],\n",
       "       [0.50823183],\n",
       "       [0.50272873],\n",
       "       [0.51313791],\n",
       "       [0.5100206 ],\n",
       "       [0.49865518],\n",
       "       [0.50639815],\n",
       "       [0.50219824],\n",
       "       [0.50120531],\n",
       "       [0.50241711],\n",
       "       [0.50209958],\n",
       "       [0.50572686],\n",
       "       [0.50653167],\n",
       "       [0.50439394],\n",
       "       [0.5100453 ],\n",
       "       [0.50654881],\n",
       "       [0.49834153],\n",
       "       [0.51190776],\n",
       "       [0.50526453],\n",
       "       [0.50811135],\n",
       "       [0.50240246],\n",
       "       [0.50219873],\n",
       "       [0.50940333],\n",
       "       [0.50765896],\n",
       "       [0.50467582],\n",
       "       [0.50858555],\n",
       "       [0.50519746],\n",
       "       [0.50856272],\n",
       "       [0.50288837],\n",
       "       [0.50330654],\n",
       "       [0.51235778],\n",
       "       [0.50335122],\n",
       "       [0.51225317],\n",
       "       [0.49926062],\n",
       "       [0.51294352],\n",
       "       [0.51337055],\n",
       "       [0.50274472],\n",
       "       [0.502301  ],\n",
       "       [0.50223983],\n",
       "       [0.50279014],\n",
       "       [0.50830839],\n",
       "       [0.50236053],\n",
       "       [0.50873213],\n",
       "       [0.50723629],\n",
       "       [0.50387745],\n",
       "       [0.51036617],\n",
       "       [0.49937573],\n",
       "       [0.51000967],\n",
       "       [0.5021089 ],\n",
       "       [0.50515768],\n",
       "       [0.50411279],\n",
       "       [0.50509666],\n",
       "       [0.50216598],\n",
       "       [0.50939618],\n",
       "       [0.51002112],\n",
       "       [0.50065386],\n",
       "       [0.50530949],\n",
       "       [0.50351837],\n",
       "       [0.50748783],\n",
       "       [0.50592015],\n",
       "       [0.51012932],\n",
       "       [0.49817292],\n",
       "       [0.51360093],\n",
       "       [0.51111072],\n",
       "       [0.50295783],\n",
       "       [0.50413106],\n",
       "       [0.51350511],\n",
       "       [0.49837036],\n",
       "       [0.50217461],\n",
       "       [0.5063938 ],\n",
       "       [0.4992537 ],\n",
       "       [0.50500538],\n",
       "       [0.51251738],\n",
       "       [0.51344321],\n",
       "       [0.50887452],\n",
       "       [0.50339275],\n",
       "       [0.50934014],\n",
       "       [0.50171133],\n",
       "       [0.50499185],\n",
       "       [0.51372812],\n",
       "       [0.50428075],\n",
       "       [0.50471797],\n",
       "       [0.50006143],\n",
       "       [0.50337647],\n",
       "       [0.50553336],\n",
       "       [0.50333929],\n",
       "       [0.50848582],\n",
       "       [0.50968023],\n",
       "       [0.50227726],\n",
       "       [0.50794766],\n",
       "       [0.50963267],\n",
       "       [0.50479187],\n",
       "       [0.51014613],\n",
       "       [0.50983089],\n",
       "       [0.50815635],\n",
       "       [0.50933815],\n",
       "       [0.50490248],\n",
       "       [0.51361361],\n",
       "       [0.50672871],\n",
       "       [0.50669599],\n",
       "       [0.51073088],\n",
       "       [0.50955161],\n",
       "       [0.49876251],\n",
       "       [0.50264136],\n",
       "       [0.51231233],\n",
       "       [0.50271558],\n",
       "       [0.50692834],\n",
       "       [0.50051319],\n",
       "       [0.50484049],\n",
       "       [0.50396285],\n",
       "       [0.50827259],\n",
       "       [0.49943002],\n",
       "       [0.4981952 ],\n",
       "       [0.50342427],\n",
       "       [0.51374014],\n",
       "       [0.50359714],\n",
       "       [0.50986458],\n",
       "       [0.50438057],\n",
       "       [0.50999756],\n",
       "       [0.51040879],\n",
       "       [0.49877614],\n",
       "       [0.50819583],\n",
       "       [0.50618742],\n",
       "       [0.50307408],\n",
       "       [0.50445641],\n",
       "       [0.51313998],\n",
       "       [0.4991309 ],\n",
       "       [0.50161632],\n",
       "       [0.51127546],\n",
       "       [0.50602036],\n",
       "       [0.50388316],\n",
       "       [0.50767027],\n",
       "       [0.50454651],\n",
       "       [0.49832372],\n",
       "       [0.50278742],\n",
       "       [0.49856051],\n",
       "       [0.50255029],\n",
       "       [0.50445018],\n",
       "       [0.50246081],\n",
       "       [0.50411027],\n",
       "       [0.5051382 ],\n",
       "       [0.49829638],\n",
       "       [0.5125162 ],\n",
       "       [0.50565264],\n",
       "       [0.50244905],\n",
       "       [0.50231165],\n",
       "       [0.50541238],\n",
       "       [0.51336537],\n",
       "       [0.51032028],\n",
       "       [0.51328029],\n",
       "       [0.51165753],\n",
       "       [0.51306548],\n",
       "       [0.51310816],\n",
       "       [0.49867497],\n",
       "       [0.50311914],\n",
       "       [0.50320306],\n",
       "       [0.51360027],\n",
       "       [0.51362919],\n",
       "       [0.50497148],\n",
       "       [0.50249982],\n",
       "       [0.50452149],\n",
       "       [0.50232387],\n",
       "       [0.50510099],\n",
       "       [0.50388281],\n",
       "       [0.50688109],\n",
       "       [0.50224365],\n",
       "       [0.50483176],\n",
       "       [0.51008665],\n",
       "       [0.49858915],\n",
       "       [0.50331138],\n",
       "       [0.50469943],\n",
       "       [0.50277239],\n",
       "       [0.50036068],\n",
       "       [0.509998  ],\n",
       "       [0.50120757],\n",
       "       [0.50524481],\n",
       "       [0.5070158 ],\n",
       "       [0.50875306],\n",
       "       [0.51282345],\n",
       "       [0.50328123],\n",
       "       [0.49998756],\n",
       "       [0.50367698],\n",
       "       [0.50341831],\n",
       "       [0.50931356],\n",
       "       [0.50689267],\n",
       "       [0.5065797 ],\n",
       "       [0.50926889],\n",
       "       [0.50841331],\n",
       "       [0.49897427],\n",
       "       [0.51256649],\n",
       "       [0.50706283],\n",
       "       [0.50659713],\n",
       "       [0.50585836],\n",
       "       [0.50552408],\n",
       "       [0.50933107],\n",
       "       [0.50408333],\n",
       "       [0.50190313],\n",
       "       [0.50997708],\n",
       "       [0.50059926],\n",
       "       [0.49898914],\n",
       "       [0.50982614],\n",
       "       [0.50242664],\n",
       "       [0.50854098],\n",
       "       [0.49966825],\n",
       "       [0.50691958],\n",
       "       [0.50623891],\n",
       "       [0.49815033],\n",
       "       [0.50202079],\n",
       "       [0.49906649],\n",
       "       [0.50203187],\n",
       "       [0.51355261],\n",
       "       [0.50197222],\n",
       "       [0.50975723],\n",
       "       [0.5029779 ],\n",
       "       [0.50675195],\n",
       "       [0.50038553],\n",
       "       [0.51333646],\n",
       "       [0.50993408],\n",
       "       [0.51138273],\n",
       "       [0.49821471],\n",
       "       [0.50056331],\n",
       "       [0.50909357],\n",
       "       [0.50850878],\n",
       "       [0.5024832 ],\n",
       "       [0.50599035],\n",
       "       [0.51094226],\n",
       "       [0.50194991],\n",
       "       [0.49815474],\n",
       "       [0.50687569],\n",
       "       [0.5133123 ],\n",
       "       [0.49823556],\n",
       "       [0.51018446],\n",
       "       [0.50201192],\n",
       "       [0.50203321],\n",
       "       [0.50861416],\n",
       "       [0.50247727],\n",
       "       [0.5041568 ],\n",
       "       [0.504051  ],\n",
       "       [0.50607023],\n",
       "       [0.50239235],\n",
       "       [0.50605849],\n",
       "       [0.50627748],\n",
       "       [0.5028887 ],\n",
       "       [0.50919907],\n",
       "       [0.50476829],\n",
       "       [0.50982456],\n",
       "       [0.50563995],\n",
       "       [0.50237348],\n",
       "       [0.5074301 ],\n",
       "       [0.49869694],\n",
       "       [0.50759409],\n",
       "       [0.50968759],\n",
       "       [0.50965303],\n",
       "       [0.50227502],\n",
       "       [0.5088346 ],\n",
       "       [0.49853563],\n",
       "       [0.50988177],\n",
       "       [0.50240935],\n",
       "       [0.50378456],\n",
       "       [0.50391429],\n",
       "       [0.50810623],\n",
       "       [0.5108307 ],\n",
       "       [0.51148556],\n",
       "       [0.5098766 ],\n",
       "       [0.50363899],\n",
       "       [0.50949132],\n",
       "       [0.51267968],\n",
       "       [0.50209509],\n",
       "       [0.49860462],\n",
       "       [0.50364426],\n",
       "       [0.50279931],\n",
       "       [0.50454273],\n",
       "       [0.50234598],\n",
       "       [0.50954801],\n",
       "       [0.50515086],\n",
       "       [0.50226773],\n",
       "       [0.50930995],\n",
       "       [0.50097278],\n",
       "       [0.50251377],\n",
       "       [0.51312801],\n",
       "       [0.49894426],\n",
       "       [0.50407015],\n",
       "       [0.50902586],\n",
       "       [0.5028925 ],\n",
       "       [0.50928615],\n",
       "       [0.50733583],\n",
       "       [0.50573651],\n",
       "       [0.51346895],\n",
       "       [0.51013251],\n",
       "       [0.49819961],\n",
       "       [0.50002097],\n",
       "       [0.50274679],\n",
       "       [0.50953637],\n",
       "       [0.50954262],\n",
       "       [0.50281296],\n",
       "       [0.51326928],\n",
       "       [0.50219238],\n",
       "       [0.50231035],\n",
       "       [0.50958611],\n",
       "       [0.49853941],\n",
       "       [0.50895291],\n",
       "       [0.51363611],\n",
       "       [0.50136148],\n",
       "       [0.51002749],\n",
       "       [0.50783057],\n",
       "       [0.50223771],\n",
       "       [0.50162738],\n",
       "       [0.50997716],\n",
       "       [0.50640662],\n",
       "       [0.50365164],\n",
       "       [0.51281387],\n",
       "       [0.51295665],\n",
       "       [0.50261484],\n",
       "       [0.50339908],\n",
       "       [0.49987347],\n",
       "       [0.50288199],\n",
       "       [0.5049152 ],\n",
       "       [0.50642667],\n",
       "       [0.50596734],\n",
       "       [0.50577418],\n",
       "       [0.51304994],\n",
       "       [0.50134896],\n",
       "       [0.50966308],\n",
       "       [0.50759555],\n",
       "       [0.50986022],\n",
       "       [0.50212223],\n",
       "       [0.50216706],\n",
       "       [0.50989542],\n",
       "       [0.51002996],\n",
       "       [0.50550259],\n",
       "       [0.49806656],\n",
       "       [0.51005549],\n",
       "       [0.5092335 ],\n",
       "       [0.50317649],\n",
       "       [0.4990941 ],\n",
       "       [0.50191476],\n",
       "       [0.50333924],\n",
       "       [0.49976275],\n",
       "       [0.51360567],\n",
       "       [0.50616956],\n",
       "       [0.50927017],\n",
       "       [0.50672283],\n",
       "       [0.50130257],\n",
       "       [0.50914757],\n",
       "       [0.50461228],\n",
       "       [0.50712432],\n",
       "       [0.50718861],\n",
       "       [0.50637223],\n",
       "       [0.50587897],\n",
       "       [0.499927  ],\n",
       "       [0.50605327],\n",
       "       [0.50572869],\n",
       "       [0.50525967],\n",
       "       [0.50412395],\n",
       "       [0.50739275],\n",
       "       [0.50258034],\n",
       "       [0.51348764],\n",
       "       [0.49795507],\n",
       "       [0.51143675],\n",
       "       [0.4984766 ],\n",
       "       [0.49981228],\n",
       "       [0.50946737],\n",
       "       [0.50324053],\n",
       "       [0.50978072],\n",
       "       [0.5135686 ],\n",
       "       [0.50616895],\n",
       "       [0.50895234],\n",
       "       [0.50990614],\n",
       "       [0.5051261 ],\n",
       "       [0.50531196],\n",
       "       [0.50183073],\n",
       "       [0.50312027],\n",
       "       [0.50193576],\n",
       "       [0.50467636],\n",
       "       [0.50455992],\n",
       "       [0.51002846],\n",
       "       [0.50578704],\n",
       "       [0.49845935],\n",
       "       [0.50876655],\n",
       "       [0.49845725],\n",
       "       [0.50761372],\n",
       "       [0.5118765 ],\n",
       "       [0.49844671],\n",
       "       [0.50314201],\n",
       "       [0.50533505],\n",
       "       [0.50875187],\n",
       "       [0.50336226],\n",
       "       [0.49887079],\n",
       "       [0.50202937],\n",
       "       [0.50989484],\n",
       "       [0.50786249],\n",
       "       [0.50827312],\n",
       "       [0.50383184],\n",
       "       [0.50249203],\n",
       "       [0.50233609],\n",
       "       [0.51363028],\n",
       "       [0.504707  ],\n",
       "       [0.50723272],\n",
       "       [0.50530098],\n",
       "       [0.5096211 ],\n",
       "       [0.50695401],\n",
       "       [0.49985161],\n",
       "       [0.50560946],\n",
       "       [0.502214  ],\n",
       "       [0.50129644],\n",
       "       [0.50939059],\n",
       "       [0.50841078],\n",
       "       [0.50406385],\n",
       "       [0.51090578],\n",
       "       [0.5002104 ],\n",
       "       [0.50972262],\n",
       "       [0.49876525],\n",
       "       [0.50226612],\n",
       "       [0.50408301],\n",
       "       [0.49961718],\n",
       "       [0.50876762],\n",
       "       [0.50454775],\n",
       "       [0.50715577],\n",
       "       [0.50331647],\n",
       "       [0.50752261],\n",
       "       [0.51281446],\n",
       "       [0.51369308],\n",
       "       [0.50430648],\n",
       "       [0.5089796 ],\n",
       "       [0.51281878],\n",
       "       [0.50213046],\n",
       "       [0.50214017],\n",
       "       [0.50501395],\n",
       "       [0.50215061],\n",
       "       [0.50202734],\n",
       "       [0.50344979],\n",
       "       [0.51265211],\n",
       "       [0.50263223],\n",
       "       [0.50966475],\n",
       "       [0.50811122],\n",
       "       [0.50760876],\n",
       "       [0.51057455],\n",
       "       [0.50982898],\n",
       "       [0.510051  ],\n",
       "       [0.50405446],\n",
       "       [0.50208897],\n",
       "       [0.50920252],\n",
       "       [0.5116623 ],\n",
       "       [0.51354558],\n",
       "       [0.50913318],\n",
       "       [0.50765686],\n",
       "       [0.51360139],\n",
       "       [0.50790277],\n",
       "       [0.50310189],\n",
       "       [0.49958459],\n",
       "       [0.50250628],\n",
       "       [0.50236473],\n",
       "       [0.50322194],\n",
       "       [0.50210275],\n",
       "       [0.50489094],\n",
       "       [0.50937991],\n",
       "       [0.50251235],\n",
       "       [0.49963208],\n",
       "       [0.50400889],\n",
       "       [0.50041707],\n",
       "       [0.50202711],\n",
       "       [0.50211207],\n",
       "       [0.5042362 ],\n",
       "       [0.50739739],\n",
       "       [0.51211926],\n",
       "       [0.502645  ],\n",
       "       [0.50289464],\n",
       "       [0.50712501],\n",
       "       [0.50993547],\n",
       "       [0.50941676],\n",
       "       [0.5036008 ],\n",
       "       [0.50816132],\n",
       "       [0.50857661],\n",
       "       [0.50491617],\n",
       "       [0.51365812],\n",
       "       [0.51136965],\n",
       "       [0.49880589],\n",
       "       [0.5093993 ],\n",
       "       [0.51288964],\n",
       "       [0.49814199],\n",
       "       [0.50523556],\n",
       "       [0.51234212],\n",
       "       [0.50916708],\n",
       "       [0.51069849],\n",
       "       [0.51206009],\n",
       "       [0.50750467],\n",
       "       [0.50767068],\n",
       "       [0.50212764],\n",
       "       [0.51357422],\n",
       "       [0.50229835],\n",
       "       [0.50687714],\n",
       "       [0.50902751],\n",
       "       [0.50937799],\n",
       "       [0.5026381 ],\n",
       "       [0.49815647],\n",
       "       [0.50244709],\n",
       "       [0.50032568],\n",
       "       [0.51216486],\n",
       "       [0.51246947],\n",
       "       [0.5079776 ],\n",
       "       [0.49961257],\n",
       "       [0.50708856],\n",
       "       [0.50892909],\n",
       "       [0.50991151],\n",
       "       [0.50351546],\n",
       "       [0.50511418],\n",
       "       [0.50998573],\n",
       "       [0.50568885],\n",
       "       [0.50985306],\n",
       "       [0.50975317],\n",
       "       [0.50798067],\n",
       "       [0.49819973],\n",
       "       [0.5001797 ],\n",
       "       [0.5123342 ],\n",
       "       [0.50274518],\n",
       "       [0.49816408],\n",
       "       [0.50787502],\n",
       "       [0.50619827],\n",
       "       [0.50606579],\n",
       "       [0.50570971],\n",
       "       [0.5117762 ],\n",
       "       [0.50558217],\n",
       "       [0.50702161],\n",
       "       [0.49819987],\n",
       "       [0.50450451],\n",
       "       [0.50961441],\n",
       "       [0.51274805],\n",
       "       [0.50040046],\n",
       "       [0.50926507],\n",
       "       [0.49841083],\n",
       "       [0.50876546],\n",
       "       [0.50542956],\n",
       "       [0.5066875 ],\n",
       "       [0.50953846],\n",
       "       [0.50706549],\n",
       "       [0.50779692],\n",
       "       [0.50987754],\n",
       "       [0.51177816],\n",
       "       [0.50295308],\n",
       "       [0.50320327],\n",
       "       [0.5097101 ],\n",
       "       [0.49950366],\n",
       "       [0.51007044],\n",
       "       [0.51210177],\n",
       "       [0.50950232],\n",
       "       [0.5022282 ],\n",
       "       [0.51178297],\n",
       "       [0.50577933],\n",
       "       [0.49871629],\n",
       "       [0.50445508],\n",
       "       [0.50150041],\n",
       "       [0.50247505],\n",
       "       [0.51003642],\n",
       "       [0.51002708],\n",
       "       [0.50362128],\n",
       "       [0.51009828],\n",
       "       [0.49813555],\n",
       "       [0.50881652],\n",
       "       [0.50861003],\n",
       "       [0.50780821],\n",
       "       [0.49810955],\n",
       "       [0.50971928],\n",
       "       [0.51369703],\n",
       "       [0.49878885],\n",
       "       [0.50162271],\n",
       "       [0.50702797],\n",
       "       [0.49961345],\n",
       "       [0.51318405],\n",
       "       [0.50518432],\n",
       "       [0.50397854],\n",
       "       [0.50219009],\n",
       "       [0.50966192],\n",
       "       [0.49819151],\n",
       "       [0.5092328 ],\n",
       "       [0.49837253],\n",
       "       [0.50359873],\n",
       "       [0.51009717],\n",
       "       [0.49860331],\n",
       "       [0.50552546],\n",
       "       [0.50474776],\n",
       "       [0.50545074],\n",
       "       [0.51206461],\n",
       "       [0.50658902],\n",
       "       [0.50626627],\n",
       "       [0.50569489],\n",
       "       [0.50019905],\n",
       "       [0.49867056],\n",
       "       [0.51058616],\n",
       "       [0.50006372],\n",
       "       [0.50411372],\n",
       "       [0.49818883],\n",
       "       [0.51278092],\n",
       "       [0.51132502],\n",
       "       [0.50721074],\n",
       "       [0.49871626],\n",
       "       [0.50326302],\n",
       "       [0.50631649],\n",
       "       [0.51322577],\n",
       "       [0.51066935],\n",
       "       [0.50887654],\n",
       "       [0.51021727],\n",
       "       [0.5049894 ],\n",
       "       [0.50360981],\n",
       "       [0.50965125],\n",
       "       [0.49837045],\n",
       "       [0.50223523],\n",
       "       [0.50208266],\n",
       "       [0.5117475 ],\n",
       "       [0.505301  ],\n",
       "       [0.50342244],\n",
       "       [0.5055217 ],\n",
       "       [0.4998431 ],\n",
       "       [0.49809815],\n",
       "       [0.50592169],\n",
       "       [0.50905813],\n",
       "       [0.49855742],\n",
       "       [0.50935228],\n",
       "       [0.50373569],\n",
       "       [0.50862949],\n",
       "       [0.50679681],\n",
       "       [0.50206949],\n",
       "       [0.50371456],\n",
       "       [0.50321747],\n",
       "       [0.5034948 ],\n",
       "       [0.50757071],\n",
       "       [0.50273032],\n",
       "       [0.49845466],\n",
       "       [0.51000832],\n",
       "       [0.50535404],\n",
       "       [0.50314767],\n",
       "       [0.50437116],\n",
       "       [0.50415126],\n",
       "       [0.50888134],\n",
       "       [0.51352083],\n",
       "       [0.50415674],\n",
       "       [0.50080018],\n",
       "       [0.50222216],\n",
       "       [0.49828155],\n",
       "       [0.50300312],\n",
       "       [0.51367937],\n",
       "       [0.50673447],\n",
       "       [0.50849556],\n",
       "       [0.50555749],\n",
       "       [0.50945532],\n",
       "       [0.51043424],\n",
       "       [0.51312882],\n",
       "       [0.51381906],\n",
       "       [0.5090326 ],\n",
       "       [0.5097838 ],\n",
       "       [0.50823975],\n",
       "       [0.50642364],\n",
       "       [0.50910438],\n",
       "       [0.50215187],\n",
       "       [0.50407994],\n",
       "       [0.49827749],\n",
       "       [0.50210971],\n",
       "       [0.50344005],\n",
       "       [0.50895832],\n",
       "       [0.49807989],\n",
       "       [0.50334163],\n",
       "       [0.50693565],\n",
       "       [0.49902127],\n",
       "       [0.50196494],\n",
       "       [0.51004861],\n",
       "       [0.50363432],\n",
       "       [0.49939829],\n",
       "       [0.50096872],\n",
       "       [0.50923427],\n",
       "       [0.50724589],\n",
       "       [0.50613934],\n",
       "       [0.50539432],\n",
       "       [0.50573019],\n",
       "       [0.50299421],\n",
       "       [0.50713366],\n",
       "       [0.50066082],\n",
       "       [0.50318787],\n",
       "       [0.50710606],\n",
       "       [0.50391986],\n",
       "       [0.50368248],\n",
       "       [0.50305299],\n",
       "       [0.50773131],\n",
       "       [0.50908822],\n",
       "       [0.50978883],\n",
       "       [0.51269372],\n",
       "       [0.5037193 ],\n",
       "       [0.50204328],\n",
       "       [0.49816909],\n",
       "       [0.50427876],\n",
       "       [0.50795072],\n",
       "       [0.49888602],\n",
       "       [0.50647604],\n",
       "       [0.50651633],\n",
       "       [0.50924402],\n",
       "       [0.50591218],\n",
       "       [0.50802134],\n",
       "       [0.49920183],\n",
       "       [0.50296144],\n",
       "       [0.50789337],\n",
       "       [0.50299253],\n",
       "       [0.50354998],\n",
       "       [0.5080568 ],\n",
       "       [0.50483042],\n",
       "       [0.50227699],\n",
       "       [0.50122202],\n",
       "       [0.51010501],\n",
       "       [0.4981426 ],\n",
       "       [0.50432425],\n",
       "       [0.50279673],\n",
       "       [0.50246279],\n",
       "       [0.50287718],\n",
       "       [0.50870107],\n",
       "       [0.50449401],\n",
       "       [0.4993559 ],\n",
       "       [0.50863908],\n",
       "       [0.51251782],\n",
       "       [0.51001804],\n",
       "       [0.49846499],\n",
       "       [0.50815748],\n",
       "       [0.50551881],\n",
       "       [0.5070665 ],\n",
       "       [0.51223956],\n",
       "       [0.50268491],\n",
       "       [0.50979006],\n",
       "       [0.5022723 ],\n",
       "       [0.50599477],\n",
       "       [0.51319754],\n",
       "       [0.5018719 ],\n",
       "       [0.50299276],\n",
       "       [0.51001222],\n",
       "       [0.50890965],\n",
       "       [0.50277926],\n",
       "       [0.50102284],\n",
       "       [0.50009549],\n",
       "       [0.50852476],\n",
       "       [0.50641306],\n",
       "       [0.51025401],\n",
       "       [0.50212919],\n",
       "       [0.5111444 ],\n",
       "       [0.5021133 ],\n",
       "       [0.50454763],\n",
       "       [0.51084551],\n",
       "       [0.49830048],\n",
       "       [0.50826211],\n",
       "       [0.50641883],\n",
       "       [0.51358098],\n",
       "       [0.50428564],\n",
       "       [0.50793274],\n",
       "       [0.5083915 ],\n",
       "       [0.5083225 ],\n",
       "       [0.49951578],\n",
       "       [0.50612997],\n",
       "       [0.49856665],\n",
       "       [0.4985094 ],\n",
       "       [0.5065726 ],\n",
       "       [0.49838366],\n",
       "       [0.49985724],\n",
       "       [0.50683244],\n",
       "       [0.50265183],\n",
       "       [0.50097088],\n",
       "       [0.49901914],\n",
       "       [0.49817817],\n",
       "       [0.50500516],\n",
       "       [0.50250406],\n",
       "       [0.49852821],\n",
       "       [0.50388032],\n",
       "       [0.49810147],\n",
       "       [0.50218313],\n",
       "       [0.50153953],\n",
       "       [0.50946488],\n",
       "       [0.49979144],\n",
       "       [0.51349295],\n",
       "       [0.50980038],\n",
       "       [0.50261871],\n",
       "       [0.50343925],\n",
       "       [0.49816938],\n",
       "       [0.50274837],\n",
       "       [0.49806805],\n",
       "       [0.50217686],\n",
       "       [0.50912793],\n",
       "       [0.50219488],\n",
       "       [0.51028998],\n",
       "       [0.50503252],\n",
       "       [0.50212979],\n",
       "       [0.50924256],\n",
       "       [0.50029071],\n",
       "       [0.51212176],\n",
       "       [0.51180009],\n",
       "       [0.5134262 ],\n",
       "       [0.50727452],\n",
       "       [0.50768697],\n",
       "       [0.51374727],\n",
       "       [0.50454219],\n",
       "       [0.5026228 ],\n",
       "       [0.50753553],\n",
       "       [0.5007355 ],\n",
       "       [0.50059395],\n",
       "       [0.50060435],\n",
       "       [0.50966918],\n",
       "       [0.49831644],\n",
       "       [0.51011103],\n",
       "       [0.51277159],\n",
       "       [0.51371118],\n",
       "       [0.50782422],\n",
       "       [0.50291351],\n",
       "       [0.50801846],\n",
       "       [0.50401836],\n",
       "       [0.50845363],\n",
       "       [0.51367471],\n",
       "       [0.50533794],\n",
       "       [0.50437863],\n",
       "       [0.51009624],\n",
       "       [0.51003915],\n",
       "       [0.50364408],\n",
       "       [0.49889863],\n",
       "       [0.50827459],\n",
       "       [0.50137927],\n",
       "       [0.50400026],\n",
       "       [0.50847287],\n",
       "       [0.50077366],\n",
       "       [0.51374618]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topology = [p, 4, 8, 1]\n",
    "\n",
    "neural_net = create_nn(topology, sigm)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2),\n",
    "           lambda Yp, Yr: (Yp - Yr))\n",
    "\n",
    "def train(neural_net, X, Y, l2_cost, lr=0.5, train=True):\n",
    "    out = [(None, X)]\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
    "        a = neural_net[l].act_f[0](z)\n",
    "        out.append((z, a))\n",
    "\n",
    "\n",
    "    if train:\n",
    "        \n",
    "        # Backward pass: propagamos el error hacia atrás con el error imputado a la neurona\n",
    "        deltas = []\n",
    "    \n",
    "        for l in reversed(range(0, len(neural_net))):\n",
    "        \n",
    "            z = out[l+1][0]\n",
    "            a = out[l+1][1]\n",
    "        \n",
    "            # Caso especial: estamos en la última capa\n",
    "            if l == len(neural_net) - 1:\n",
    "                \n",
    "                deltas.insert(0, l2_cost[1](a, Y)* neural_net[1].act_f[1](a))\n",
    "            \n",
    "            # Gradient descent\n",
    "            else:\n",
    "                \n",
    "                deltas.insert(0, deltas[0] @ _W.T * neural_net[1].act_f[1](a))\n",
    "                \n",
    "                 \n",
    "            _W = neural_net[l].W\n",
    "                \n",
    "            neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * lr\n",
    "            neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * lr\n",
    "                \n",
    "    return out[-1][1]\n",
    "train(neural_net, X, Y, l2_cost, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A probar\n",
    "\n",
    "Pues, ahora, probaremos la red neuronal, iterando muchas veces la función train, viendo si el resultado se aproxima poco a poco lo que buscamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comprobamos con una visualización\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Creamos una red neuronal con la topología diseñada\n",
    "neural_n = create_nn(topology, sigm)\n",
    "\n",
    "# Almacenamos los diferentes costes (graficamos)\n",
    "loss = []\n",
    "\n",
    "# Iteramos muchas veces\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Entrenemos la red\n",
    "    pY = train(neural_n, X, Y, l2_cost, lr=0.008)\n",
    "    \n",
    "    # Cada 25 iteraciones calculamos el coste\n",
    "    if i % 250 == 0:\n",
    "        \n",
    "        loss.append(l2_cost[0](pY, Y))\n",
    "        \n",
    "        # Resolución de la gráfica\n",
    "        res = 50\n",
    "        \n",
    "        _x0 = np.linspace(-1.5, 1.5, res)\n",
    "        _x1 = np.linspace(-1.5, 1.5, res)\n",
    "        \n",
    "        _Y = np.zeros((res, res))\n",
    "        \n",
    "        for i0, x0 in enumerate(_x0):\n",
    "            for i1, x1 in enumerate(_x1):\n",
    "                # Predicción\n",
    "                _Y[i0, i1] = train(neural_n, np.array([[x0, x1]]), Y, l2_cost, train=False)[0][0]\n",
    "        \n",
    "        plt.pcolormesh(_x0, _x1, _Y, cmap='PuOr')\n",
    "        plt.axis(\"equal\")\n",
    "        \n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y_e)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y_e)\n",
    "        \n",
    "        # Borrar el contenido y volver a dibujar (animación)\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        plt.plot(range(len(loss)), loss)\n",
    "        plt.show()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Bueno, así es la vida, ¿no? Posiblemente, hubo algún error en el traspaso de código, dado que si bien la convergencia del 0.25 se mantuvo (tal como en el video de DOT CSV), no logró salir de allí, siendo lo ideal minimizarlo lo más cercano a 0.\n",
    "¡Para otra oportunidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
