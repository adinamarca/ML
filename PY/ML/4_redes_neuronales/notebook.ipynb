{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "cf7922f0840c5393fef0e56634befbde274dd0b581610b62f6a1754d511fb74c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Redes neuronales\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado en el video de <b>DOT.CSV</b>: [Redes Neuronales](https://www.youtube.com/watch?v=W8AeOXa_FqU) y curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Redes neuronales\n",
    "\n",
    "Las redes neuronales fueron desarrolladas como simulación de las neuronas o de las redes de neuronas en el cerebro. \n",
    "\n",
    "## Neuronas\n",
    "\n",
    "<img src=\"2.png\"></img>\n",
    "\n",
    "Las neuronas - <i>como la que está adjunta en la imagen</i> - son una perfecta analogía para entender la representación de la hipótesis de las redes neuronales.\n",
    "\n",
    "Las primeras cosas que llaman la atención es que, las neuronas, tienen un cuerpo celular morado, con ramificaciones, llamadas \"dentritas\". Estas dentritas son similares a nuestras entradas de la función, 'inputs' o X (características).\n",
    "\n",
    "Además, estos cables de entrada reciben inputs de otras ubicaciones.\n",
    "\n",
    "Finalmente, tenemos que estos cables de entrada se conectan a un cable de salida, denominado axón, cuya función es enviar señales a otras neuronas. Este 'output' vendría siendo nuestras salidas 'h(x)'.\n",
    "\n",
    "Cobra sentido, ¿no parece?\n",
    "\n",
    "## Unidad computacional\n",
    "\n",
    "En analogía a las neuronas biológicas, y en un nivel simple, una neurona es una <b>unidad computacional</b> que tiene un número de entradas. Es, a través de estos cables de entrada, que se reciben características con las cuales se realizan algunos cálculos, y luego, se envían estos resultados mediante su axón (output) a otros nodos o a otras neuronas del cerebro.\n",
    "\n",
    "## Neurona: unidad de logística\n",
    "\n",
    "En una red neuronal artificial, lo que implementamos en una computadora es un modelo muy simple respecto a lo que realizan las neuronas biológicas.\n",
    "\n",
    "Modelaremos una neurona como una unidad logística. Así que, en el caso de la representación del modelo:\n",
    "\n",
    "• El círculo amarillo representaría una función análoga del cuerpo de una neurona (los cálculos sobre los input que darán un output).\n",
    "\n",
    "• Nuestro resultado, $ h_ \\theta (x) $, vendría siendo el output gracias al axón (en analogía biológica).\n",
    "\n",
    "<img src=\"3.png\"></img>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La función de activación\n",
    "\n",
    "Por lo general, lo que representa el cálculo $ h_{\\theta}(x) $ es:\n",
    "\n",
    "- Dada las entradas $ X_{1}, X_{2} $ y $ X_{3} $ se aplica una función (que puede ser, por ejemplo, una regresión logística) $ h_{\\theta} $, resultando en $ h_{\\theta}(x) $.\n",
    "\n",
    "En este sentido, recordemos que de notebooks pasados...\n",
    "\n",
    "Nuestra función está parametrizada por nuestro vector de parámetros:\n",
    "\n",
    "$ \\theta = \\begin{pmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} $\n",
    "\n",
    "Y definida por nuestro vector de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias)\n",
    "\n",
    "Hasta ahora, en los notebooks solo se ha hablado acerca de los parámetros $ \\theta $ como $\\theta_{i}$. Sin embargo, y por lo general, en terminología de las redes neuronales, a estos parámetros se les conoce como 'pesos' del modelo:\n",
    "\n",
    "- En inglés, 'weight'.\n",
    "\n",
    "Por lo que es usual encontrar el vector de parámetros expresados en función de $ W $ en vez de $ \\theta $.\n",
    "\n",
    "## Red neuronal\n",
    "\n",
    "En el diagrama anterior definimos una neurona, que es la base de una red neuronal.\n",
    "\n",
    "Una red neuronal consiste en diferentes neuronas unidas. Específicamente, tenemos:\n",
    "\n",
    "- Capa de entrada X (donde se introducen las variables o características).\n",
    "\n",
    "    - $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Capas intermedias o ocultas (neuronas).\n",
    "\n",
    "    - $ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $, siendo el superíndice el número de capa y $ a_{0}^{2} = 1 $ (neurona de oscilación).\n",
    "\n",
    "- Capa de salida Y (resultado final de la hipótesis).\n",
    "\n",
    "    - $ h_{\\Theta} $ que es el valor o valores que calcula la hipótesis.\n",
    "\n",
    "Más adelante, veremos redes neuronales con más de una capa oculta, pero básicamente cualquier capa que no sea de entrada o salida será una capa oculta.\n",
    "\n",
    "## Notación 1\n",
    "\n",
    "Sí, lo sé. La notación es poco entendible al principio, pero créeme, te acostumbras, por lo que la odias al principio y luego la amas por su simplicidad.\n",
    "\n",
    "Sigamos.\n",
    "\n",
    "Arriba, si eres atento a los detalles, cambiamos $ \\theta $ por $ \\Theta $, y también, por supuesto, introducimos neuronas denotadas como $ a $, ¿por qué?\n",
    "\n",
    "Para explicar los cálculos específicos representados por una red neuronal, introducimos un poco más de notación:\n",
    "\n",
    "- Usaremos $ \\Theta^{(j)} $ para referirnos a los pesos de las neuronas de una determinada capa, siendo j la capa.\n",
    "\n",
    "- Además, las unidades de activación (neurona) serán definidas como $ a_{i}^{(j)} $ de la capa j y unidad i.\n",
    "\n",
    "- En sí, el nombre \"neurona\" no deja de ser un nombre extraordinariamente interesante para referirnos a una \"función\".\n",
    "\n",
    "## Notación 2\n",
    "\n",
    "Practiquemos un poco la notación. Sea:\n",
    "\n",
    "$ a = \\begin{pmatrix} a_{0}^{2} \\\\ a_{1}^{2} \\\\ a_{2}^{2} \\\\ a_{3}^{2} \\end{pmatrix} $\n",
    "\n",
    "¿Qué nos dice la neurona $ a_{1}^{2} $?\n",
    "\n",
    "- Pues, implica que, es la primera unidad de la capa dos, realizando la activación de dicha unidad. Por cierto, con \"activación\" nos referimos al valor calculado por la neurona, es decir, su resultado. \n",
    "\n",
    "¿Qué nos dice $ \\Theta^{(j)} $?\n",
    "\n",
    "Como habíamos definido, $ \\Theta^{(j)} $ se encarga de parametrizar la red neuronal. En sí, es una matriz de ondas, la que controla el mapeo de la función de las capas.\n",
    "\n",
    "## El mapeo de la función de las capas\n",
    "\n",
    "¿Qué es el mapeo de la función de capas? \n",
    "\n",
    "Recordemos lo que hicimos en la regresión lineal, o en la regresión logística. ¿No es que intentamos ajustar una función a los datos? Pues sí, y esa es la base del \"mapeo\".\n",
    "\n",
    "Ésto lo define Hans Lehnert Merino, en [“Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales\"](https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y).\n",
    "\n",
    "- \"Lo que busca un método de machine learning es encontrar un mapeo entre\n",
    "una entrada X y una salida Y. Esto es precisamente lo que realiza una red neuronal: buscar una aproximación de una función mediante un modelo parametrizado\"\n",
    "\n",
    "Lehnert Merino, H. L. M. (2019, abril). Mecanismos Bio-Inspirados Aplicados a Tareas de Navegación en Agentes Artificiales. Universidad Técnica Federico Santa María. Recuperado de https://repositorio.usm.cl/bitstream/handle/11673/46319/3560900260874UTFSM.pdf?sequence=1&isAllowed=y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Los cálculos\n",
    "\n",
    "Así que, teniendo definido los puntos anteriores, presentaremos los cálculos:\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Definiremos las matrices de mapeo para aclarar mejor la fórmula (recordamos que debemos trasponerlas para realizar los cálculos). Entonces, definimos de la segunda capa de la red neuronal, pero, con la primera capa de neuronas.\n",
    "\n",
    "La matriz de mapeo acotada (1) $ \\Theta^{(1)} = \\begin{pmatrix} \\Theta_{1}^{(1)} & \\Theta_{2}^{(1)} & \\Theta_{3}^{(1)} \\end{pmatrix} $, en donde:\n",
    "\n",
    "- $ \\Theta_{1}^{(1)} = \\begin{pmatrix} \\Theta_{10}^{(1)} \\\\ \\Theta_{11}^{(1)} \\\\ \\Theta_{12}^{(1)} \\\\ \\Theta_{13}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{2}^{(1)} = \\begin{pmatrix} \\Theta_{20}^{(1)} \\\\ \\Theta_{21}^{(1)} \\\\ \\Theta_{22}^{(1)} \\\\ \\Theta_{23}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "- $ \\Theta_{3}^{(1)} = \\begin{pmatrix} \\Theta_{30}^{(1)} \\\\ \\Theta_{31}^{(1)} \\\\ \\Theta_{32}^{(1)} \\\\ \\Theta_{33}^{(1)} \\end{pmatrix} $\n",
    "\n",
    "De forma que la matriz de mapeo completa, que define la segunda capa, se construye a partir de reemplazar las matrices anteriores en la matriz acotada expuesta en (1). Esta matriz quedará de 3 x 4 dimensiones en este caso particular.\n",
    "\n",
    "Definimos de la tercera capa en la red neuronal, pero, con la segunda capa de neuronas:\n",
    "\n",
    "$ \\Theta_{1}^{(2)} = \\begin{pmatrix} \\Theta_{10}^{(2)} \\\\ \\Theta_{11}^{(2)} \\\\ \\Theta_{12}^{(2)} \\\\ \\Theta_{13}^{(2)} \\end{pmatrix} $\n",
    "\n",
    "Y la matriz de características:\n",
    "\n",
    "$ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $\n",
    "\n",
    "Siendo definidas nuestras neuronas en la capa oculta como:\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "$ a_{2}^{(2)} = g(\\Theta_{20}^{2}X_{0}+\\Theta_{21}^{2}X_{1}+\\Theta_{22}^{2}X_{2} + \\Theta_{23}^{2}X_{3}) $\n",
    "\n",
    "$ a_{3}^{(2)} = g(\\Theta_{30}^{3}X_{0}+\\Theta_{31}^{3}X_{1}+\\Theta_{32}^{3}X_{2} + \\Theta_{33}^{3}X_{3}) $\n",
    "\n",
    "Y una vez realizados su cálculos, se calcula la hipótesis en la tercera capa de la red neuronal, pero con la segunda capa de neuronas:\n",
    "\n",
    "$ h_{\\Theta}(X) = a_{1}^{(3)} = g(\\Theta_{10}^{2}X_{0}+\\Theta_{11}^{2}X_{1}+\\Theta_{12}^{2}X_{2} + \\Theta_{13}^{2}X_{3}) $\n",
    "\n",
    "Bastante jaleo, ¿no es cierto? Evidentemente, el procedimiento de transponer se realiza en base a la necesidad de operar las matrices (contenidos que se exploran en álgebra matricial).\n",
    "\n",
    "## ¿Cómo entendemos una neurona particular?\n",
    "\n",
    "Por ejemplo, la neurona $a_{1}^{2}$ es igual a la función sigmoide o a la función de activación sigmoidal, aplicada a la siguiente combinación lineal de sus entradas.\n",
    "\n",
    "$ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $\n",
    "\n",
    "Y así, sucesivamente, para cada neurona.\n",
    "\n",
    "## La dimensionalidad\n",
    "\n",
    "Si una red neuronal tiene:\n",
    "\n",
    "- Un número de $ s_{j} $ unidades en la capa j.\n",
    "\n",
    "- Un número de $ s_{j+1} $ unidades en la capa $ j + 1 $.\n",
    "\n",
    "Entonces, $\\Theta^{(j)}$ tendrá una dimensión de $ s_{j+1} \\times (s_{j} + 1) $.\n",
    "\n",
    "## Las conexiones\n",
    "\n",
    "En sí, todos los cálculos en una red neuronal se encuentran intrínsicamente relacionados unos con otros. \n",
    "\n",
    "Los cálculos de la segunda capa se explican a través de la primera capa:\n",
    "\n",
    "- La primera capa corresponden a las entradas (input), X.\n",
    "\n",
    "- La segunda capa realiza el cálculo sobre las variables $ X $ de la primera capa, parametrizando con $ \\Theta_{i}^{(1)} $ y resultando en $ a_{i}^{(2)} $, aplicando la combinación lineal de sus entradas y la función sigmoide.\n",
    "\n",
    "- La tercera capa realiza el cálculo sobre las variables $ a_{i}^{(2)} $ originadas en la capa anterior, parametrizando con $ \\Theta_{i}^{(2)}$.\n",
    "\n",
    "Así que, tras hablar de lo que hacen las tres unidades ocultas para calcular sus valores, hablaremos del último espinal: la última unidad.\n",
    "\n",
    "## La salida\n",
    "\n",
    "La última unidad, calcula $h_{\\theta}(X)$ que es Y, de forma que también podemos escribirlo como:\n",
    "\n",
    "- La salida es igual a $h_{\\theta}(X)=Y=a_{1}^{3}$, que es la función sigmoide evaluada en la combinación de parámetros.\n",
    "\n",
    "De esta forma, la función $ h_{\\theta}(X)$ mapea los valores de entrada $ X $, que, en algunos espacios o disposiciones $ Y $, definirá, con cierta precisión, un conjunto de hipótesis, llegando a diferentes funciones de mapeo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La búsqueda de la eficiencia\n",
    "\n",
    "En las últimas seldas, vimos la definición matemática sobre cómo representar y calcular las hipótesis utilizadas por las redes neuronales. \n",
    "\n",
    "A continuación, veremos <b>cómo realizar este cálculo de forma eficiente</b>, con la implementación vectorizada.\n",
    "\n",
    "En segundo lugar, Andrew NG. desea empezar a darnos una intuición sobre por qué las representaciones de las redes neuronales son una buena idea para ayudarnos con hipótesis complejas no lineales. Es decir, similar a lo que vimos en regresión logística en situaciones donde la hipótesis podía asemejarse a una circunferencia, o a curvas cualesquiera.\n",
    "\n",
    "## La implementación vectorizada\n",
    "\n",
    "<img src=\"5.png\"></img>\n",
    "\n",
    "Consideremos la red neuronal de arriba. \n",
    "\n",
    "Anteriormente, habíamos dicho que la secuencia de pasos que necesitamos para calcular la salida de una hipótesis en estas ecuaciones:\n",
    "\n",
    "- Calcular los valores de activación de las unidades ocultas con los valores de entrada.\n",
    "\n",
    "- Utilizando las salidas anteriores, calculamos para la última unidad de la espinal, obteniendo el resultado de la red neuronal $h_{\\theta}(X)$.\n",
    "\n",
    "Entonces, ahora es cuando definiremos la implementación vectorizada, debiendo introducir términos adicionales.\n",
    "\n",
    "Los términos subrayados los redefiniremos, por lo que, sea la combinación lineal ponderada de la funciones de activación:\n",
    "\n",
    "- Recordemos de [notebooks anteriores](https://github.com/adinamarca/notebooks/blob/main/PY/ML/3_regresion_logistica/notebook.ipynb). \n",
    "\n",
    "En dicho notebook, definimos $ h(\\Theta^{T}X) $, donde $ Z = \\Theta^{T}X $. \n",
    "\n",
    "Utilizaremos una definición similar para las hipótesis de las redes neuronales, definiendo por cada combinación lineal de cada neurona como $ Z_{i}^{j} $:\n",
    "\n",
    "- Donde $ i $ indica el número de la neurona.\n",
    "\n",
    "- Donde $ j $ corresponde al número de capa.\n",
    "\n",
    "De esta forma, redefinimos:\n",
    "\n",
    "- Sea $ a_{1}^{(2)} = g(\\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3}) $, redefinimos $ Z_{1}^{2} = \\Theta_{10}^{1}X_{0}+\\Theta_{11}^{1}X_{1}+\\Theta_{12}^{1}X_{2} + \\Theta_{13}^{1}X_{3} $, de forma que quedamos con $ a_{1}^{(2)} = g(Z_{1}^{2}) $.\n",
    "\n",
    "- Repitiendo el paso anterior, quedamos con $ a_{2}^{(2)} = g(Z_{2}^{2}) $.\n",
    "\n",
    "- Y $ a_{3}^{(2)} = g(Z_{3}^{2}) $.\n",
    "\n",
    "Observamos que, estas nuevas definiciones, corresponden de forma sospechosamente similar a la operación $ \\Theta^{(i)}*X $ (en forma análoga a $\\Theta^{T}X$, pero estando $ \\Theta $ ya transpuesta).\n",
    "\n",
    "Mediante esta observación, seremos capaces de vectorizar este cálculo de la red neuronal:\n",
    "\n",
    "- Recordamos nuestro vector de características como $ X = \\begin{pmatrix} X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $, donde $ X_{0} = 1 $ (denominada unidad de bias).\n",
    "\n",
    "- Definimos nuestro vector de combinaciones lineales (que definen, cada uno, una función de mapeo a través de los parámetros $ \\Theta $) como $ Z^{(2)} = \\begin{pmatrix} Z_{1}^{2} \\\\ Z_{2}^{2} \\\\ Z_{3}^{2} \\end{pmatrix} $.\n",
    "\n",
    "¡Necesitamos un valor más! Que es la unidad de sesgo (bias unit). Para solucionar este problema, lo que haremos será sumar $ a_{0}^{2} $:\n",
    "\n",
    "- La unidad de oscilación la definimos como $ a_{0}^{2} = 1$.\n",
    "\n",
    "Ahora tendremos que $ a^{(2)} $ será un vector de cuatro dimensiones.\n",
    "\n",
    "Finalmente, para calcular el valor real de la salida de las hipótesis:\n",
    "\n",
    "- Queda $h_{\\Theta}(X)=a_{1}^{(3)}=g(\\Theta_{10}^{(2)}a_{0}^{(2)}+\\Theta_{11}^{(2)}a_{1}^{(2)}+\\Theta_{12}^{(2)}a_{2}^{(2)}+\\Theta_{13}^{(2)}a_{3}^{(2)})$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Funciones no lineales complejas\n",
    "\n",
    "En esta y la próxima celda trabajaremos con un ejemplo detallado que muestra cómo una red neuronal puede calcular una función no lineal compleja de entrada.\n",
    "\n",
    "<img src=\"6.png\"></img>\n",
    "\n",
    "Consideremos el problema de la imagen anterior, donde tenemos $ X_{1} $ y $ X_{2} $ como variables de entrada, lo que nos define que tenemos solo dos valores posibles: valores binarios.\n",
    "\n",
    "<img src=\"7.png\"></img>\n",
    "\n",
    "Podemos considerar la imagen anterior como una versión simplificada de un problema más complejo. En este sentido, se debería aprender un límite de decisión no lineal, de forma que debamos separar los ejemplos positivos y negativos.\n",
    "\n",
    "## Puertas lógicas\n",
    "\n",
    "<img src=\"8.png\"></img>\n",
    "\n",
    "~ Imagen por [Alejandro García](http://aletecnocampello.blogspot.com/2015/12/puertas-logicas.html\n",
    ").\n",
    "\n",
    "Antes de continuar, es recomendable leer acerca de las puertas lógicas (y no, no es para nada necesario aprenderse la nomenclatura).\n",
    "\n",
    "Una puerta lógica, o compuerta lógica, es un dispositivo electrónico con una función booleana u otras funciones como sumar o restar, incluyen o excluyen según sus propiedades lógicas (extraído desde [Wikipedia](https://es.wikipedia.org/wiki/Puerta_l%C3%B3gica)).\n",
    "\n",
    "- [Puerta lógica XOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XOR). Implica una salida verdadera (1 o TRUE) si una, y solo una de las entradas a la puerta es verdadera.\n",
    "\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XOR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 0\n",
    "\n",
    "- [Puerta lógica XNOR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_XNOR). Es la inversa de la puerta lógica XOR.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;XNOR \\; X_{2} $ |\n",
    "| 0 | 0 | 1\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica OR en Wikipedia](https://es.wikipedia.org/wiki/Puerta_OR). Es verdadero si al menos una entrada es verdadera.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;OR \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 1\n",
    "| 1 | 0 | 1\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica AND en Wikipedia](https://es.wikipedia.org/wiki/Puerta_AND). Es verdadero si y solo si las dos entradas son verdaderas.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ X_{1} \\;AND \\; X_{2} $ |\n",
    "| 0 | 0 | 0\n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1\n",
    "\n",
    "- [Puerta lógica NOT en Wikipedia](https://es.wikipedia.org/wiki/Puerta_NOT). Es verdadero si la entrada es falsa.\n",
    "\n",
    "| INPUT | OUTPUT |\n",
    "| :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $\\;NOT \\; X_{1} $ |\n",
    "| 0 | 1\n",
    "| 1 | 0\n",
    "\n",
    "## Redes neuronales en funciones no lineales complejas\n",
    "\n",
    "Entonces, ¿cómo podemos hacer que una red neuronal se ajuste a este problema?\n",
    "\n",
    "Específicamente, lo que está calculando Y es equivalente a: $ Y = X_{1} \\;XNOR \\; X_{2} $. Es decir, tendremos ejemplos positivos siempre que ambas entradas sean positivas o ambas sean falsas.\n",
    "\n",
    "<img src=\"9.png\"></img>\n",
    "\n",
    "En este sentido, queremos saber si podemos hacer que una red neuronal sea capaz de ajustarse a este tipo de conjuntos de aprendizaje.\n",
    "\n",
    "## ¿Qué es la hipótesis no lineal compleja? \n",
    "\n",
    "Cuando nos referimos con hipótesis no lineales complejas, podemos utilizar el mismo ejemplo de la puerta lógica XNOR. En dicho ejemplo sería imposible trazar una recta que realizase el límite de decisión correctamente a ambos conjuntos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ajuste a puerta lógica AND\n",
    "\n",
    "Con el fin de construir una red que se ajuste al ejemplo XNOR, vamos a comenzar con un ejemplo ligeramente más simple: con una puerta lógica AND.\n",
    "\n",
    "<img src=\"10.png\"></img>\n",
    "\n",
    "Concretamente, digamos que tenemos las entradas $ X_{1} $ y $ X_{2} $ que, nuevamente, son binarias {$1 \\; , \\; 0$}.\n",
    "\n",
    "Entonces, ¿podremos hacer que una sola red neuronal se ajuste a este problema? En orden para ello, ¡consideraremos las unidades de oscilación!\n",
    "\n",
    "<img src=\"11.png\"></img>\n",
    "\n",
    "Ahora, asignaremos algunos valores a los pesos de los parámetros de la red.\n",
    "\n",
    "<img src=\"12.png\"></img>\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-30, 20, 20] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-30 + 20X_{1} + 20X_{2}) $.\n",
    "\n",
    "A veces, es conveniente dibujar los pesos de los parámetros en el diagrama de la red neuronal. \n",
    "\n",
    "Solo para recordarnos, la función de activación sigmoidea $ g(Z) $ comienza en $0$, se eleva lentamente, pasando por $ 0.5 $ y asintotando en $ 1 $.\n",
    "\n",
    "En este sentido:\n",
    "\n",
    "- Si $ Z = 4.6 $, entonces la función sigmoidea será de 0.99, y por otro lado, si evaluamos en $ -4.6 $ la función sigmoidea será de 0.01, lo cual es muy cercano a 0.\n",
    "\n",
    "<img src=\"13.png\"></img>\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -30 | 0\n",
    "| 0 | 1 | -10 | 0\n",
    "| 1 | 0 | -10 | 0\n",
    "| 1 | 1 | 10 | 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## La importancia de los pesos\n",
    "\n",
    "Notar como, con los pesos de los parámetros, fuimos capaces de ajustar a este problema concreto, una sola neurona, dando el resultado esperado en la <b>tabla de verdad</b>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ajuste a puerta lógica OR\n",
    "\n",
    "Esta vez, construyamos una neurona para la puerta lógica OR.\n",
    "\n",
    "De esta forma, definimos la función de activación como:\n",
    "\n",
    "- Sean nuestros parámetros: $ [\\Theta_{10}^{(1)}, \\Theta_{11}^{(1)}, \\Theta_{12}^{(1)}] = [-20, 21, 21] $.\n",
    "\n",
    "- Para este problema concreto $ h_{\\Theta}(X)=g(-20 + 21X_{1} + 21X_{2}) $.\n",
    "\n",
    "Veamos entonces los <b>cuatro posibles valores de entrada </b> para $ X_{1} \\; y \\; X_{2} $:\n",
    "\n",
    "| INPUT | INPUT | EVALUACIÓN | OUTPUT |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $ X_{1} $ | $ X_{2} $ | $ Z $ | $ h_{\\Theta}(X) $ |\n",
    "| 0 | 0 | -20 | 0\n",
    "| 0 | 1 | 1 | 1\n",
    "| 1 | 0 | 1 | 1\n",
    "| 1 | 1 | 22 | 1\n",
    "\n",
    "## Las neuronas individuales\n",
    "\n",
    "De esta forma, concluimos cómo las neuronas individuales pueden utilizarse para funciones lógicas como AND o OR, entre otras.\n",
    "\n",
    "En la siguiente celda abordaremos ejemplos más complejos, llegando a trabajar con más de una neurona, formando una red neuronal capaz de usarse para calcular funciones más complejas como XOR o XNOR."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Un paso más allá"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}