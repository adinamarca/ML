{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "510dd6fa91319fddc5d79621a07358f04a8fa48e6ad602d55eaa17fc7a04de2d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Regresión logística\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado, gran parte, en el curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, Statdeveloper, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Aviso\n",
    "\n",
    "Antes de comenzar, y para cualquiera que esté leyendo (<i>por cierto, me sorprendería que a alguien le sea útil</i>), este notebook será más largo de lo habitual por dos razones:\n",
    "\n",
    "• Se explicarán los problemas de clasificación.\n",
    "\n",
    "• Tengo muchísimo material de regresión logística.\n",
    "\n",
    "Bueno, antes de iniciar de lleno con redes neuronales (en el próximo notebook), debemos aprender y comprender lo que son los problemas de clasificación, dado que con ellos, seremos capaces de categorizar y utilizar clases, trabajando con datos discretos.\n",
    "\n",
    "## (1) Problemas de clasificación\n",
    "\n",
    "Los problemas de clasificación implican predecir una variable que tiene valores discretos. Esto implica que nuestro output tendrá un carácter booleano:\n",
    "\n",
    "• $(Y = 0)$ OR $(Y = 1)$.\n",
    "\n",
    "• En este sentido, nuestra hipótesis, $ 0 \\leq h_ \t\\theta (X) \\leq 1 $.\n",
    "\n",
    "Empezaremos con problemas de dos clases o de <b>clasificación binaria</b>. Entonces, ¿cómo se desarrolla un algoritmo de clasificación? A continuación, hallaremos un conjunto de entrenamiento para una tarea de clasificación sobre si un tumor es beligno o maligno. Observemos que si es benigno o no, adquiere únicamente dos valores, 0 de no y 1 de sí.\n",
    "\n",
    "<img src=\"3.png\"></img>\n",
    "\n",
    "Anteriormente, habíamos mencionado que el clasificador $ 0 \\leq h_ \\theta (X) \\leq 1 $, por lo que nos gustaría una hipótesis que satisfaga esta propiedad, es decir, que estas predicciones estén entre cero y uno.\n",
    "\n",
    "Cuando usábamos la regresión lineal, conocíamos nuestra hipótesis como $ h_ \\theta (X) $. Además, si nos ayudamos del álgebra lineal, podemos apreciar que, podemos expresar nuestra fórmula de la siguiente manera:\n",
    "\n",
    "• Consideramos $ \\theta ^{T} $ nuestra matriz de parámetros $ \\theta $s.\n",
    "\n",
    "• Además, $ X $ estaría compuesta por nuestras características.\n",
    "\n",
    "De esta forma, podemos expresar nuestra regresión lineal de forma acotada:\n",
    "\n",
    "$ h_ \\theta (X) = \\theta ^{T}X $\n",
    "\n",
    "En caso de no haber comprendido del todo el procedimiento, recomiendo leer [el siguiente artículo de Andrew NG sobre aprendizaje supervisado (aprox. pág. 3)](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf).\n",
    "\n",
    "## Regresión logística\n",
    "\n",
    "La regresión logística pretende estimar la probabilidad de un determinado evento, medible por variables categóricas y numéricas, <b>que se sabe está correlacionado con ciertas variables cuantitativas</b>. \n",
    "\n",
    "Puede ser, que hasta el momento, se esté algo perdido, dado que introducimos dos conceptos que no están del todo explicados.\n",
    "\n",
    "Recordemos:\n",
    "\n",
    "• $(Y = 0)$ OR $(Y = 1)$.\n",
    "\n",
    "• Hipótesis, $ 0 \\leq h_ \\theta (X) \\leq 1 $.\n",
    "\n",
    "¿Por qué no utilizar la regresión lineal en vez de la regresión logística?\n",
    "\n",
    "Pues, intentemos. \n",
    "\n",
    "Dado el conjunto de entrenamiento del ejemplo anterior, de los tumores, podríamos ajustar una regresión lineal, obteniendo una hipótesis que tal vez, se parecería a la siguiente.\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Pero, ¿cómo haríamos para obtener, predicciones en valores discretos entre 0 y 1?Para ello, aplicaremos un umbral, de forma que dado los resultados del clasificador:\n",
    "\n",
    "• Nuestra hipótesis definirá $ Y = 1 $ si nuestros resultados, son $ 0.5 \\leq h_ \\theta (X) $.\n",
    "\n",
    "• De otra forma ($ 0.5 > h_ \\theta (X) $), nuestra hipótesis definirá $ Y = 0 $.\n",
    "\n",
    "Pareciera que la regresión lineal está realizando algo razonable, a pesar se trata de una tarea de clasificación. Sin embargo, cambiemos un poco el problema.\n",
    "\n",
    "Supongamos que tenemos otro conjunto de entrenamiento, pero con outliers (datos atípicos), de esta forma, la pendiente variará, resultando en un modelo con menos exactitud.\n",
    "\n",
    "Por dicha razón, <b color=\"red\">no es conveniente aplicar regresión lineal en problemas de clasificación</b>, y de hecho, es una <i>pésima práctica</i>.\n",
    "\n",
    "## Representación de la regresión logística\n",
    "\n",
    "A continuación, representaremos hipótesis, que es, la función que vamos a utilizar para los problemas de clasificación.\n",
    "\n",
    "Anteriormente, habíamos dicho que nos gustaría que el clasificador genere valores que se encuentren entre cero y uno, por lo que necesitamos una hipótesis que satisfaga esta propiedad.\n",
    "\n",
    "Antes, con al regresión lineal, habíamos definido la siguiente hipótesis:\n",
    "\n",
    "<img src=\"5.png\"></img>\n",
    "\n",
    "Donde:\n",
    "\n",
    "• $ \\theta ^{T} $ definía nuestro vector de parámetros.\n",
    "\n",
    "• $ X $ definía nuestro vector de características (variables).\n",
    "\n",
    "• $ h_ \\theta (X) $ es nuestra hipótesis.\n",
    "\n",
    "Para la regresión logística modificaremos levemente la hipótesis de la regresión lineal $ h_ \\theta (X) = \\theta ^{T}X $, de forma que $ h_ \\theta (X) \\in [0, 1] $, por lo que la definiremos de la siguiente forma:\n",
    "\n",
    "- Regresión lineal: $ h_ \\theta (X) = \\theta ^{T}X $.\n",
    "\n",
    "- Regresión logística: $ h_ \\theta (X) = g( \\theta ^{T}X) $.\n",
    "\n",
    "De esta forma, definimos la regresión logística a través de una composición de funciones (combinación en palabras simples), dado que, la función $ h_ \\theta (X) $ está definida en otra función $ g( \\theta ^{T}X) $. \n",
    "\n",
    "Dado que en el Machine Learning tiende a acotarse la nomenclatura para que sea conveniente de escribir en código (y más eficiente a través de matrices), definiremos otra abreviatura a través de la nueva introducida hipótesis:\n",
    "\n",
    "- Siendo $ h_ \\theta (X) = g( \\theta ^{T}X) $, definimos $ Z =  \\theta ^{T}X $, de forma que, $ h_ \\theta (X) = g(Z) $. ¿Más simple, no?\n",
    "\n",
    "Bien. Hasta ahora, hemos hablado de composición de funciones, nomenclatura... Pero, ¿qué función es en verdad $ g(Z) $?\n",
    "\n",
    "- Definimos la regresión logística o sigmoidea como $ h_ \\theta (X) = g(Z) $, donde $ g(Z) = \\frac{1}{1+e^{-z}} $ y $ Z = \\theta ^{T}X $, por lo que $ g( \\theta ^{T}X) = \\frac{1}{1+e^{- \\theta ^{T}X}} $.\n",
    "\n",
    "Gracias a un fragmento de Wikipedia, así se ve el gráfico de la función de la hipótesis (azul) con su derivada (línea roja).\n",
    "\n",
    "<img src=\"6.png\"></img>\n",
    "\n",
    "- La función sigmoidea tiene dos asíntotas. Mientras $ Z $ va hacia menos infinito, $ g(Z) $ se aproxima a cero y mientras $ g(Z) $ tiende a infinito $ Z $ se aproxima a uno. Es decir, $ g(Z) $ ofrece valores que están entre 0 y 1 pero también tenemos que $ h_ \\theta (X) $ debe estar entre 0 y 1.\n",
    "\n",
    "- Como antes, lo que debemos hacer es ajustar los parámetros $ \\theta $ a nuestros datos. Así que dado un conjunto de entrenamiento, necesitamos elegir un valor para los parámetros $ \\theta $ y esta hipótesis nos permitirá hacer predicciones. \n",
    "\n",
    "- Hablaremos del algoritmo de aprendizaje más adelante respecto al ajuste de los parámetros $ \\theta $, mientras tanto, consideraremos la interpretación del modelo. Es decir, cómo se interpreta el resultado de la hipótesis $ h_ \\theta (X) $.\n",
    "\n",
    "- La interpretación de la salida de la hipótesis se entiende a partir de $ h_ \\theta (X) = probabilidad $, donde la $ probabilidad $ es la prob. estimada que $ y = 1 $ en el ejemplo de entrada de $ X $.\n",
    "\n",
    "Si suponemos que, $ h_ \\theta (X) = probabilidad $, siendo  $ X = [ X_{0}, X_{1} ] = [1, houseSize] $, donde, tras introducir la variable en la hipótesis, nos arroja la <i>probabilidad de vender una casa</i> igual a $ h_ \\theta (X) = 0.7 $, entonces la hipótesis estará prediciendo que existe una probabilidad del 70% de que se venda una casa dado el tamaño de la misma.\n",
    "\n",
    "En probabilidad, ésto se entiende:\n",
    "\n",
    "<img src=\"7.png\"></img>\n",
    "\n",
    "Donde:\n",
    "\n",
    "- Entendemos $ Y \\in [0, 1] $, siendo $ Y $ la probabilidad de que se venda la casa ($ Y = 1 $), dado que \"X\" tiene un tamaño particular, siendo esta probabilidad parametrizada por $ \\theta $.\n",
    "\n",
    "- La hipótesis nos dará las estimaciones de la probabilidad de que $ Y = 1 $.\n",
    "\n",
    "Ahora, de la misma forma, podemos estimar la probabilidad de $ Y = 0 $. Y de hecho, la suma de que $ Y = 0 OR  Y = 1 $ es igual a la probabilidad de unión, que en este caso, corresponde al 100% al tratarse de un evento con solo dos posibilidades (binario). Si tuviésemos una regresión logística multiclase,deberíamos incorporar mayor número de eventos, o clasificadores, y por ende, su suma no sería el 100%, sino el n-número de clases.\n",
    "\n",
    "<img src=\"8.jpg\"></img>\n",
    "\n",
    "<i>Extraído desde</i> [Slideshare](https://es.slideshare.net/RichardHuamanDurand/estadistica-y-probabilidades-cap-vi2).\n",
    "\n",
    "De esta forma, podemos obtener las probabilidades de que $ Y = 0 $ o bien, $ Y = 1 $ con simples despejes.\n",
    "\n",
    "## Límite de decisión\n",
    "\n",
    "En el punto anterior, vimos la representación de la regresión logística, hablamos de su representación de la hipótesis y además, del cómo la regresión lineal no se ajusta a las necesidades de los problemas de clasificación.\n",
    "\n",
    "En el presente punto, hablaremos del <b>límite de decisión</b>, que nos dará una mejor idea de lo que las regresiones logísticas, y en particular, sus hipótesis, son capaces de computar.\n",
    "\n",
    "Para recapitular, recordemos lo que establecimos como hipótesis:\n",
    "\n",
    "- Siendo $ h_ \\theta (X) = g( \\theta ^{T}X) $, definimos $ Z = \\theta ^{T}X $, de forma que, $ h_ \\theta (X) = g(Z) $.\n",
    "\n",
    "- Definimos la regresión logística o sigmoidea como $ h_ \\theta (X) = g(Z) $, donde $ g(Z) = \\frac{1}{1+e^{-z}} $ y $ Z = \\theta ^{T}X $, por lo que $ g( \\theta ^{T}X) = \\frac{1}{1+e^{- \\theta ^{T}X}} $.\n",
    "\n",
    "Concretamente, esta hipótesis, está dando estimaciones de la probabilidad de que $ Y = 1 $, dado X y parametrizado por $ \\theta $, por lo que, si quisiéramos predecir si $ Y = 1 $ o $ Y = 0 $, lo que deberíamos hacer sería:\n",
    "\n",
    "- Suponer predicción de que $ Y = 1 $ solo si \n",
    "$ 0.5 \\leq h_ \\theta (X) $.\n",
    "\n",
    "- De otra forma, suponemos $ Y = 0 $ solo si $ 0.5 > h_ \\theta (X) $.\n",
    "\n",
    "Es decir, siempre que la hipótesis da como resultado la probabilidad de que $ Y = 1 $ es debido a que la probabilidad de que $ Y = 1 $ es mayor o igual al 50% según nuestra hipótesis (hay más probabilidades de que $ Y = 1 $ de que $ Y = 0 $). De lo contrario, si la probabilidad estimada de que $ Y = 1 $ es menor al 50%, entonces vamos a predecir que $ Y = 0 $.\n",
    "\n",
    "Ahora, ¿cómo vemos ésto en la gráfica? ¿Con qué valores de $ Z $ ($ \\theta ^{T}X $) obtendremos determinados valores para $ Y $?\n",
    "\n",
    "<img src=\"9.png\"></img>\n",
    "\n",
    "Notemos que, en la función sigmoide, $ g(Z) $ es mayor o igual a 0.5 siempre que $ Z $ sea mayor o igual a cero:\n",
    "\n",
    "- Es decir, siempre que $ 0 \\leq Z $, tendremos que $ 0.5 \\leq g(Z) $.\n",
    "\n",
    "Dado que, en la hipótesis, definimos $ Z = \\theta ^{T}X $, entonces este término debe ser mayor o igual a 0 en orden para que $ g(Z) $ sea mayor o igual a 0.5:\n",
    "\n",
    "- De esta forma, $ 0.5 \\leq g(Z) $ siempre que $ 0 \\leq Z $, por lo que $ 0.5 \\leq g( \\theta ^{T}X) $, por lo que $ 0 \\leq \\theta ^{T}X $.\n",
    "\n",
    "Lo anterior es un simple reemplazo asumiendo que $ Z = \\theta ^{T}X $.\n",
    "\n",
    "En resumen, y para terminar majaderamente con estas definiciones:\n",
    "\n",
    "- Suponemos $ Y = 1 $ cuando $ 0.5 \\leq g(Z) $.\n",
    "- Suponemos $ Y = 0 $ cuando $ 0.5 > g(Z) $.\n",
    "\n",
    "## Entendamos con el conjunto de entrenamiento\n",
    "\n",
    "Usemos el punto anterior para entender cómo la hipótesis de regresión logística realiza estas predicciones. Por lo que, ahora, supongamos que tenemos un conjunto de entrenamiento como el que se muestra en la imagen adjunta.\n",
    "\n",
    "<img src=\"10.png\"></img>\n",
    "\n",
    "Respecto al ajuste de los parámetros, hablaremos de ello más adelante.\n",
    "\n",
    "Suponiendo que, a través de un procedimiento especificado, terminamos con los siguientes valores para los parámetros:\n",
    "\n",
    "- Es decir, $ \\theta = [ \\theta _{0}, \\theta _{1}, \\theta _{2}] = [-3, 1, 1] $ (recordemos $ \\theta _{0} $ es el bias).\n",
    "\n",
    "Entonces, al tener esta elección de parámetros, intentemos averiguar donde una hipótesis terminará prediciendo que $ Y = 1 $ y donde predeciría que $ Y = 0 $. Pues, usando las fórmulas anteriormente mencionadas:\n",
    "\n",
    "- Sabemos que nuestra hipótesis predicirá $ Y = 1 $ solo si $ 0.5 \\leq g(Z) $.\n",
    "\n",
    "Al tener  $ \\theta = [ \\theta _{0}, \\theta _{1}, \\theta _{2}] = [-3, 1, 1] $, y realizando la múltiplicación de $ \\theta ^{T} $ por X:\n",
    "\n",
    "- Al ser $ \\theta ^{T} $ una matriz de 1 x 3 y X una matriz de 3 x 1, es posible realizar el producto punto. He allí el por qué se transpone $ \\theta $.\n",
    "\n",
    "Obtenemos que:\n",
    "\n",
    "- Nuestra hipótesis está definida por $ g(-3 + X_{1} + X_{2}) = \\frac{1}{1+e^{-(-3 + X_{1} + X_{2})}} $ y $ Z = -3 + X_{1} + X_{2} $, por lo que $ g(-3 + X_{1} + X_{2}) = \\frac{1}{1+e^{-(-3 + X_{1} + X_{2})}} $.\n",
    "\n",
    "En este caso particular, tenemos más de un parámetro.\n",
    "\n",
    "Realizando un poco de aritmética básica y despejando la inecuación:\n",
    "\n",
    "- Obtenemos $ 3 \\leq X_{1} \\leq X_{2} $.\n",
    "\n",
    "De esta forma, $ Y = 1 $ siempre que $ 3 \\leq X_{1} \\leq X_{2} $, y esta, corresponde a una recta.\n",
    "\n",
    "<img src=\"11.png\"></img>\n",
    "\n",
    "Al graficar la recta, veremos que pasa a través de 3 en el eje $ X_{1} $, y de igual manera en el eje $ X_{2} $. Así que, la parte del plano $ X_{1} $ y $ X_{2} $ que corresponde a cuando $ X_{1} + X_{2} $ es mayor o igual a 3, será toda la mitad derecha, y esta parte corresponde a $ Y = 1 $.\n",
    "\n",
    "Por contraparte, todas aquellas zonas donde no se satisfaga la inecuación, será la zona $ Y = 0 $.\n",
    "\n",
    "Es aquí cuando definimos lo relevante, la recta rosada nos permitió establecer los límites de la predicción de la hipótesis sobre Y, lo que se define como <b>límite de decisión</b>.\n",
    "\n",
    "<img src=\"12.png\"></img>\n",
    "\n",
    "Esta recta, el límite de decisión, corresponde a cuando la hipótesis es igual a 0.5 exactamente. La línea, separa la región de donde se predice los valores de Y:\n",
    "\n",
    "- Además, el <b>límite de decisión es una propiedad de la hipótesis que incluye los parámetros</b>. Por lo que, si no tuviéramos visualizados los datos o parámetros, el límite de decisión podría seguir graficándose.\n",
    "\n",
    "Recordemos que el vector de características son los coeficientes que acompañan a las variables independientes, que, una vez evaluadas, nos resultará en la variable dependiente Y, de modo que, anteriormente, en la regresión lineal, lo que se realizaba era construir una función que cada vez se acercara más al fenónemo (recorrido de los datos), de modo que, introduciendo nuestros conjuntos de entrenamiento de entrada, nos produzcan con menor error posible la salida real de dicho conjunto de entrenamiento.\n",
    "\n",
    "## Límites de decisión no lineales\n",
    "\n",
    "Veamos ahora un ejemplo más complejo, donde, como de costumbre, tenemos \"X\" para los ejemplos positivos y \"O\" para denotar los ejemplos negativos. \n",
    "\n",
    "Dado un conjunto de entrenamiento como el siguiente, ¿cómo obtenemos una hipótesis que se ajuste a este tipo de datos con regresión logística? ¿Un spoiler? Sí, una circunferencia sería útil.\n",
    "\n",
    "<img src=\"13.png\"></img>\n",
    "\n",
    "Es decir, veremos los límites de decisión no lineales. \n",
    "\n",
    "Anteriormente, cuando introducimos la regresión lineal, la introducimos de forma bastante simplificada. Sin embargo, y como hemos mencionado, los problemas del mundo real tienden a tener un gran número de variables, y es allí, cuando se utiliza la regresión lineal multivariable (con tres o más variables).\n",
    "\n",
    "En este sentido, y en la regresión logística, podemos aplicar lo mismo, de forma de tener términos polinómicos de orden superior que se ajusten al recorrido de los datos.\n",
    "\n",
    "- Un ejemplo conveniente de hipótesis sería $ h_ \\theta (X) = g( \\theta _{0} + \\theta _{1}X_{1} + \\theta _{2}X_{2} + \\theta _{3}X_{1}^{2} + \\theta _{4}X_{2}^{2})  $.\n",
    "\n",
    "Digamos que la hipótesis es el \"ejemplo conveniente\", donde agregamos dos características adicionales, $ X_{1}^2 $ y $ X_{2}^2 $. De esta forma, tenemos cinco parámetros $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] $. Tal como dijimos, más adelante veremos cómo ajustar estos parámetros automáticamente.\n",
    "\n",
    "Digamos que el procedimiento de ajuste de los $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] $ terminó eligiendo los siguientes valores:\n",
    "\n",
    "- Ajuste $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] = [-1, 0, 0, 1, 1] $.\n",
    "\n",
    "Ésto significa que, esta elección particular de parámetros, predice que $ Y = 1 $ siempre que:\n",
    "\n",
    "- Se predice $ Y = 1 $ si y solo si $ 0.5 \\leq g(Z) $. La única forma en que ésto suceda es que Z no sea un número negativo, de otra forma, en la función sigmoide el denominador incrementará, produciendo que converga a 0. Dicho de otra forma, necesitamos que $ 0 \\leq Z $.\n",
    "\n",
    "Resolviendo la inecuación, llegamos a que:\n",
    "\n",
    "- Se predice $ Y = 1 $ si y solo si $ 0 \\leq -1 + X_{1}^{2} + X_{2}^{2}) $, que despejando  $ 1 \\leq + X_{1}^{2} + X_{2}^{2}) $. Recordemos $ X_{0} = 1 $ (bias).\n",
    "\n",
    "De acuerdo a la inecuación, nuestro límite de decisión se vería de la siguiente manera:\n",
    "\n",
    "<img src=\"14.png\"></img>\n",
    "\n",
    "Viéndose como una circunferencia, de forma que, todo fuera del círculo se predecirá como $ Y = 1 $ mientras, lo que está adentro como $ Y = 0 $ (recordemos, X para ejemplos positivos y O para negativos).\n",
    "\n",
    "Así que, al agregar estos términos más complejos, se pueden obtener límites de decisión aún más complejos.\n",
    "\n",
    "Una vez más, el límite de decisión es una propiedad de la hipótesis bajo los parámetros, y no del conjunto de entrenamiento, por lo que:\n",
    "\n",
    "- El vector de parámetros $ \\theta $ siempre definirán el límite de decisión.\n",
    "\n",
    "Sin embargo, lo que utilizamos para definir este límite de decisión, esta propiedad de los parámetros, es el conjunto de entrenamiento.\n",
    "\n",
    "Al tener términos polinómicos de mayor grado, es posible obtener límites de decisión aún más complejos, y la regresión puede ser utilizada para límites de decisión como elipses, u otras formas extravagantes que se ajusten a las nubes de datos.\n",
    "\n",
    "<img src=\"15.jpg\"></img>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (2) La función de costo para la regresión logística\n",
    "\n",
    "Finalmente, ¿cómo ajustamos los parámetros para $ \\theta $ para la regresión logística?\n",
    "\n",
    "Para ello, debemos definir nuestra función objetivo: minimizar el costo a través de la modificación de los parámetros.\n",
    "\n",
    "<img src=\"16.png\"></img>\n",
    "\n",
    "En la imagen, tenemos el problema de aprendizaje supervisado para ajustar la regresión logística al modelo:\n",
    "\n",
    "<img src=\"17.png\"></img>\n",
    "\n",
    "- Representamos el conjunto de entrenamiento con m ejemplos.\n",
    "\n",
    "- El vector de ejemplos X es representado por n + 1 dimensiones.\n",
    "\n",
    "- La primera característica es simple igual a 1 (el bias).\n",
    "\n",
    "- Al tener un problema de clasificación, $ Y = 0 $ o $ Y = 1 $.\n",
    "\n",
    "## Nota\n",
    "\n",
    "Si no se entiende por qué $ m $ y $ n $:\n",
    "\n",
    "- Pues, $ m $ refiere al número de filas, o al número de observaciones.\n",
    "\n",
    "- Por otro lado, la cantidad de características son de $ n $ columnas.\n",
    "\n",
    "- Al vector de parámetros tener todas las características más, un parámetro adicional, que lo llamamos bias, tiene $ n + 1 $ dimensiones.\n",
    "\n",
    "## Continuando\n",
    "\n",
    "La hipótesis y los parámetros $ \\theta $... ¿Cómo los ajustamos dado el conjunto de entrenamiento?\n",
    "\n",
    "<img src=\"18.png\"></img>\n",
    "\n",
    "Antes, cuando estábamos desarrollando el modelo de regresión lineal, usábamos la función de costos. Y pues, no usaremos un algoritmo de optimización muy distinto, de hecho, es bastante similar en términos de la convergencia, puesto la fórmula varía.\n",
    "\n",
    "<img src=\"19.png\"></img>\n",
    "\n",
    "La forma alternativa a esta función de costos la definimos:\n",
    "\n",
    "<img src=\"20.png\"></img>\n",
    "\n",
    "La interpretación de esta función de costo es que queremos que el algoritmo de aprendizaje tenga que <i>pagar o asumir</i> si las salidas de la función no se acercan a los valores reales de $ Y $.\n",
    "\n",
    "## El problema de la no linealidad de la hipótesis\n",
    "\n",
    "Tal como definimos para la regresión lineal, la definición de la función del costo era la mitad del cuadrado de la diferencia entre el valor predicho por la hipótesis $ h_ \\theta (X) $ y el valor real de $ Y $.\n",
    "\n",
    "Aquella función de costos funcionó bien para la regresión lineal, pero ahora estamos interesados en la regresión logística, por lo que, si pudiéramos minimizar esta función de costo particular (considerando la función sigmoide), sucedería que la función <b>no sería convexa</b>, resultando en que el descenso del gradiente no funcionase bien, puesto habrían demasiados mínimos locales, pudiendo no converger correctamente al mínimo global.\n",
    "\n",
    "Al referirnos a que la función de costo no sea convexa, es que la función $ h_ \\theta (X) $ no tiene linearidad, por lo que al colocar $ h_ \\theta (X) $ en la función de costos y la graficamos, $ J(\\theta) $ se vería de la siguiente manera:\n",
    "\n",
    "<img src=\"21.png\"></img>\n",
    "\n",
    "Es decir, tendría muchos mínimos locales, por lo que si iniciáramos el desscenso del gradiente sobre esta función, no habría garantía de que convergiese al mínimo global.\n",
    "\n",
    "En contraste, a lo que nos gustaría como una función de costos $ J(\\theta) $ es que fuera <b>convexa</b> (con forma de bowl), de forma que al arrancar el descenso del gradiente convergiese al mínimo global.\n",
    "\n",
    "<img src=\"22.png\"></img>\n",
    "\n",
    "Sin embargo, el gran problema de ocupar esta función de costos es que debido a esta hipótesis ampliamente no lineal, $ J(\\theta) $ termina siendo una función convexa. Por lo que nos gustaría formular otra función de costos distinta, que sí fuese convexa. De esta forma, <b>tendríamos garantías de que se encontrase el mínimo global</b>.\n",
    "\n",
    "## Primer acercamiento a la función de costo para la función sigmoide\n",
    "\n",
    "Ahora sí, tras ver las problemáticas de aplicar la función de costos de la regresión lineal, a la regresión logística, veremos entonces la función de costos a esta última.\n",
    "\n",
    "<img src=\"23.png\"></img>\n",
    "\n",
    "Diremos que el costo o la penalidad que el algoritmo paga es el costo asociado al valor predicho $ h_ \\theta (X) $ con su valor real:\n",
    "\n",
    "- Siendo el logaritmo negativo de la hipótesis si $ Y = 1 $.\n",
    "\n",
    "- O el logaritmo negativo de la diferencia entre $ 1 $ y la hipótesis si $ Y = 0 $.\n",
    "\n",
    "Graficaremos esta función para tener cierta intuición de lo que hace esta función. Empezaremos con el caso $ Y = 1 $.\n",
    "\n",
    "<img src=\"24.png\"></img>\n",
    "\n",
    "Una forma de comprender por qué el gráfico es así, es porque si tuviéramos que graficar el logaritmo de Z, con Z en el eje horizontal, se vería así:\n",
    "\n",
    "<img src=\"25.png\"></img>\n",
    "\n",
    "Dado que solo nos interesa el intervalo entre 0 y 1 (y que no sea negativo), entonces se le antepone el signo negativo a la función y nos quedamos solo con el intervalo $ [0, 1] $.\n",
    "\n",
    "## Propiedades para $ Y = 0 $ o $ Y = 1 $\n",
    "\n",
    "Esta función de costos, en particular, tiene propiedades interesantes y atractivas:\n",
    "\n",
    "- El costo es cero si $ Y = 1 $ y $ h_ \\theta (X) = 1$: $ COST = 0 $ IF $ Y = 0 $ AND $ h_ \\theta (X) = 1$.\n",
    "\n",
    "- Si $ Y = 1 $ y $ h_ \\theta (X) \\to 0$, entonces el costo es infinito: $ COST = \t\\infty $ IF $ Y = 1 $ AND $ h_ \\theta (X) = 0$.\n",
    "\n",
    "## Resumen de la función de costo (hasta ahora)\n",
    "\n",
    "En este punto, definimos la función de costo para un simple ejemplo de entrenamiento. El tópico de convexividad es posible profundizarlo aún más, siendo posible demostrar que con una elección particular de una función de costo nos dará solución a la problemática de la no linealidad.\n",
    "\n",
    "En general, la función de costo $ J(\\theta) debe ser convexa y óptima para obtener el mínimo global.\n",
    "\n",
    "En el siguiente punto, tomaremos estas ideas vistas, y las llevaremos un poco más lejos, definiendo la función de costos para el conjunto de entrenamiento completo. \n",
    "\n",
    "También, nos daremos cuenta que existe una manera simple y acotada de escribir la función de costo, basado en el algoritmo del descenso del gradiente.\n",
    "\n",
    "## La función de costo simplificada\n",
    "\n",
    "En este punto y el próximo título (3):\n",
    "\n",
    "- Encontraremos una manera más sencilla de escribir la función de coste.\n",
    "\n",
    "- También, descubriremos cómo aplicar el descenso del gradiente para ajustar los parámetros de la regresión logística.\n",
    "\n",
    "De esta forma, al final del punto, seremos capaces de implementar una versión totalmente funcional de la regresión logística.\n",
    "\n",
    "<img src=\"26.png\"></img>\n",
    "\n",
    "He ahí nuestra función de costo para la regresión logística. Es la misma que habíamos definido en el punto anterior.\n",
    "\n",
    "Recordemos que, para los problemas de clasificación, siempre $ Y = 0 $ o $ Y = 1 $. Gracias a esta propiedad, podremos escribir de una manera más acotada nuestra función de costo.\n",
    "\n",
    "Recordemos nuestra función de costo antes definida:\n",
    "\n",
    "- La definíamos como $ \\ J(\\theta) = \\frac{1}{m}\n",
    "\\sum_{i=1}^{m}COST(h_\\theta(X^{(i)}), y^{(i)})) \n",
    "\\\n",
    "$\n",
    "\n",
    "- Donde la función por tramos $ COST(h_\\theta(X^{(i)}), y^{(i)})) $ correspondía a:\n",
    "\n",
    "$ \\  \n",
    "COST(h_\\theta(X^{(i)}), y^{(i)}) = \n",
    "     \\begin{cases}\n",
    "       -\\log(h_\\theta(x)) &\\quad\\text{if Y = 1} \\\\\n",
    "       -\\log(1 - h_\\theta(x)) &\\quad\\text{if Y = 0} \\\\\n",
    "     \\end{cases}\n",
    "\\\n",
    "$\n",
    "\n",
    "- Con $ Y = 0 $ o $ Y = 1 $ siempre.\n",
    "\n",
    "Pues, en vez de escribir en dos líneas separadas, con dos casos separados (función por tramos), podemos escribirlo de forma más comprimida, en una sola ecuación de la siguiente manera:\n",
    "\n",
    "- Definimos $ COST(h_\\theta(X^{(i)}), y^{(i)})) = -y\\log(h_\\theta(X)) - (1 - y)\\log(1-h_\\theta(X)) $.\n",
    "\n",
    "De forma equivalente, simplificamos la función de costo anterior. Veamos el por qué. Sabemos que solo hay dos casos posibles, $ Y = 0 $ o $ Y = 1 $:\n",
    "\n",
    "- Si suponemos  $ Y = 1 $, entonces en la función de costos se cancela el término correspondiente a $ Y = 0 $ en la función por tramos. Es decir, haciendo aritmética básica, queda: $ COST(h_\\theta(X^{(i)}), y^{(i)})) = -\\log(h_\\theta(X)) $.\n",
    "\n",
    "- Por otro lado, al suponer $ Y = 0 $, se cancela el término correspondiente a $ Y = 1 $, quedando: $ COST(h_\\theta(X^{(i)}), y^{(i)})) = \\log(1-h_\\theta(X)) $.\n",
    "\n",
    "Por lo que se comprueba que, la expresión corresponde a una función de costo equivalente a tomar la función por tramos.\n",
    "\n",
    "Ahora, podemos escribir nuestra función de costo para la regresión logística de forma acotada, y esta es la que se utiliza para ajustar modelos de regresión logística.\n",
    "\n",
    "Esta función de costo puede derivarse de las estadísticas usando el [principio de estimación de máxima verosimilitud](https://www.alceingenieria.net/bioestadistica/maxverosim.pdf). El link adjunto de [alceingenieria.net](https://www.alceingenieria.net/bioestadistica/maxverosim.pdf) define, concisamente, el término. \n",
    "\n",
    "En general, es una idea de la estadística para encontrar datos de parámetros eficientemente para diferentes modelos, y también tiene la propiedad de ser convexa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (3) Descenso del gradiente\n",
    "\n",
    "Definida la función de costo, debemos definir nuestra función objetivo, a la cual será necesario ajustar los parámetros.\n",
    "\n",
    "De esta forma:\n",
    "\n",
    "- Para ajustar los parámetros $ \\theta $: $ MIN \\quad J(\\theta) $.\n",
    "\n",
    "Al tratar de minimizar $ J(\\theta) $, se nos brindará un conjunto de parámetros $ \\theta $ que minimizan la función de costo.\n",
    "\n",
    "Finalmente, si se nos da un nuevo ejemplo con características X, entonces podremos tomar los parámetros $ \\theta $ que ajustamos a nuestro conjunto de entrenamiento, y producir nuestra predicción.\n",
    "\n",
    "Y solo para recordar, la salida de la hipótesis se interpretará como la <b>probabilidad</b> de que $ Y = 1 $ dada la entrada $ X $ parametrizada por $ \\theta $.\n",
    "\n",
    "## Nota\n",
    "\n",
    "Parametrizada por \"$ \\theta $\" refiere a que la función que estima la probabilidad se encuentra \"trazada\", o el análogo a parametrizada por $ \\theta $.\n",
    "\n",
    "## Continuando\n",
    "\n",
    "Así que, todo lo que queda por hacer es averiguar cómo minimizar $ J(\\theta)$ para ajustar los parámetros a nuestro conjunto de entrenamiento.\n",
    "\n",
    "La forma en que haremos ésto es con el descenso del gradiente.\n",
    "\n",
    "<img src=\"27.png\"></img>\n",
    "\n",
    "La forma en que vamos a minimizar la función de costo es usando el descenso del gradiente. Recordamos que este algoritmo actualiza, repetidamente, cada parámetro, tomando y actualizándolo como:\n",
    "\n",
    "- $ \\theta_j = \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}}J(\\theta) $.\n",
    "\n",
    "- Simultáneamente actualizando todos los $\\theta_{j}$.\n",
    "\n",
    "En forma más formal, Andrew NG., define:\n",
    "\n",
    "<img src=\"28.png\"></img>\n",
    "\n",
    "Así que, si tenemos $ n $ características, tendríamos un vector de parámetros $ \\theta $ con dimensiones $ n + 1 $.\n",
    "\n",
    "Usando la actualización a través del descenso del gradiente, se podrá actualizar simultáneamente todos los parámetros $ \\theta $.\n",
    "\n",
    "## ¿Son diferentes los algoritmos de actualización?\n",
    "\n",
    "Ahora, si tomamos la regla de actualización y la comparamos con la que estábamos haciendo para la regresión lineal, podemos darnos cuenta que es idéntica a simple vista, pero no nos engañemos.\n",
    "\n",
    "Recordemos ambas hipótesis:\n",
    "\n",
    "- Para la regresión lineal tenemos $ h_\\theta(X) = \\theta^{T}X $.\n",
    "\n",
    "- Para la regresión logística tenemos $ h_\\theta(X) = \\frac{1}{1+e^{- \\theta ^{T}X}} $.\n",
    "\n",
    "Entonces, ¿la regresión lineal y la regresion logística son diferentes algoritmos? Observando la regresión logística observamos que lo que varia es la hipótesis, resultando que:\n",
    "\n",
    "- Al variar la hipótesis, varía el algoritmo del descenso del gradiente, dado que la definición de la hipótesis, en ambos casos son distintas.\n",
    "\n",
    "En un punto anterior, cuando hablábamos de la regresión lineal:\n",
    "\n",
    "- Habíamos hablado de cómo ajustarla para que convergiese correctamente. Usualmente, con la regresión logística, se utiliza el mismo procedimiento.\n",
    "\n",
    "- Respecto a la escala de características, también es aplicable a la regresión logística."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (4) Al fin, la programación\n",
    "\n",
    "~ Sección basada en el video de <b>codebasics</b>: [Logistic Regression (Binary Classification)](https://www.youtube.com/watch?v=zM4VZR0px8E).\n",
    "\n",
    "~ Las temáticas ya abordadas del video serán omitidas.\n",
    "\n",
    "## Regresión lineal\n",
    "\n",
    "En general, en un problema de regresión lineal, podríamos predecir:\n",
    "\n",
    "- Precios de casas.\n",
    "\n",
    "- El clima.\n",
    "\n",
    "- Cotización en la bolsa.\n",
    "\n",
    "En todos estos ejemplos, el valor de la predicción es <b>continuo</b>.\n",
    "\n",
    "## Regresión logística (problemas de clasificación)\n",
    "\n",
    "Existen otros tipos de problemas, tales como:\n",
    "\n",
    "- Predecir si un email es spam o no (sí o no).\n",
    "\n",
    "- Si un cliente comprará un seguro de vida (sí o no).\n",
    "\n",
    "- ¿Qué votación realizará en una determinada persona en una elección?\n",
    "\n",
    "Todos estos ejemplos, el valor de la predicción es <b>categórico</b>. Ésto a razón que la predicción que intentamos realizar es en base a las categorías disponibles.\n",
    "\n",
    "Además, en aquellos problemas en los que tenemos una decisión de un sí o no (afirmativo o positivo), son <b>problemas de clasificación binaria</b>.\n",
    "\n",
    "Por otro lado, en los problemas con más de dos categorías, son <b>problemas de clasificación multiclase</b>.\n",
    "\n",
    "## La base de datos\n",
    "\n",
    "Digamos que estamos trabajando como un Científico de Datos en una compañía de seguros y nuestro encargado nos brinda la tarea de predecir qué tan posible un potencial cliente comprará nuestros servicios.\n",
    "\n",
    "Lo que veremos, a continuación, son nuestros datos disponibles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27 entries, 0 to 26\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype\n---  ------            --------------  -----\n 0   age               27 non-null     int64\n 1   bought_insurance  27 non-null     int64\ndtypes: int64(2)\nmemory usage: 560.0 bytes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   age\n",
       "                 count\n",
       "bought_insurance      \n",
       "0                   13\n",
       "1                   14"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>age</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>bought_insurance</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Importamos numpy y pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Leemos el csv\n",
    "datos_seguros = pd.read_csv('insurance_data.csv')\n",
    "\n",
    "# Vemos la información\n",
    "datos_seguros.info()"
   ]
  },
  {
   "source": [
    "# ¿Con qué datos contamos?\n",
    "\n",
    "Vemos que, tenemos dos datos:\n",
    "\n",
    "- La edad.\n",
    "\n",
    "- Las personas con seguros.\n",
    "\n",
    "De estos datos, tenemos las personas con seguros (sí o no) con sus edades, en formato CSV."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             age  bought_insurance\ncount  27.000000         27.000000\nmean   39.666667          0.518519\nstd    15.745573          0.509175\nmin    18.000000          0.000000\n25%    25.000000          0.000000\n50%    45.000000          1.000000\n75%    54.500000          1.000000\nmax    62.000000          1.000000\n\n Podemos ver que, en el primer cuartil de edad, no hay (a simple vista) personas que compren seguros. Una hipótesis puede ser que, a mayor edad, más probable que las personas compren un seguro.\n"
     ]
    }
   ],
   "source": [
    "# Veamos los estadísticos\n",
    "print(datos_seguros.describe())\n",
    "\n",
    "print(\\\n",
    "    \"\"\"\\n Podemos ver que, en el primer cuartil de edad, no hay (a simple vista) personas que compren seguros. Una hipótesis puede ser que, a mayor edad, más probable que las personas compren un seguro.\"\"\"\\\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Mientras más joven, menos probabilidad de que las personas compren seguros de vida\n",
    "\n",
    "Podemos apreciar como, mientras más joven las personas observadas, será menos probable encontrar personas que hayan comprado un seguro de vida. Y por el contrario, mientras más edad, será más probable que hayan comprado un seguro de vida.\n",
    "\n",
    "## Agrupemos por edad y grafiquemos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='grupo_edad', ylabel='count'>"
      ]
     },
     "metadata": {},
     "execution_count": 136
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.474375pt\" version=\"1.1\" viewBox=\"0 0 382.603125 262.474375\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-02-19T20:08:05.088682</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.474375 \r\nL 382.603125 262.474375 \r\nL 382.603125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\nL 375.403125 7.2 \r\nL 40.603125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 47.299125 224.64 \r\nL 74.083125 224.64 \r\nL 74.083125 17.554286 \r\nL 47.299125 17.554286 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 0 0 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 0 0 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 248.179125 224.64 \r\nL 274.963125 224.64 \r\nL 274.963125 186.988052 \r\nL 248.179125 186.988052 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 0 0 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 74.083125 224.64 \r\nL 100.867125 224.64 \r\nL 100.867125 205.814026 \r\nL 74.083125 205.814026 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 141.043125 224.64 \r\nL 167.827125 224.64 \r\nL 167.827125 205.814026 \r\nL 141.043125 205.814026 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 208.003125 224.64 \r\nL 234.787125 224.64 \r\nL 234.787125 130.51013 \r\nL 208.003125 130.51013 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 274.963125 224.64 \r\nL 301.747125 224.64 \r\nL 301.747125 130.51013 \r\nL 274.963125 130.51013 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p796c74965a)\" d=\"M 341.923125 224.64 \r\nL 368.707125 224.64 \r\nL 368.707125 186.988052 \r\nL 341.923125 186.988052 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1033b01063\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"74.083125\" xlink:href=\"#m1033b01063\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 18 a 30 años -->\r\n      <g transform=\"translate(41.63 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n        <path id=\"DejaVuSans-32\"/>\r\n        <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\nM 32.234375 66.890625 \r\nL 29.4375 69.578125 \r\nQ 28.375 70.5625 27.5625 71.015625 \r\nQ 26.765625 71.484375 26.125 71.484375 \r\nQ 24.265625 71.484375 23.390625 69.703125 \r\nQ 22.515625 67.921875 22.40625 63.921875 \r\nL 16.3125 63.921875 \r\nQ 16.40625 70.515625 18.890625 74.09375 \r\nQ 21.390625 77.6875 25.828125 77.6875 \r\nQ 27.6875 77.6875 29.25 77 \r\nQ 30.8125 76.3125 32.625 74.703125 \r\nL 35.40625 72.015625 \r\nQ 36.46875 71.046875 37.28125 70.578125 \r\nQ 38.09375 70.125 38.71875 70.125 \r\nQ 40.578125 70.125 41.453125 71.90625 \r\nQ 42.328125 73.6875 42.4375 77.6875 \r\nL 48.53125 77.6875 \r\nQ 48.4375 71.09375 45.9375 67.5 \r\nQ 43.453125 63.921875 39.015625 63.921875 \r\nQ 37.15625 63.921875 35.59375 64.59375 \r\nQ 34.03125 65.28125 32.234375 66.890625 \r\nz\r\n\" id=\"DejaVuSans-241\"/>\r\n        <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n        <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"220.3125\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"252.099609\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"315.722656\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"379.345703\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"411.132812\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-241\"/>\r\n       <use x=\"535.791016\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"596.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"141.043125\" xlink:href=\"#m1033b01063\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 30 a 40 años -->\r\n      <g transform=\"translate(108.59 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"220.3125\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"252.099609\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"315.722656\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"379.345703\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"411.132812\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-241\"/>\r\n       <use x=\"535.791016\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"596.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"208.003125\" xlink:href=\"#m1033b01063\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 a 50 años -->\r\n      <g transform=\"translate(175.55 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"220.3125\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"252.099609\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"315.722656\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"379.345703\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"411.132812\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-241\"/>\r\n       <use x=\"535.791016\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"596.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"274.963125\" xlink:href=\"#m1033b01063\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 50 a 60 años -->\r\n      <g transform=\"translate(242.51 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"220.3125\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"252.099609\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"315.722656\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"379.345703\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"411.132812\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-241\"/>\r\n       <use x=\"535.791016\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"596.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.923125\" xlink:href=\"#m1033b01063\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 60 a 65 años -->\r\n      <g transform=\"translate(309.47 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"220.3125\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"252.099609\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"315.722656\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"379.345703\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"411.132812\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-241\"/>\r\n       <use x=\"535.791016\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"596.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_6\">\r\n     <!-- grupo_edad -->\r\n     <g transform=\"translate(178.382031 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"104.589844\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"167.96875\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"231.445312\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"292.626953\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"342.626953\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"404.150391\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"467.626953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"528.90625\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"meed9f17178\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.240625 228.439219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"186.988052\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(27.240625 190.787271)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"149.336104\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(27.240625 153.135323)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"111.684156\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(27.240625 115.483375)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"74.032208\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(27.240625 77.831427)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#meed9f17178\" y=\"36.38026\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(20.878125 40.179478)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_13\">\r\n     <!-- count -->\r\n     <g transform=\"translate(14.798438 130.02625)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"116.162109\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"179.541016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"242.919922\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 40.603125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path d=\"M 375.403125 224.64 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path d=\"M 40.603125 7.2 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_17\">\r\n     <path d=\"M 275.101562 59.5125 \r\nL 368.403125 59.5125 \r\nQ 370.403125 59.5125 370.403125 57.5125 \r\nL 370.403125 14.2 \r\nQ 370.403125 12.2 368.403125 12.2 \r\nL 275.101562 12.2 \r\nQ 273.101562 12.2 273.101562 14.2 \r\nL 273.101562 57.5125 \r\nQ 273.101562 59.5125 275.101562 59.5125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- bought_insurance -->\r\n     <g transform=\"translate(277.101562 23.798438)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"124.658203\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"188.037109\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"251.513672\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"314.892578\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"354.101562\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"404.101562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"431.884766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"495.263672\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"547.363281\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"610.742188\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"651.855469\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"713.134766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"776.513672\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"831.494141\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_18\">\r\n     <path d=\"M 304.571094 38.754688 \r\nL 324.571094 38.754688 \r\nL 324.571094 31.754688 \r\nL 304.571094 31.754688 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- 0 -->\r\n     <g transform=\"translate(332.571094 38.754688)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-48\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_19\">\r\n     <path d=\"M 304.571094 53.432813 \r\nL 324.571094 53.432813 \r\nL 324.571094 46.432813 \r\nL 304.571094 46.432813 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- 1 -->\r\n     <g transform=\"translate(332.571094 53.432813)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-49\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p796c74965a\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEHCAYAAACp9y31AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAagklEQVR4nO3dfXhU9Z338ffXBIwKipDYagMmtmoVAhHCQxeKWargutRHtHhpF25ESqvebl1d7eWuCu0+2HvXu67uyoXeFWsVraFWpJeKRal1pWACAQKUxRYWolYDurs+oPLwvf84J8kQBzIkmTmT/D6v65orZ86cc37f/DL5zJnfmTnH3B0REQnHEUkXICIiuaXgFxEJjIJfRCQwCn4RkcAo+EVEAlOYdAGZKC4u9rKysqTLEBHpVurq6na6e0nb+d0i+MvKyqitrU26DBGRbsXM/jPdfA31iIgERsEvIhIYBb+ISGC6xRi/iOSvPXv20NjYyMcff5x0KcEqKiqitLSUXr16ZbS8gl9EOqWxsZG+fftSVlaGmSVdTnDcnV27dtHY2Eh5eXlG62ioR0Q65eOPP2bAgAEK/YSYGQMGDDisd1wKfhHpNIV+sg63/xX8IiKBUfCLiASm2x/cHXHzT3LSTt3/+YuctCPSE2zbto3JkyfT0NCQ8+0vWLCAiRMnctJJJx10/ZkzZ3LjjTdy5plnZqW+fKc9fhHpURYsWMCbb755yGUefPDBrIb+vn37srbtrqDgF5Gs2Lt3L9OmTWPo0KFMmTKFjz76iGXLlnHWWWdRUVHBjBkz+OSTT4DofFw7d+4EoLa2lurqagCampo499xzGT58ON/61rc4+eSTW5bbt28f11xzDYMHD2bixIns3r2bmpoaamtrufLKK6msrGT37t1pa6uurm45/1efPn247bbbGDZsGGPGjOHtt98G4Mknn2TIkCEMGzaM8ePHA9GLynXXXdeyncmTJ7N8+fKW7dx+++2MHj2aFStWMHfuXEaOHMmQIUOYNWsWzZe5ra6u5pZbbmHUqFGcdtpp/OY3v2n5fW666SYqKioYOnQo9957LwB1dXWcffbZjBgxgkmTJvHWW291+m+j4BeRrNi8eTOzZs1i3bp1HHvssdx9991Mnz6dJ554gvXr17N3717uv//+Q25jzpw5TJgwgdWrV3PxxRezffv2lse2bNnCtddey4YNG+jXrx+LFi1iypQpVFVV8eijj1JfX89RRx3Vbp0ffvghY8aMYe3atYwfP54HHngAgLlz5/L888+zdu1aFi9enNF2hgwZwsqVKxk3bhzXXXcdr732Gg0NDezevZslS5a0LLt3715WrVrFj370I+bMmQPA/Pnz2bp1K2vWrGHdunVceeWV7Nmzh+uvv56amhrq6uqYMWMGt912W7u1tEfBLyJZMXDgQMaOHQvAVVddxbJlyygvL+e0004DYNq0abz88suH3MYrr7zC1KlTATjvvPM4/vjjWx4rLy+nsrISgBEjRrBt27YO1dm7d28mT578me2MHTuW6dOn88ADD2Q0dFNQUMCll17acv+ll15i9OjRVFRU8OKLL7Jhw4aWxy655JLPtPerX/2K2bNnU1gYHXrt378/mzdvpqGhgXPPPZfKykp+8IMf0NjY2KHfM1W3P7grIvnpcD5bXlhYyP79+wEO+CJS8/BIOkceeWTLdEFBwUGHddrTq1evlloLCgrYu3cvAPPmzWPlypX88pe/pLKykvr6+gPqbFtrUVERBQUFLfO/853vUFtby8CBA7nzzjsPWLa59tT23P0zfebuDB48mBUrVnTodzsY7fGLSFZs3769JbAWLlzIOeecw7Zt23j99dcBeOSRRzj77LOBaIy/rq4OgEWLFrVsY9y4cfzsZz8DYOnSpbz33nvtttu3b1/ef//9Ttf/+9//ntGjRzN37lyKi4vZsWMHZWVl1NfXs3//fnbs2MGqVavSrtsc8sXFxXzwwQfU1NS0297EiROZN29eywvBu+++y+mnn05TU1NLP+7Zs+eAdw4dpeAXkaw444wzePjhhxk6dCjvvvsu3/3ud3nooYe47LLLqKio4IgjjmD27NkA3HHHHdxwww189atfbdlrbp6/dOlShg8fzrPPPsuJJ55I3759D9nu9OnTmT179iEP7mbi5ptvpqKigiFDhjB+/HiGDRvG2LFjKS8vp6Kigptuuonhw4enXbdfv35cc801VFRUcNFFFzFy5Mh225s5cyaDBg1i6NChDBs2jMcee4zevXtTU1PDLbfcwrBhw6isrOTVV1/t8O/UzA71VipfVFVV+cGuwKXP8Yska9OmTZxxxhlZ2fYnn3xCQUEBhYWFrFixgm9/+9vU19dnpa3uLt3fwczq3L2q7bIa4xeRvLV9+3Yuv/xy9u/fT+/evVs+cSOdo+AXkbx16qmnsmbNmg6vf/HFF7N169YD5t11111MmjSps6V1awp+EemxnnrqqaRLyEs6uCsiEhgFv4hIYBT8IiKBydoYv5n9GJgMvOPuQ+J5/YEngDJgG3C5u7f/jQwRkYPo6o90Z/rR7eeee44bbriBffv2MXPmTG699dYurSObsrnHvwA4r828W4Fl7n4qsCy+LyLSrezbt49rr72WZ599lo0bN7Jw4UI2btyYdFkZy1rwu/vLwLttZl8IPBxPPwxclK32RUSyZdWqVXzpS1/ilFNOoXfv3kydOpWnn3466bIylusx/s+5+1sA8c8Tcty+iEinvfHGGwwcOLDlfmlpKW+88UaCFR2evD24a2azzKzWzGqbmpqSLkdEpEW6U90cztlIk5br4H/bzE4EiH++c7AF3X2+u1e5e1VJSUnOChQRaU9paSk7duxoud/Y2HjIa/zmm1wH/2JgWjw9Deg+g2IiIrGRI0eyZcsWtm7dyqeffsrjjz/OBRdckHRZGcvmxzkXAtVAsZk1AncA/wj8zMyuBrYDl2WrfREJQxJnzi0sLOS+++5j0qRJ7Nu3jxkzZjB48OCc19FRWQt+d7/iIA99LVttiojkyvnnn8/555+fdBkdkrcHd0VEJDsU/CIigVHwi4gERsEvIhIYBb+ISGAU/CIigdGlF0WkW9s+t6JLtzfo9vXtLjNjxgyWLFnCCSecQENDQ5e2nwva4xcROUzTp0/nueeeS7qMDlPwi4gcpvHjx9O/f/+ky+gwBb+ISGAU/CIigVHwi4gERsEvIhIYfZxTRLq1TD5+2dWuuOIKli9fzs6dOyktLWXOnDlcffXVOa+joxT8IiKHaeHChUmX0Cka6hERCYyCX0QkMAp+Eek0d0+6hKAdbv8r+EWkU4qKiti1a5fCPyHuzq5duygqKsp4HR3cFZFOKS0tpbGxkaampqRLCVZRURGlpaUZL6/gF5FO6dWrF+Xl5UmXIYdBQz0iIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgEgl+M/uumW0wswYzW2hmmX/XWEREOiXnwW9mXwD+N1Dl7kOAAmBqrusQEQlVUkM9hcBRZlYIHA28mVAdIiLByXnwu/sbwD8B24G3gP9296VtlzOzWWZWa2a1OvmTiEjXSWKo53jgQqAcOAk4xsyuarucu8939yp3ryopKcl1mSIiPVYSQz3nAFvdvcnd9wA/B/4kgTpERIKURPBvB8aY2dFmZsDXgE0J1CEiEqQkxvhXAjXAamB9XMP8XNchIhKqRC7E4u53AHck0baISOj0zV0RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcBkFPxmtiyTeSIikv8KD/WgmRUBRwPFZnY8YPFDxwInZbk2ERHJgkMGP/At4C+JQr6O1uD/H+Bfs1eWiIhkyyGD393vAe4xs+vd/d4c1SQiIlnU3h4/AO5+r5n9CVCWuo67/6QjjZpZP+BBYAjgwAx3X9GRbYmIyOHJKPjN7BHgi0A9sC+e7UCHgh+4B3jO3aeYWW+i4wgiIpIDGQU/UAWc6e7e2QbN7FhgPDAdwN0/BT7t7HZFRCQzmX6OvwH4fBe1eQrQBDxkZmvM7EEzO6btQmY2y8xqzay2qampi5oWEZFMg78Y2Ghmz5vZ4uZbB9ssBIYD97v7WcCHwK1tF3L3+e5e5e5VJSUlHWxKRETaynSo584ubLMRaHT3lfH9GtIEv4iIZEemn+r5dVc16O5/NLMdZna6u28GvgZs7Krti4jIoWX6qZ73iT7FA9Ab6AV86O7HdrDd64FH40/0/AH4Xx3cjoiIHKZM9/j7pt43s4uAUR1t1N3riT4pJCIiOdahs3O6+y+ACV1bioiI5EKmQz2XpNw9gmhvvdOf6RcRkdzL9FM9X0+Z3gtsAy7s8mpERCTrMh3j18FXEZEeItMLsZSa2VNm9o6ZvW1mi8ysNNvFiYhI18v04O5DwGKi8/J/AXgmniciIt1MpsFf4u4Pufve+LYA0HkURES6oUyDf6eZXWVmBfHtKmBXNgsTEZHsyDT4ZwCXA38E3gKmoG/bioh0S5l+nPP7wDR3fw/AzPoD/0T0giAiIt1Ipnv8Q5tDH8Dd3wXOyk5JIiKSTZkG/xFmdnzznXiPP9N3CyIikkcyDe9/Bl41sxqiUzVcDvxd1qoSEZGsyfSbuz8xs1qiE7MZcIm76xz6Erztcyty1tag29fnrK2OUF90HxkP18RBr7AXEenmOnRaZhER6b4U/CIigVHwi4gERsEvIhIYBb+ISGAU/CIigVHwi4gERsEvIhIYBb+ISGAU/CIigVHwi4gERsEvIhIYBb+ISGASC/74ou1rzGxJUjWIiIQoyT3+G4BNCbYvIhKkRILfzEqBPwceTKJ9EZGQJbXH/yPgr4H9CbUvIhKsnAe/mU0G3nH3unaWm2VmtWZW29TUlKPqRER6viT2+McCF5jZNuBxYIKZ/bTtQu4+392r3L2qpKQk1zWKiPRYOQ9+d/+eu5e6exkwFXjR3a/KdR0iIqHS5/hFRAJTmGTj7r4cWJ5kDSIiodEev4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEhgFv4hIYHIe/GY20MxeMrNNZrbBzG7IdQ0iIiErTKDNvcBfuftqM+sL1JnZC+6+MYFaRESCk/M9fnd/y91Xx9PvA5uAL+S6DhGRUCWxx9/CzMqAs4CVaR6bBcwCGDRoUG4LE+lBRtz8k5y081TfnDTTLWyfW5Gztgbdvv6w10ns4K6Z9QEWAX/p7v/T9nF3n+/uVe5eVVJSkvsCRUR6qESC38x6EYX+o+7+8yRqEBEJVRKf6jHg/wGb3P3uXLcvIhK6JPb4xwLfBCaYWX18Oz+BOkREgpTzg7vu/gpguW5XREQi+uauiEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgEr30YneS75dSyyX1hUj3pj1+EZHAKPhFRAKj4BcRCYyCX0QkMAp+EZHAKPhFRAKj4BcRCYyCX0QkMAp+EZHAKPhFRAKj4BcRCYyCX0QkMAp+EZHAKPhFRAKj4BcRCUwiwW9m55nZZjN73cxuTaIGEZFQ5Tz4zawA+Ffgz4AzgSvM7Mxc1yEiEqok9vhHAa+7+x/c/VPgceDCBOoQEQmSuXtuGzSbApzn7jPj+98ERrv7dW2WmwXMiu+eDmzOaaGfVQzsTLiGfKG+aKW+aKW+aJUvfXGyu5e0nZnENXctzbzPvPq4+3xgfvbLyYyZ1bp7VdJ15AP1RSv1RSv1Rat874skhnoagYEp90uBNxOoQ0QkSEkE/2vAqWZWbma9ganA4gTqEBEJUs6Hetx9r5ldBzwPFAA/dvcNua6jA/Jm2CkPqC9aqS9aqS9a5XVf5PzgroiIJEvf3BURCYyCX0QkMN0q+M3sx2b2jpk1tJlfaWa/NbN6M6s1s1GdaOM5M1trZhvMbF78TWPM7EgzeyI+zcRKMys7zO32N7NlZrbUzO7qaH3xtorMbFVKnXPatPOCmW2Jfx7fmbbibS5O7fN86ouUbRaY2RozW9KmnS7pCzNbHp9mpD6+nRDPz6u+MLNtZra++X+hTTtd1Re9zWy+mf2Hmf3OzC6N5+dbX/Qzs5q4xk1m9pWUdrLdF9PNrCnl+TLzMLfb5f8jB3D3bnMDxgPDgYY285cCfxZPnw8s70Qbx8Y/DVgETI3vfweYF09PBZ5IsB8M6BNP9wJWAmPi+z8Ebo2nbwXu6mRblwCPpfZ5PvVFSk03xnUuSZnXZX0BLAeq0szPq74AtgHFaeZ3ZV/MAX4QTx/R3F4e9sXDwMx4ujfQL4d9MR24L8nf/5B1J11ABzq6LE3wPw98I56+AngszXp9gGXAamA9cGE77fQCnknZ7vPAV+LpQqJv5Vma9X4B1AEbgFkp8z8A/g5YC/wW+Fw8/+S4rnXxz0Hx/MuAhnj5lw9R59Hx7zQ6vr8ZODGePhHY3NG+iJd7heicSqnBn1d9QfRdkGXABA4M/q7si+WkD/5864ttpA/+ruyLHcAx+dwXwLHA1oO0n4u+mE4GwZ+r58Vn2m1vgXy7kT74zwC2x3+EN4i+ptx2vUJa9+aLgdfTPSlSnsDvEe1BFsTzGoDSlGV+f5B/sP7xz6PidQbE9x34ejz9Q+Bv4ulngGnx9AzgF/H0euAL8XS/NO0UAPXxE+SulPn/1Wa59zraF8D/BS5u2+d52Bc1wAigmgODvyv7YnlcRz3wt83L5GFfbCUKqzoODJIu6QugH9H/2d1xO0/SGkp50xdAJbAKWACsAR4kDugc9cV04C2igK4BBrZtI5fPi8+0294C+XYjffD/C3BpPH058Ks06/UC7ov/EPXAbuDzh2iniGio59z4/oY0T+oBada7k+hVdy3w37QOwXxCa1h8A3gwnt4J9EqpcWc8PQ94AbgmXTttnnwvAUMO40ndbl/E/zjPpOvzfOoLYDLwb/F0NYcf/Bk9L2j9p+pLNLT4F/nWF/HjJ8U/T4jbGt/Fz4tiolBq/n+7EXgk3/oCqAL20vpO+B7g+znsiwHAkfH0bODFg/z/5uR50fbWrQ7uHsI04Ofx9JNEZwBt60qgBBjh7pXA20Thnpa7f0z0jeIL41ktp5ows0LgOODd1HXMrBo4h+jt7jCiPY3mNvZ4/BcC9nHwL8953P5s4G/iNuvNbMBB6vwvor3R8+JZb5vZiXE9JwLvpFktk774CjDCzLYRDfecZmbL48fyqS/GAhfEdT4OTDCzn3ZxX+Dub8Q/3yd6J9j8HMunvsDd34x/vgM8lVJnV/XFLuCjeNsQ/b8Nz8O+aAQa3X1lfL8mpc6s94W773L3T+L5DxC9Iz1AEnnRrKcE/5vA2fH0BGBLmmWOA95x9z1m9qdEY2UHMLM+KU+IQqIDxb+LH15M9AIDMIXoFdzbbOI4or2Hj8zsy8CYDGp/lehAGERPtlfi9r/o7ivd/XaiV/mW8xuZWYmZ9YunjyJ68qSrcxrwdJo22+0Ld7/f3U9y9zJgHPAf7l6dpo1E+8Ldv+fupXGdU+NarkpTZ4f7wswKzaw4nu5F9C6j+VNOedMXZnaMmfVtngYmHqTOzjwvnGi4oTqe9TVgY5o2kn5e/BHYYWant1NnVvqiOUdiFwCbDtJG1vsirfbeEuTTDVhING62h+gV/ep4/jiiMc21RJ9wGZFm3WJgBVBLNN63CShrs8zniM4ltI7obeu9QGH8WBHRK/rrRGOHp6Rp40jg2Xj9J4n2xKvjxz5IWW4KsCCeLgNe5LMHa35ONG7XQPQ21VLWH0q0d7Aufvz2lMcGxNvZEv/s35G+aLN8GQcO9eRNX7Rps5oDh3q6pC+AY+LnV/Pz4h5aj/3kTV8Ap9A6bLABuC0bzwuiEHw5TW150xfx45Xx77KO6CDq8Tnsi3+I/wZriYZiv5wP/yPNN52yQUQkMD1lqEdERDKk4BcRCYyCX0QkMAp+EZHAKPhFRAKj4BcRCYyCXyQBZladegrpDNdZbmZV2apJwqHgl6DE38gWCZqCX3oUM/vb+IIYL5jZQjO7Kd5T/nsz+zVwg5ktMLMpKet8EP+sNrOXzewpM9to0YV4jogfuyK+wElDexfGMLOJZrbCzFab2ZNm1ieef15c2ytE1zloXn6Umb1q0YVkXm0+zYCZHWVmj5vZOjN7gugMjiKdpuCXHiMeBrkUOIsoWFOHRfq5+9nu/s/tbGYU8FdABfBF4BIzOwm4i+g8UJXASDO76CA1FBOdLOscdx9O9JX/G82siOhkXV8Hvgp8PmW13xGdRfMs4Hbg7+P53wY+cvehROdm/8yJvkQ6Qm97pScZBzzt7rsBzOyZlMeeyHAbq9z9D/H6C+Nt7iG6qltTPP9RoqvB/SLN+mOILlzz72YG0ZWfVgBfBra6+5Z4Gz8FZsXrHAc8bGanEp1tsVc8fzzRKcdx93Vmti7D30HkkBT80pPYIR77MGV6L/G7XYvSuXfKY21PXuXtbDddDS+4+xUHzDSrTLPtZt8HXnL3iy26Tu3yQ9Qj0mka6pGe5BXg6xZdjL4P8OcHWW4brcMmF9K6hw0wyszK47H9b8TbXAmcbWbFZlZAdHnPXx9k278FxprZlwDM7GgzO41oOKfczL4YL5f6wnAc0ZXjILpyU7OXiU69i5kNITorq0inKfilx3D314jOtb6W6DS1tURXNWrrAaIgXwWM5sB3AyuAfyQ6ve1W4Cl3fwv4HtHpddcCq9093TnciYeDpgML46GZ3xKdkvdjoqGdX8YHd/8zZbUfAv9gZv9OdEnNZvcDfeLt/DXRqY5FOk2nZZYexcz6uPsHZnY00R7zLHdfneG61cBN7j45iyWKJE5j/NLTzDezM4kuCvJwpqEvEhLt8Yt0kJmtJLqKUqpvuvv6JOoRyZSCX0QkMDq4KyISGAW/iEhgFPwiIoFR8IuIBOb/A48qPvoAPt+0AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Agrupemos por grupo de edad\n",
    "bins = [17, 30, 40, 50, 60, 65]\n",
    "labels = ['18 a 30 años', '30 a 40 años', '40 a 50 años', '50 a 60 años', '60 a 65 años']\n",
    "datos_seguros['grupo_edad'] = pd.cut(datos_seguros['age'], bins=bins, labels=labels)\n",
    "\n",
    "# Imprimimos\n",
    "datos_seguros\n",
    "\n",
    "# Agrupamos por edad y vemos cuántos tienen seguros y cuántos no\n",
    "grupos_edad = datos_seguros.groupby(by=['grupo_edad', 'bought_insurance']).agg(['count'])\n",
    "grupos_edad\n",
    "\n",
    "# (1) Importamos seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Graficamos con Seaborn rápidamente\n",
    "sns.countplot(x='grupo_edad', hue='bought_insurance', data=datos_seguros)\n",
    "\n",
    "# (2) Importamos matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficamos con matplotlib (más tedioso)\n",
    "# colors = {0:'red', 1:'blue'}\n",
    "# ax.bar(datos_seguros['grupo_edad'], datos_seguros['bought_insurance'], color=datos_seguros['bought_insurance'].apply(lambda x: colors[x]))"
   ]
  },
  {
   "source": [
    "Con el gráfico, se hace más visible la tendencia.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Ahora, queremos realizar una predicción de si los clientes comprarán un seguro de vida o no en base a sus edades."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}