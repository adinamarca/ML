{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "510dd6fa91319fddc5d79621a07358f04a8fa48e6ad602d55eaa17fc7a04de2d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Regresión logística\n",
    "\n",
    "<img src=\"1.png\"></img>\n",
    "\n",
    "~ Basado en el video de <b>codebasics</b>: [Logistic Regression](https://www.youtube.com/watch?v=zM4VZR0px8E) y curso de Machine Learning de la Universidad de Standford, de Andrew Ng.\n",
    "\n",
    "~ Imágenes extraídas desde el curso de Andrew NG de Coursera (Machine Learning de Standford University), canal de YouTube de DOT.CSV, Wikipedia, Slideshare, Statdeveloper, entre otros.\n",
    "\n",
    "~ Recomendado saber sobre cursos de cálculo de universidad (cálculo I, II y III), programación, álgebra y estadística.\n",
    "\n",
    "## Aviso\n",
    "\n",
    "Antes de comenzar, y para cualquiera que esté leyendo (<i>por cierto, me sorprendería que a alguien le sea útil</i>), este notebook será más largo de lo habitual por dos razones:\n",
    "\n",
    "• Se explicarán los problemas de clasificación.\n",
    "\n",
    "• Tengo muchísimo material de regresión logística.\n",
    "\n",
    "Bueno, antes de iniciar de lleno con redes neuronales (en el próximo notebook), debemos aprender y comprender lo que son los problemas de clasificación, dado que con ellos, seremos capaces de categorizar y utilizar clases, trabajando con datos discretos.\n",
    "\n",
    "## (1) Problemas de clasificación\n",
    "\n",
    "Los problemas de clasificación implican predecir una variable que tiene valores discretos. Esto implica que nuestro output tendrá un carácter booleano:\n",
    "\n",
    "• $(Y = 0)$ OR $(Y = 1)$.\n",
    "\n",
    "• En este sentido, nuestra hipótesis, $ 0 \\leq h_ \t\\theta (X) \\leq 1 $.\n",
    "\n",
    "Empezaremos con problemas de dos clases o de <b>clasificación binaria</b>. Entonces, ¿cómo se desarrolla un algoritmo de clasificación? A continuación, hallaremos un conjunto de entrenamiento para una tarea de clasificación sobre si un tumor es beligno o maligno. Observemos que si es benigno o no, adquiere únicamente dos valores, 0 de no y 1 de sí.\n",
    "\n",
    "<img src=\"3.png\"></img>\n",
    "\n",
    "Anteriormente, habíamos mencionado que el clasificador $ 0 \\leq h_ \\theta (X) \\leq 1 $, por lo que nos gustaría una hipótesis que satisfaga esta propiedad, es decir, que estas predicciones estén entre cero y uno.\n",
    "\n",
    "Cuando usábamos la regresión lineal, conocíamos nuestra hipótesis como $ h_ \\theta (X) $. Además, si nos ayudamos del álgebra lineal, podemos apreciar que, podemos expresar nuestra fórmula de la siguiente manera:\n",
    "\n",
    "• Consideramos $ \\theta ^{T} $ nuestra matriz de parámetros $ \\theta $s.\n",
    "\n",
    "• Además, $ X $ estaría compuesta por nuestras características.\n",
    "\n",
    "De esta forma, podemos expresar nuestra regresión lineal de forma acotada:\n",
    "\n",
    "$ h_ \\theta (X) = \\theta ^{T}X $\n",
    "\n",
    "En caso de no haber comprendido del todo el procedimiento, recomiendo leer [el siguiente artículo de Andrew NG sobre aprendizaje supervisado (aprox. pág. 3)](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf).\n",
    "\n",
    "## Regresión logística\n",
    "\n",
    "La regresión logística pretende estimar la probabilidad de un determinado evento, medible por variables categóricas y numéricas, <b>que se sabe está correlacionado con ciertas variables cuantitativas</b>. \n",
    "\n",
    "Puede ser, que hasta el momento, se esté algo perdido, dado que introducimos dos conceptos que no están del todo explicados.\n",
    "\n",
    "Recordemos:\n",
    "\n",
    "• $(Y = 0)$ OR $(Y = 1)$.\n",
    "\n",
    "• Hipótesis, $ 0 \\leq h_ \\theta (X) \\leq 1 $.\n",
    "\n",
    "¿Por qué no utilizar la regresión lineal en vez de la regresión logística?\n",
    "\n",
    "Pues, intentemos. \n",
    "\n",
    "Dado el conjunto de entrenamiento del ejemplo anterior, de los tumores, podríamos ajustar una regresión lineal, obteniendo una hipótesis que tal vez, se parecería a la siguiente.\n",
    "\n",
    "<img src=\"4.png\"></img>\n",
    "\n",
    "Pero, ¿cómo haríamos para obtener, predicciones en valores discretos entre 0 y 1?Para ello, aplicaremos un umbral, de forma que dado los resultados del clasificador:\n",
    "\n",
    "• Nuestra hipótesis definirá $ Y = 1 $ si nuestros resultados, son $ 0.5 \\leq h_ \\theta (X) $.\n",
    "\n",
    "• De otra forma ($ 0.5 > h_ \\theta (X) $), nuestra hipótesis definirá $ Y = 0 $.\n",
    "\n",
    "Pareciera que la regresión lineal está realizando algo razonable, a pesar se trata de una tarea de clasificación. Sin embargo, cambiemos un poco el problema.\n",
    "\n",
    "Supongamos que tenemos otro conjunto de entrenamiento, pero con outliers (datos atípicos), de esta forma, la pendiente variará, resultando en un modelo con menos exactitud.\n",
    "\n",
    "Por dicha razón, <b color=\"red\">no es conveniente aplicar regresión lineal en problemas de clasificación</b>, y de hecho, es una <i>pésima práctica</i>.\n",
    "\n",
    "## Representación de la regresión logística\n",
    "\n",
    "A continuación, representaremos hipótesis, que es, la función que vamos a utilizar para los problemas de clasificación.\n",
    "\n",
    "Anteriormente, habíamos dicho que nos gustaría que el clasificador genere valores que se encuentren entre cero y uno, por lo que necesitamos una hipótesis que satisfaga esta propiedad.\n",
    "\n",
    "Antes, con al regresión lineal, habíamos definido la siguiente hipótesis:\n",
    "\n",
    "<img src=\"5.png\"></img>\n",
    "\n",
    "Donde:\n",
    "\n",
    "• $ \\theta ^{T} $ definía nuestro vector de parámetros.\n",
    "\n",
    "• $ X $ definía nuestro vector de características (variables).\n",
    "\n",
    "• $ h_ \\theta (X) $ es nuestra hipótesis.\n",
    "\n",
    "Para la regresión logística modificaremos levemente la hipótesis de la regresión lineal $ h_ \\theta (X) = \\theta ^{T}X $, de forma que $ h_ \\theta (X) \\in [0, 1] $, por lo que la definiremos de la siguiente forma:\n",
    "\n",
    "- Regresión lineal: $ h_ \\theta (X) = \\theta ^{T}X $.\n",
    "\n",
    "- Regresión logística: $ h_ \\theta (X) = g( \\theta ^{T}X) $.\n",
    "\n",
    "De esta forma, definimos la regresión logística a través de una composición de funciones (combinación en palabras simples), dado que, la función $ h_ \\theta (X) $ está definida en otra función $ g( \\theta ^{T}X) $. \n",
    "\n",
    "Dado que en el Machine Learning tiende a acotarse la nomenclatura para que sea conveniente de escribir en código (y más eficiente a través de matrices), definiremos otra abreviatura a través de la nueva introducida hipótesis:\n",
    "\n",
    "- Siendo $ h_ \\theta (X) = g( \\theta ^{T}X) $, definimos $ Z =  \\theta ^{T}X $, de forma que, $ h_ \\theta (X) = g(Z) $. ¿Más simple, no?\n",
    "\n",
    "Bien. Hasta ahora, hemos hablado de composición de funciones, nomenclatura... Pero, ¿qué función es en verdad $ g(Z) $?\n",
    "\n",
    "- Definimos la regresión logística o sigmoidea como $ h_ \\theta (X) = g(Z) $, donde $ g(Z) = \\frac{1}{1+e^{-z}} $ y $ Z = \\theta ^{T}X $, por lo que $ g( \\theta ^{T}X) = \\frac{1}{1+e^{- \\theta ^{T}X}} $.\n",
    "\n",
    "Gracias a un fragmento de Wikipedia, así se ve el gráfico de la función de la hipótesis (azul) con su derivada (línea roja).\n",
    "\n",
    "<img src=\"6.png\"></img>\n",
    "\n",
    "- La función sigmoidea tiene dos asíntotas. Mientras $ Z $ va hacia menos infinito, $ g(Z) $ se aproxima a cero y mientras $ g(Z) $ tiende a infinito $ Z $ se aproxima a uno. Es decir, $ g(Z) $ ofrece valores que están entre 0 y 1 pero también tenemos que $ h_ \\theta (X) $ debe estar entre 0 y 1.\n",
    "\n",
    "- Como antes, lo que debemos hacer es ajustar los parámetros $ \\theta $ a nuestros datos. Así que dado un conjunto de entrenamiento, necesitamos elegir un valor para los parámetros $ \\theta $ y esta hipótesis nos permitirá hacer predicciones. \n",
    "\n",
    "- Hablaremos del algoritmo de aprendizaje más adelante respecto al ajuste de los parámetros $ \\theta $, mientras tanto, consideraremos la interpretación del modelo. Es decir, cómo se interpreta el resultado de la hipótesis $ h_ \\theta (X) $.\n",
    "\n",
    "- La interpretación de la salida de la hipótesis se entiende a partir de $ h_ \\theta (X) = probabilidad $, donde la $ probabilidad $ es la prob. estimada que $ y = 1 $ en el ejemplo de entrada de $ X $.\n",
    "\n",
    "Si suponemos que, $ h_ \\theta (X) = probabilidad $, siendo  $ X = [ X_{0}, X_{1} ] = [1, houseSize] $, donde, tras introducir la variable en la hipótesis, nos arroja la <i>probabilidad de vender una casa</i> igual a $ h_ \\theta (X) = 0.7 $, entonces la hipótesis estará prediciendo que existe una probabilidad del 70% de que se venda una casa dado el tamaño de la misma.\n",
    "\n",
    "En probabilidad, ésto se entiende:\n",
    "\n",
    "<img src=\"7.png\"></img>\n",
    "\n",
    "Donde:\n",
    "\n",
    "- Entendemos $ Y \\in [0, 1] $, siendo $ Y $ la probabilidad de que se venda la casa ($ Y = 1 $), dado que \"X\" tiene un tamaño particular, siendo esta probabilidad parametrizada por $ \\theta $.\n",
    "\n",
    "- La hipótesis nos dará las estimaciones de la probabilidad de que $ Y = 1 $.\n",
    "\n",
    "Ahora, de la misma forma, podemos estimar la probabilidad de $ Y = 0 $. Y de hecho, la suma de que $ Y = 0 OR  Y = 1 $ es igual a la probabilidad de unión, que en este caso, corresponde al 100% al tratarse de un evento con solo dos posibilidades (binario). Si tuviésemos una regresión logística multiclase,deberíamos incorporar mayor número de eventos, o clasificadores, y por ende, su suma no sería el 100%, sino el n-número de clases.\n",
    "\n",
    "<img src=\"8.jpg\"></img>\n",
    "\n",
    "<i>Extraído desde</i> [Slideshare](https://es.slideshare.net/RichardHuamanDurand/estadistica-y-probabilidades-cap-vi2).\n",
    "\n",
    "De esta forma, podemos obtener las probabilidades de que $ Y = 0 $ o bien, $ Y = 1 $ con simples despejes.\n",
    "\n",
    "## Límite de decisión\n",
    "\n",
    "En el punto anterior, vimos la representación de la regresión logística, hablamos de su representación de la hipótesis y además, del cómo la regresión lineal no se ajusta a las necesidades de los problemas de clasificación.\n",
    "\n",
    "En el presente punto, hablaremos del <b>límite de decisión</b>, que nos dará una mejor idea de lo que las regresiones logísticas, y en particular, sus hipótesis, son capaces de computar.\n",
    "\n",
    "Para recapitular, recordemos lo que establecimos como hipótesis:\n",
    "\n",
    "- Siendo $ h_ \\theta (X) = g( \\theta ^{T}X) $, definimos $ Z = \\theta ^{T}X $, de forma que, $ h_ \\theta (X) = g(Z) $.\n",
    "\n",
    "- Definimos la regresión logística o sigmoidea como $ h_ \\theta (X) = g(Z) $, donde $ g(Z) = \\frac{1}{1+e^{-z}} $ y $ Z = \\theta ^{T}X $, por lo que $ g( \\theta ^{T}X) = \\frac{1}{1+e^{- \\theta ^{T}X}} $.\n",
    "\n",
    "Concretamente, esta hipótesis, está dando estimaciones de la probabilidad de que $ Y = 1 $, dado X y parametrizado por $ \\theta $, por lo que, si quisiéramos predecir si $ Y = 1 $ o $ Y = 0 $, lo que deberíamos hacer sería:\n",
    "\n",
    "- Suponer predicción de que $ Y = 1 $ solo si \n",
    "$ 0.5 \\leq h_ \\theta (X) $.\n",
    "\n",
    "- De otra forma, suponemos $ Y = 0 $ solo si $ 0.5 > h_ \\theta (X) $.\n",
    "\n",
    "Es decir, siempre que la hipótesis da como resultado la probabilidad de que $ Y = 1 $ es debido a que la probabilidad de que $ Y = 1 $ es mayor o igual al 50% según nuestra hipótesis (hay más probabilidades de que $ Y = 1 $ de que $ Y = 0 $). De lo contrario, si la probabilidad estimada de que $ Y = 1 $ es menor al 50%, entonces vamos a predecir que $ Y = 0 $.\n",
    "\n",
    "Ahora, ¿cómo vemos ésto en la gráfica? ¿Con qué valores de $ Z $ ($ \\theta ^{T}X $) obtendremos determinados valores para $ Y $?\n",
    "\n",
    "<img src=\"9.png\"></img>\n",
    "\n",
    "Notemos que, en la función sigmoide, $ g(Z) $ es mayor o igual a 0.5 siempre que $ Z $ sea mayor o igual a cero:\n",
    "\n",
    "- Es decir, siempre que $ 0 \\leq Z $, tendremos que $ 0.5 \\leq g(Z) $.\n",
    "\n",
    "Dado que, en la hipótesis, definimos $ Z = \\theta ^{T}X $, entonces este término debe ser mayor o igual a 0 en orden para que $ g(Z) $ sea mayor o igual a 0.5:\n",
    "\n",
    "- De esta forma, $ 0.5 \\leq g(Z) $ siempre que $ 0 \\leq Z $, por lo que $ 0.5 \\leq g( \\theta ^{T}X) $, por lo que $ 0 \\leq \\theta ^{T}X $.\n",
    "\n",
    "Lo anterior es un simple reemplazo asumiendo que $ Z = \\theta ^{T}X $.\n",
    "\n",
    "En resumen, y para terminar majaderamente con estas definiciones:\n",
    "\n",
    "- Suponemos $ Y = 1 $ cuando $ 0.5 \\leq g(Z) $.\n",
    "- Suponemos $ Y = 0 $ cuando $ 0.5 > g(Z) $.\n",
    "\n",
    "## Entendamos con el conjunto de entrenamiento\n",
    "\n",
    "Usemos el punto anterior para entender cómo la hipótesis de regresión logística realiza estas predicciones. Por lo que, ahora, supongamos que tenemos un conjunto de entrenamiento como el que se muestra en la imagen adjunta.\n",
    "\n",
    "<img src=\"10.png\"></img>\n",
    "\n",
    "Respecto al ajuste de los parámetros, hablaremos de ello más adelante.\n",
    "\n",
    "Suponiendo que, a través de un procedimiento especificado, terminamos con los siguientes valores para los parámetros:\n",
    "\n",
    "- Es decir, $ \\theta = [ \\theta _{0}, \\theta _{1}, \\theta _{2}] = [-3, 1, 1] $ (recordemos $ \\theta _{0} $ es el bias).\n",
    "\n",
    "Entonces, al tener esta elección de parámetros, intentemos averiguar donde una hipótesis terminará prediciendo que $ Y = 1 $ y donde predeciría que $ Y = 0 $. Pues, usando las fórmulas anteriormente mencionadas:\n",
    "\n",
    "- Sabemos que nuestra hipótesis predicirá $ Y = 1 $ solo si $ 0.5 \\leq g(Z) $.\n",
    "\n",
    "Al tener  $ \\theta = [ \\theta _{0}, \\theta _{1}, \\theta _{2}] = [-3, 1, 1] $, y realizando la múltiplicación de $ \\theta ^{T} $ por X:\n",
    "\n",
    "- Al ser $ \\theta ^{T} $ una matriz de 1 x 3 y X una matriz de 3 x 1, es posible realizar el producto punto. He allí el por qué se transpone $ \\theta $.\n",
    "\n",
    "Obtenemos que:\n",
    "\n",
    "- Nuestra hipótesis está definida por $ g(-3 + X_{1} + X_{2}) = \\frac{1}{1+e^{-(-3 + X_{1} + X_{2})}} $ y $ Z = -3 + X_{1} + X_{2} $, por lo que $ g(-3 + X_{1} + X_{2}) = \\frac{1}{1+e^{-(-3 + X_{1} + X_{2})}} $.\n",
    "\n",
    "En este caso particular, tenemos más de un parámetro.\n",
    "\n",
    "Realizando un poco de aritmética básica y despejando la inecuación:\n",
    "\n",
    "- Obtenemos $ 3 \\leq X_{1} \\leq X_{2} $.\n",
    "\n",
    "De esta forma, $ Y = 1 $ siempre que $ 3 \\leq X_{1} \\leq X_{2} $, y esta, corresponde a una recta.\n",
    "\n",
    "<img src=\"11.png\"></img>\n",
    "\n",
    "Al graficar la recta, veremos que pasa a través de 3 en el eje $ X_{1} $, y de igual manera en el eje $ X_{2} $. Así que, la parte del plano $ X_{1} $ y $ X_{2} $ que corresponde a cuando $ X_{1} + X_{2} $ es mayor o igual a 3, será toda la mitad derecha, y esta parte corresponde a $ Y = 1 $.\n",
    "\n",
    "Por contraparte, todas aquellas zonas donde no se satisfaga la inecuación, será la zona $ Y = 0 $.\n",
    "\n",
    "Es aquí cuando definimos lo relevante, la recta rosada nos permitió establecer los límites de la predicción de la hipótesis sobre Y, lo que se define como <b>límite de decisión</b>.\n",
    "\n",
    "<img src=\"12.png\"></img>\n",
    "\n",
    "Esta recta, el límite de decisión, corresponde a cuando la hipótesis es igual a 0.5 exactamente. La línea, separa la región de donde se predice los valores de Y:\n",
    "\n",
    "- Además, el <b>límite de decisión es una propiedad de la hipótesis que incluye los parámetros</b>. Por lo que, si no tuviéramos visualizados los datos o parámetros, el límite de decisión podría seguir graficándose.\n",
    "\n",
    "Recordemos que el vector de características son los coeficientes que acompañan a las variables independientes, que, una vez evaluadas, nos resultará en la variable dependiente Y, de modo que, anteriormente, en la regresión lineal, lo que se realizaba era construir una función que cada vez se acercara más al fenónemo (recorrido de los datos), de modo que, introduciendo nuestros conjuntos de entrenamiento de entrada, nos produzcan con menor error posible la salida real de dicho conjunto de entrenamiento.\n",
    "\n",
    "## Límites de decisión no lineales\n",
    "\n",
    "Veamos ahora un ejemplo más complejo, donde, como de costumbre, tenemos \"X\" para los ejemplos positivos y \"O\" para denotar los ejemplos negativos. \n",
    "\n",
    "Dado un conjunto de entrenamiento como el siguiente, ¿cómo obtenemos una hipótesis que se ajuste a este tipo de datos con regresión logística? ¿Un spoiler? Sí, una circunferencia sería útil.\n",
    "\n",
    "<img src=\"13.png\"></img>\n",
    "\n",
    "Es decir, veremos los límites de decisión no lineales. \n",
    "\n",
    "Anteriormente, cuando introducimos la regresión lineal, la introducimos de forma bastante simplificada. Sin embargo, y como hemos mencionado, los problemas del mundo real tienden a tener un gran número de variables, y es allí, cuando se utiliza la regresión lineal multivariable (con tres o más variables).\n",
    "\n",
    "En este sentido, y en la regresión logística, podemos aplicar lo mismo, de forma de tener términos polinómicos de orden superior que se ajusten al recorrido de los datos.\n",
    "\n",
    "- Un ejemplo conveniente de hipótesis sería $ h_ \\theta (X) = g( \\theta _{0} + \\theta _{1}X_{1} + \\theta _{2}X_{2} + \\theta _{3}X_{1}^{2} + \\theta _{4}X_{2}^{2})  $.\n",
    "\n",
    "Digamos que la hipótesis es el \"ejemplo conveniente\", donde agregamos dos características adicionales, $ X_{1}^2 $ y $ X_{2}^2 $. De esta forma, tenemos cinco parámetros $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] $. Tal como dijimos, más adelante veremos cómo ajustar estos parámetros automáticamente.\n",
    "\n",
    "Digamos que el procedimiento de ajuste de los $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] $ terminó eligiendo los siguientes valores:\n",
    "\n",
    "- Ajuste $ \\theta = [ \\theta _{0} , \\theta _{1} , \\theta _{2} , \\theta _{3} , \\theta _{4} ] = [-1, 0, 0, 1, 1] $.\n",
    "\n",
    "Ésto significa que, esta elección particular de parámetros, predice que $ Y = 1 $ siempre que:\n",
    "\n",
    "- Se predice $ Y = 1 $ si y solo si $ 0.5 \\leq g(Z) $. La única forma en que ésto suceda es que Z no sea un número negativo, de otra forma, en la función sigmoide el denominador incrementará, produciendo que converga a 0. Dicho de otra forma, necesitamos que $ 0 \\leq Z $.\n",
    "\n",
    "Resolviendo la inecuación, llegamos a que:\n",
    "\n",
    "- Se predice $ Y = 1 $ si y solo si $ 0 \\leq -1 + X_{1}^{2} + X_{2}^{2}) $, que despejando  $ 1 \\leq + X_{1}^{2} + X_{2}^{2}) $. Recordemos $ X_{0} = 1 $ (bias).\n",
    "\n",
    "De acuerdo a la inecuación, nuestro límite de decisión se vería de la siguiente manera:\n",
    "\n",
    "<img src=\"14.png\"></img>\n",
    "\n",
    "Viéndose como una circunferencia, de forma que, todo fuera del círculo se predecirá como $ Y = 1 $ mientras, lo que está adentro como $ Y = 0 $ (recordemos, X para ejemplos positivos y O para negativos).\n",
    "\n",
    "Así que, al agregar estos términos más complejos, se pueden obtener límites de decisión aún más complejos.\n",
    "\n",
    "Una vez más, el límite de decisión es una propiedad de la hipótesis bajo los parámetros, y no del conjunto de entrenamiento, por lo que:\n",
    "\n",
    "- El vector de parámetros $ \\theta $ siempre definirán el límite de decisión.\n",
    "\n",
    "Sin embargo, lo que utilizamos para definir este límite de decisión, esta propiedad de los parámetros, es el conjunto de entrenamiento.\n",
    "\n",
    "Al tener términos polinómicos de mayor grado, es posible obtener límites de decisión aún más complejos, y la regresión puede ser utilizada para límites de decisión como elipses, u otras formas extravagantes que se ajusten a las nubes de datos.\n",
    "\n",
    "<img src=\"15.jpg\"></img>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (2) La función de costo para la regresión logística\n",
    "\n",
    "Finalmente, ¿cómo ajustamos los parámetros para $ \\theta $ para la regresión logística?\n",
    "\n",
    "Para ello, debemos definir nuestra función objetivo: minimizar el costo a través de la modificación de los parámetros.\n",
    "\n",
    "<img src=\"16.png\"></img>\n",
    "\n",
    "En la imagen, tenemos el problema de aprendizaje supervisado para ajustar la regresión logística al modelo:\n",
    "\n",
    "<img src=\"17.png\"></img>\n",
    "\n",
    "- Representamos el conjunto de entrenamiento con m ejemplos.\n",
    "\n",
    "- El vector de ejemplos X es representado por n + 1 dimensiones.\n",
    "\n",
    "- La primera característica es simple igual a 1 (el bias).\n",
    "\n",
    "- Al tener un problema de clasificación, $ Y = 0 $ o $ Y = 1 $.\n",
    "\n",
    "## Nota\n",
    "\n",
    "Si no se entiende por qué $ m $ y $ n $:\n",
    "\n",
    "- Pues, $ m $ refiere al número de filas, o al número de observaciones.\n",
    "\n",
    "- Por otro lado, la cantidad de características son de $ n $ columnas.\n",
    "\n",
    "- Al vector de parámetros tener todas las características más, un parámetro adicional, que lo llamamos bias, tiene $ n + 1 $ dimensiones.\n",
    "\n",
    "## Continuando\n",
    "\n",
    "La hipótesis y los parámetros $ \\theta $... ¿Cómo los ajustamos dado el conjunto de entrenamiento?\n",
    "\n",
    "<img src=\"18.png\"></img>\n",
    "\n",
    "Antes, cuando estábamos desarrollando el modelo de regresión lineal, usábamos la función de costos. Y pues, no usaremos un algoritmo de optimización muy distinto, de hecho, es bastante similar en términos de la convergencia, puesto la fórmula varía.\n",
    "\n",
    "<img src=\"19.png\"></img>\n",
    "\n",
    "La forma alternativa a esta función de costos la definimos:\n",
    "\n",
    "<img src=\"20.png\"></img>\n",
    "\n",
    "La interpretación de esta función de costo es que queremos que el algoritmo de aprendizaje tenga que <i>pagar o asumir</i> si las salidas de la función no se acercan a los valores reales de $ Y $.\n",
    "\n",
    "## El problema de la no linealidad de la hipótesis\n",
    "\n",
    "Tal como definimos para la regresión lineal, la definición de la función del costo era la mitad del cuadrado de la diferencia entre el valor predicho por la hipótesis $ h_ \\theta (X) $ y el valor real de $ Y $.\n",
    "\n",
    "Aquella función de costos funcionó bien para la regresión lineal, pero ahora estamos interesados en la regresión logística, por lo que, si pudiéramos minimizar esta función de costo particular (considerando la función sigmoide), sucedería que la función <b>no sería convexa</b>, resultando en que el descenso del gradiente no funcionase bien, puesto habrían demasiados mínimos locales, pudiendo no converger correctamente al mínimo global.\n",
    "\n",
    "Al referirnos a que la función de costo no sea convexa, es que la función $ h_ \\theta (X) $ no tiene linearidad, por lo que al colocar $ h_ \\theta (X) $ en la función de costos y la graficamos, $ J(\\theta) $ se vería de la siguiente manera:\n",
    "\n",
    "<img src=\"21.png\"></img>\n",
    "\n",
    "Es decir, tendría muchos mínimos locales, por lo que si iniciáramos el desscenso del gradiente sobre esta función, no habría garantía de que convergiese al mínimo global.\n",
    "\n",
    "En contraste, a lo que nos gustaría como una función de costos $ J(\\theta) $ es que fuera <b>convexa</b> (con forma de bowl), de forma que al arrancar el descenso del gradiente convergiese al mínimo global.\n",
    "\n",
    "<img src=\"22.png\"></img>\n",
    "\n",
    "Sin embargo, el gran problema de ocupar esta función de costos es que debido a esta hipótesis ampliamente no lineal, $ J(\\theta) $ termina siendo una función convexa. Por lo que nos gustaría formular otra función de costos distinta, que sí fuese convexa. De esta forma, <b>tendríamos garantías de que se encontrase el mínimo global</b>.\n",
    "\n",
    "## Primer acercamiento a la función de costo para la función sigmoide\n",
    "\n",
    "Ahora sí, tras ver las problemáticas de aplicar la función de costos de la regresión lineal, a la regresión logística, veremos entonces la función de costos a esta última.\n",
    "\n",
    "<img src=\"23.png\"></img>\n",
    "\n",
    "Diremos que el costo o la penalidad que el algoritmo paga es el costo asociado al valor predicho $ h_ \\theta (X) $ con su valor real:\n",
    "\n",
    "- Siendo el logaritmo negativo de la hipótesis si $ Y = 1 $.\n",
    "\n",
    "- O el logaritmo negativo de la diferencia entre $ 1 $ y la hipótesis si $ Y = 0 $.\n",
    "\n",
    "Graficaremos esta función para tener cierta intuición de lo que hace esta función. Empezaremos con el caso $ Y = 1 $.\n",
    "\n",
    "<img src=\"24.png\"></img>\n",
    "\n",
    "Una forma de comprender por qué el gráfico es así, es porque si tuviéramos que graficar el logaritmo de Z, con Z en el eje horizontal, se vería así:\n",
    "\n",
    "<img src=\"25.png\"></img>\n",
    "\n",
    "Dado que solo nos interesa el intervalo entre 0 y 1 (y que no sea negativo), entonces se le antepone el signo negativo a la función y nos quedamos solo con el intervalo $ [0, 1] $.\n",
    "\n",
    "## Propiedades para $ Y = 0 $ o $ Y = 1 $\n",
    "\n",
    "Esta función de costos, en particular, tiene propiedades interesantes y atractivas:\n",
    "\n",
    "- El costo es cero si $ Y = 1 $ y $ h_ \\theta (X) = 1$: $ COST = 0 $ IF $ Y = 0 $ AND $ h_ \\theta (X) = 1$.\n",
    "\n",
    "- Si $ Y = 1 $ y $ h_ \\theta (X) \\to 0$, entonces el costo es infinito: $ COST = \t\\infty $ IF $ Y = 1 $ AND $ h_ \\theta (X) = 0$.\n",
    "\n",
    "## Resumen de la función de costo (hasta ahora)\n",
    "\n",
    "En este punto, definimos la función de costo para un simple ejemplo de entrenamiento. El tópico de convexividad es posible profundizarlo aún más, siendo posible demostrar que con una elección particular de una función de costo nos dará solución a la problemática de la no linealidad.\n",
    "\n",
    "En general, la función de costo $ J(\\theta) debe ser convexa y óptima para obtener el mínimo global.\n",
    "\n",
    "En el siguiente punto, tomaremos estas ideas vistas, y las llevaremos un poco más lejos, definiendo la función de costos para el conjunto de entrenamiento completo. \n",
    "\n",
    "También, nos daremos cuenta que existe una manera simple y acotada de escribir la función de costo, basado en el algoritmo del descenso del gradiente.\n",
    "\n",
    "## La función de costo simplificada y el descenso del gradiente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}